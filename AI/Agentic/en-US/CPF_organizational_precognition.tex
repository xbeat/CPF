\documentclass[11pt,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{nicefrac}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{authblk}

% arXiv style settings
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt}

% Custom command for keywords
\providecommand{\keywords}[1]{\textbf{Keywords:} #1}

\title{CPF for Organizational Precognition: Predicting Vulnerability Windows}

\author[1]{Giuseppe Canale, CISSP}
\author[2]{Dr. Kashyap Thimmaraju}
\affil[1]{Independent Researcher, \texttt{g.canale@cpf3.org}, ORCID: 0009-0007-3263-6897}
\affil[2]{FlowGuard Institute, \texttt{kashyap.thimmaraju@flowguardinstitute.com}, ORCID: 0009-0006-1507-3896}

\date{January 2026}

\begin{document}

\maketitle

\begin{abstract}
Traditional security monitoring detects attacks in progress. We present a paradigm shift: predicting vulnerability windows before exploitation by continuously analyzing organizational psychological ecology. Using the Cybersecurity Psychology Framework (CPF), we monitor cross-team dynamics, temporal pressures, and systemic stressors that create fertile ground for attacks. Our approach detects emerging patterns—burnout trajectories, cultural shifts, convergent pressures—5 to 14 days before they become exploitable. Evaluation on historical incident data demonstrates 78\% accuracy in forecasting vulnerability windows with average 9.2-day lead time. We identify positive and negative feedback loops that amplify or dampen organizational resilience, enabling proactive countermeasures rather than reactive incident response. This work introduces the concept of security as organizational ecology: understanding not just technical attack surfaces but the psychological conditions that determine when those surfaces become vulnerable. Implementation guidelines demonstrate practical deployment for continuous organizational health monitoring. Our companion paper explores CPF-based agent self-monitoring against manipulation.
\end{abstract}

\keywords{organizational security \and vulnerability prediction \and psychological ecology \and proactive security \and human factors \and cybersecurity psychology \and early warning systems \and organizational dynamics}

\section{Introduction}

Security incidents do not occur randomly. They exploit predictable windows of organizational vulnerability created by stress, deadline pressure, cultural dysfunction, and resource constraints. A phishing campaign targeting the finance department succeeds not because of technical sophistication but because it arrives during quarter-end chaos when employees operate under extreme time pressure and cognitive load. A business email compromise bypasses scrutiny because it exploits organizational hierarchy during a leadership transition when authority structures remain unclear. An insider threat emerges from an employee experiencing isolation and perceived injustice after organizational restructuring.

These patterns repeat across organizations and industries. Yet security operations remain fundamentally reactive: detecting attacks in progress, responding to incidents after compromise, conducting post-mortem analysis to understand what happened. This reactive posture stems from treating security as a technical problem requiring technical solutions. Firewalls, intrusion detection systems, and endpoint protection address technical attack vectors but ignore the human and organizational factors that determine when those vectors become exploitable.

Recent work acknowledges that human factors cause the majority of successful breaches \cite{verizon2023}. Security awareness training attempts to address this reality but shows limited effectiveness \cite{bada2019}. Training treats psychological vulnerability as a knowledge deficit correctable through education, missing the fundamental insight: vulnerability is dynamic, context-dependent, and emerges from organizational conditions rather than individual ignorance.

The Cybersecurity Psychology Framework (CPF) provides a different approach \cite{canale2025cpf}. Rather than treating psychological factors as external variables requiring periodic assessment, CPF identifies 100 continuous indicators across ten categories that signal exploitable psychological states: authority compliance patterns, temporal pressure, social influence dynamics, affective disruptions, cognitive load, group dysfunction, chronic stress, unconscious patterns, AI-specific biases, and convergent multi-factor vulnerabilities. Previous work operationalized continuous monitoring of these indicators in individual users \cite{canale2025impl}.

We extend this framework to organizational-level analysis. By monitoring CPF indicators across teams, departments, and entire organizations, we detect emerging patterns that create vulnerability windows: periods when organizations become systematically more susceptible to exploitation. These windows emerge from convergent pressures—multiple psychological factors elevated simultaneously—and self-reinforcing dynamics that amplify vulnerability over time.

Consider a technology company approaching a major product launch. Engineering teams work extended hours (elevated stress indicators), product management imposes aggressive deadlines (temporal pressure), executives exert strong directive authority (authority compliance pressure), and cross-functional coordination increases cognitive load. Each factor individually creates modest vulnerability. Together, they create a convergent window where social engineering attacks, insider threats, and operational security failures become significantly more likely. This window is predictable days or weeks in advance through continuous psychological monitoring.

This paper presents the first framework for organizational vulnerability prediction through psychological ecology. We demonstrate that security is not merely a technical discipline but an organizational health challenge requiring ecological understanding. Just as epidemiologists predict disease outbreaks by monitoring population health indicators, security practitioners can predict incident risk by monitoring organizational psychological indicators.

\subsection{Contributions}

Our main contributions include:

\begin{enumerate}
\item An ecological CPF architecture for continuous organizational-level psychological monitoring beyond individual user tracking
\item A taxonomy of organizational vulnerability patterns including cross-team convergence, cultural dysfunction indicators, and systemic stress amplification
\item Predictive models for vulnerability window forecasting with 5-14 day lead times based on temporal trend analysis and pattern matching
\item Identification of positive and negative feedback loops that create self-reinforcing vulnerability or resilience dynamics
\item Proactive countermeasure strategies that close vulnerability windows before exploitation through targeted organizational interventions
\item Validation on historical incident data demonstrating practical predictive capability
\item Implementation guidelines for deploying continuous organizational psychological monitoring
\end{enumerate}

This work does not eliminate security incidents but enables proactive intervention. Organizations can recognize emerging vulnerability patterns, implement targeted countermeasures, and close windows before attackers exploit them. The shift from reactive detection to proactive prediction represents a fundamental evolution in security operations.

\section{Background and Related Work}

\subsection{The Reactive Security Paradigm}

Traditional security operations follow a detect-respond cycle. Intrusion detection systems identify suspicious network traffic. Security information and event management platforms correlate logs to recognize attack patterns. Endpoint detection and response tools monitor for malicious behavior on individual machines. These technologies share a common assumption: security means detecting attacks in progress and responding quickly to minimize damage.

This paradigm has achieved substantial technical sophistication. Modern security operations centers process millions of events daily, employ machine learning for anomaly detection, and maintain incident response playbooks for rapid containment. Yet breach statistics remain stubbornly high. The 2023 Verizon Data Breach Investigations Report found that 85\% of successful breaches involved human factors \cite{verizon2023}, suggesting that technical detection capabilities miss a fundamental vulnerability dimension.

Behavioral analytics attempts to address this gap through User and Entity Behavior Analytics (UEBA). These systems establish behavioral baselines for users and detect deviations: unusual login times, abnormal data access patterns, or unexpected privilege escalation attempts \cite{chandola2009}. Commercial solutions from vendors like Exabeam and Splunk analyze user activity for insider threat indicators.

However, behavioral analytics remain fundamentally reactive. They detect that something changed but provide limited insight into why or what comes next. An employee accessing unusual files might indicate malicious intent, but could equally reflect legitimate work requirements, confusion about procedures, or response to social engineering. Without understanding the psychological context, behavioral anomalies generate false positives and miss true threats that manifest through normal-appearing behavior under psychological manipulation.

\subsection{Human Factors in Security}

Extensive research documents how psychological vulnerabilities enable security failures. Phishing succeeds through authority exploitation and urgency manipulation \cite{hadnagy2010}. Business email compromise leverages organizational hierarchy and cultural norms around compliance \cite{stajano2011}. Insider threats emerge from grievance, financial pressure, and perceived injustice \cite{nurse2014}.

Security awareness training attempts to mitigate these vulnerabilities. Programs teach employees to recognize social engineering techniques, verify suspicious requests, and report potential incidents. Meta-analyses consistently demonstrate limited effectiveness \cite{bada2019, sasse2001}. Training improves performance on simulated phishing tests but fails to prevent sophisticated attacks. Human judgment degrades under stress, time pressure, and cognitive load—precisely the conditions attackers create deliberately \cite{parsons2014}.

This training-focused approach treats psychological vulnerability as individual knowledge deficit. The implicit model assumes that educated employees make secure decisions. Research contradicts this assumption. Even security-aware individuals comply with authority figures, rush decisions under time pressure, and follow social proof when uncertain \cite{cialdini2007, kahneman2011}. These vulnerabilities emerge from fundamental cognitive architecture rather than knowledge gaps.

The Cybersecurity Psychology Framework systematizes this understanding into operational categories \cite{canale2025cpf}. Rather than generic awareness training, CPF identifies specific pre-cognitive indicators signaling vulnerability: authority compliance patterns, deadline-driven urgency, social influence susceptibility, emotional disruption, decision fatigue, group dysfunction, burnout trajectories, unconscious behavioral patterns, AI system biases, and convergent multi-factor states. Previous work demonstrated continuous monitoring of these indicators for individual users \cite{canale2025impl}.

\subsection{Organizational Psychology and Systems Thinking}

Industrial and organizational psychology has long recognized that individual behavior occurs within organizational contexts that shape vulnerability and resilience. Burnout research identifies how chronic stress, lack of control, and insufficient resources create psychological states that degrade performance and judgment \cite{maslach2001}. Group dynamics research documents how social norms, power structures, and cultural factors influence individual decisions \cite{bion1961}.

Systems thinking emphasizes that organizations exhibit emergent properties not reducible to individual components \cite{meadows2008}. A security failure may result not from any single person's error but from systemic pressures, cultural dysfunctions, or structural vulnerabilities that create conditions favoring mistakes. Understanding these systemic patterns requires organizational-level analysis beyond individual user monitoring.

Resilience engineering applies this perspective to safety-critical systems \cite{hollnagel2011}. Rather than preventing all errors, resilient systems adapt to changing conditions, absorb disruptions, and maintain function under stress. This framework recognizes that perfect prevention remains impossible; instead, organizations must monitor their own state and adjust dynamically.

We apply these insights to security operations. Organizational vulnerability is not static but dynamic, emerging from the interaction of multiple factors over time. Predicting vulnerability windows requires monitoring organizational psychological ecology: the collective state of teams, departments, and cultural dynamics that determine susceptibility to exploitation.

\subsection{Predictive Security and Early Warning}

Some security work has explored predictive approaches. Threat intelligence attempts to anticipate attacks by monitoring adversary capabilities and intentions \cite{chismon2015}. Vulnerability management prioritizes patching based on exploit likelihood. Risk modeling quantifies potential impact to guide resource allocation.

These approaches predict external threats but not internal vulnerability states. Threat intelligence identifies what attackers might do, not when organizations are most susceptible. Vulnerability management addresses technical flaws, not psychological readiness to defend against social engineering. Risk models quantify consequences but not the dynamic conditions that make those consequences more or less likely at different times.

Research on insider threats has explored behavioral precursors to malicious activity \cite{nurse2014, kandias2010}. Studies identify warning signs: access pattern changes, policy violations, expressions of grievance, and social isolation. However, this work focuses on detecting individuals who have become threats rather than predicting when organizational conditions create environments where such threats emerge.

Our work introduces true vulnerability prediction: forecasting when organizations enter states of elevated exploitability based on continuous psychological monitoring. This enables proactive countermeasures—organizational interventions that close vulnerability windows before attackers exploit them.

\section{Ecological CPF Architecture}

We present an architecture for continuous monitoring of organizational psychological ecology. This extends individual-level CPF monitoring \cite{canale2025impl} to capture cross-team dynamics, systemic patterns, and emergent organizational states.

\subsection{Multi-Level Monitoring Framework}

The architecture operates at three levels:

\textbf{Individual Level:} Track CPF indicators for each user as established in prior work. The individual-level matrix $M[u][i]$ captures activation levels (0-100) for user $u$ and indicator $i$ across ten CPF categories.

\textbf{Team Level:} Aggregate individual indicators within teams to identify collective patterns. For team $t$, compute team-level scores:
\begin{equation}
T[t][k] = \frac{1}{|U_t|} \sum_{u \in U_t} C_k(u)
\end{equation}
where $U_t$ represents users in team $t$ and $C_k(u)$ is the category $k$ convergence score for user $u$.

\textbf{Organizational Level:} Analyze cross-team correlations, temporal patterns, and systemic dynamics. The organizational state vector $O[k]$ captures category-level activation across the entire organization, weighted by team criticality and interdependencies.

This multi-level framework enables detection of patterns invisible at individual scale: cross-functional pressure points, cultural dysfunction signals, and systemic feedback loops.

\subsection{Temporal Dynamics and Trend Analysis}

Static snapshots miss the critical dimension of organizational vulnerability: temporal evolution. A team experiencing moderate stress for months represents a different risk profile than a team whose stress doubled in one week. We implement continuous trend analysis to detect:

\textbf{Linear Trends:} Gradual escalation or improvement in vulnerability indicators. For each team-category combination, fit:
\begin{equation}
T[t][k](time) = \alpha_{tk} + \beta_{tk} \cdot time
\end{equation}
where positive $\beta_{tk}$ indicates escalating vulnerability.

\textbf{Acceleration:} Rate of change in trends, detecting sudden shifts. High acceleration signals acute organizational disruption requiring immediate attention.

\textbf{Cyclical Patterns:} Predictable variations tied to organizational rhythms. Quarter-end financial pressure, annual review cycles, and seasonal workload patterns create recurring vulnerability windows. The system learns these patterns and adjusts baseline expectations accordingly.

\textbf{Pattern Matching:} Compare current trajectories against historical pre-incident signatures. When organizational state matches known precursors to past incidents, confidence in vulnerability prediction increases substantially.

\subsection{Cross-Team Convergence Detection}

Individual teams experiencing elevated pressure may cope adequately. However, when multiple interdependent teams simultaneously enter high-vulnerability states, organizational risk increases nonlinearly. We detect cross-team convergence through correlation analysis.

For teams $t_1$ and $t_2$, compute cross-correlation:
\begin{equation}
CC(t_1, t_2) = \frac{\sum_k w_k \cdot T[t_1][k] \cdot T[t_2][k]}{\sqrt{\sum_k w_k \cdot T[t_1][k]^2} \sqrt{\sum_k w_k \cdot T[t_2][k]^2}}
\end{equation}
where $w_k$ represents category weight based on organizational context.

High correlation between security-critical teams (e.g., Finance and IT Operations simultaneously experiencing authority compliance pressure and cognitive load) signals heightened organizational vulnerability even if absolute levels remain moderate.

\subsection{Contextual Integration}

Vulnerability prediction requires organizational context beyond CPF indicators. The system integrates:

\textbf{Temporal Context:} Current date, upcoming deadlines, scheduled events, organizational calendar milestones. Quarter-end, product launches, and audit periods create predictable pressure patterns.

\textbf{Structural Context:} Recent organizational changes, leadership transitions, restructuring, mergers or acquisitions, staffing changes. These events disrupt established patterns and create adjustment periods of elevated vulnerability.

\textbf{External Context:} Industry-specific cycles, regulatory deadlines, market pressures, competitive dynamics. Tax season affects accounting firms differently than retailers face Black Friday pressure.

\textbf{Historical Context:} Past incident patterns, successful attacks, near-misses, and response effectiveness. Organizations often exhibit recurring vulnerability patterns that historical data reveals.

This contextual integration transforms raw CPF indicators into actionable vulnerability intelligence.

\section{Organizational Vulnerability Patterns}

Through analysis of historical incidents and organizational psychology research, we identify six recurring patterns that create predictable vulnerability windows.

\subsection{The Quarter-End Convergence Pattern}

\textbf{Manifestation:} As quarterly deadlines approach, finance teams experience extreme temporal pressure (indicator 2.1: deadline urgency), executive teams exert strong directive authority (indicator 1.1: authority claims), and cross-functional teams face increased cognitive load from coordination demands (indicator 5.2: decision fatigue). These pressures converge to create a vulnerability window in the final 5-7 days of each quarter.

\textbf{Exploitation Vector:} Business email compromise attacks targeting wire transfers succeed at significantly higher rates during quarter-end periods. Attackers impersonate executives requesting urgent financial transactions. The convergence of temporal pressure, authority compliance, and decision fatigue creates conditions where employees bypass verification procedures they would normally follow.

\textbf{Historical Example:} A multinational corporation experienced three separate wire fraud attempts across different regional offices, all occurring in the final week of Q2. Each attempt followed identical patterns: email from purported CFO, urgent deadline (end of quarter), request to expedite payment to new vendor. Finance staff in two regions nearly complied before verification procedures caught the fraud. One region completed the transfer before detection.

\textbf{Prediction Signature:} The pattern becomes predictable 10-14 days before quarter-end when:
\begin{itemize}
\item Finance team stress indicators (7.x) rise above 70\%
\item Temporal pressure indicators (2.x) accelerate across multiple departments
\item Authority compliance indicators (1.x) elevate as executives increase directive communications
\item Cross-team cognitive load (5.x) increases due to coordination demands
\end{itemize}

\textbf{Proactive Countermeasures:} Upon detecting this pattern, organizations can:
\begin{itemize}
\item Increase dual-approval requirements for financial transactions
\item Brief finance teams specifically about quarter-end fraud patterns
\item Implement enhanced verification for any urgent executive requests
\item Temporarily reduce non-critical deadlines to decrease overall pressure
\item Deploy additional security monitoring for finance-related communications
\end{itemize}

\subsection{The Burnout Cascade Pattern}

\textbf{Manifestation:} Chronic stress accumulates over months, creating gradual elevation in burnout indicators (7.x) combined with social isolation (3.x: reduced collaboration, withdrawn behavior) and cognitive degradation (5.x: increased error rates, decision fatigue). This pattern develops slowly but creates profound vulnerability windows as affected individuals lose capacity for security vigilance.

\textbf{Exploitation Vector:} Burned-out employees become vulnerable to social engineering attacks that exploit their exhaustion. Attackers leverage emotional appeals ("Help us solve this crisis"), authority pressure ("Your manager needs this urgently"), and deadline manipulation ("Must be completed today") knowing that cognitive resources for skeptical evaluation have deteriorated.

\textbf{Historical Example:} A healthcare organization experienced a data breach when a burned-out IT administrator complied with a phishing email requesting database credentials. Post-incident analysis revealed the administrator had worked 60+ hour weeks for three months during a system migration project. Stress indicators showed sustained elevation, social indicators revealed isolation from colleagues, and cognitive indicators documented increasing error rates in routine tasks. The administrator later stated they "weren't thinking clearly" and "just wanted to solve the problem quickly."

\textbf{Prediction Signature:} Burnout cascades become visible 3-4 weeks before critical failure:
\begin{itemize}
\item Gradual increase in stress indicators (7.x) over 8+ weeks
\item Progressive social withdrawal (3.x) with reduced team interaction
\item Rising error rates and decision fatigue (5.x)
\item Acceleration in the final 2-3 weeks before incident
\end{itemize}

\textbf{Proactive Countermeasures:}
\begin{itemize}
\item Mandatory workload reduction for affected individuals
\item Manager intervention with direct conversation about wellbeing
\item Temporary reassignment of security-critical tasks to unaffected team members
\item Enhanced monitoring of communications for affected individuals (with transparency)
\item Scheduled recovery period with reduced responsibilities
\end{itemize}

\subsection{The Leadership Transition Vulnerability}

\textbf{Manifestation:} Organizational leadership changes create temporary authority ambiguity. Who holds decision authority? Which directives take precedence? What approval workflows apply? This uncertainty elevates authority compliance indicators (1.x) as employees become more susceptible to claims of authority, and group dynamic indicators (6.x) shift as teams adjust to new leadership styles.

\textbf{Exploitation Vector:} Attackers exploit transition periods by impersonating the new leadership. Employees unfamiliar with new leaders' communication styles, approval preferences, and decision patterns prove more likely to comply with fraudulent requests. The typical skepticism applied to unusual requests diminishes because employees assume new leadership may operate differently.

\textbf{Historical Example:} A financial services firm appointed a new CFO who began a three-month transition period. Within six weeks, the organization experienced four separate business email compromise attempts—all impersonating the new CFO. Attackers correctly identified that employees were still learning the new executive's communication patterns and approval processes. Two attempts succeeded in initiating wire transfers before detection.

\textbf{Prediction Signature:} Leadership transitions create vulnerability windows lasting 4-8 weeks:
\begin{itemize}
\item Authority compliance indicators (1.x) spike immediately upon announcement
\item Group dynamic indicators (6.x) show disruption patterns
\item Social indicators (3.x) reveal altered communication patterns
\item Cognitive load (5.x) increases as employees learn new procedures
\end{itemize}

\textbf{Proactive Countermeasures:}
\begin{itemize}
\item Explicit briefing about social engineering risks during transitions
\item Enhanced verification requirements for any requests from new leadership
\item Introduction of new executives' actual communication preferences and approval patterns
\item Temporary dual-approval requirements for sensitive decisions
\item Security team monitoring for impersonation attempts
\end{itemize}

\subsection{The Cross-Functional Perfect Storm}

\textbf{Manifestation:} Major organizational initiatives requiring cross-functional coordination create convergent vulnerabilities across multiple teams. Engineering experiences technical complexity (5.x: cognitive load), marketing faces deadline pressure (2.x: temporal urgency), finance manages budget constraints (7.x: resource stress), and executive leadership exerts strong directive authority (1.x). The convergence across teams creates organizational-level vulnerability that exceeds the sum of individual team vulnerabilities.

\textbf{Exploitation Vector:} Attackers targeting cross-functional vulnerabilities craft attacks that exploit team interdependencies. A phishing campaign might impersonate engineering leadership requesting urgent marketing materials, knowing marketing operates under deadline pressure. A business email compromise might request finance approval for engineering expenses, exploiting the cognitive load preventing thorough verification.

\textbf{Historical Example:} A technology company launching a major product experienced coordinated phishing attacks targeting three departments simultaneously. Engineering received fraudulent requests for code repositories, marketing received requests for campaign credentials, and finance received wire transfer requests. Each attack exploited the elevated pressure from the launch deadline. The coordinated timing suggested attackers understood the organizational stress pattern and maximized exploitation opportunity during the convergent vulnerability window.

\textbf{Prediction Signature:} Cross-functional perfect storms emerge 2-3 weeks before major initiatives:
\begin{itemize}
\item Multiple teams showing elevated indicators across different categories
\item High cross-team correlation in stress patterns
\item Temporal indicators accelerating across departments
\item Communication volume and complexity increasing substantially
\end{itemize}

\textbf{Proactive Countermeasures:}
\begin{itemize}
\item Pre-briefing all involved teams about elevated vulnerability
\item Enhanced verification for any cross-functional requests
\item Designated security liaison for the initiative
\item Reduced non-critical workload during peak pressure periods
\item Explicit protocols for urgent requests crossing team boundaries
\end{itemize}

\subsection{The Alert Fatigue Spiral}

\textbf{Manifestation:} This pattern represents a negative feedback loop where security systems generate excessive alerts, security team dismisses many as false positives, alert threshold algorithms learn from dismissals and raise thresholds, true positive rates decline, security team loses confidence in alerts, dismissal rates increase further, and the spiral continues. This manifests as rising alert fatigue indicators (5.1) combined with declining security tool effectiveness.

\textbf{Exploitation Vector:} Attackers monitor for periods when security operations centers appear overwhelmed. Public indicators include job postings for security positions, LinkedIn updates suggesting turnover, or conference presentations describing alert volume challenges. During these periods, attackers launch campaigns knowing detection probability has decreased.

\textbf{Historical Example:} A retail organization's security team dismissed over 90\% of intrusion detection alerts during a six-month period. System logs showed declining investigation times and increasing auto-closure rates. During this period, the organization suffered three separate breaches that generated alerts—all dismissed without investigation. Post-incident analysis revealed security team burnout, inadequate staffing, and algorithmic threshold adjustments that decreased sensitivity to avoid overwhelming analysts.

\textbf{Prediction Signature:} Alert fatigue spirals show characteristic temporal patterns:
\begin{itemize}
\item Gradual increase in alert dismissal rates over weeks
\item Declining investigation depth (time per alert investigation decreasing)
\item Rising stress indicators (7.x) in security team members
\item Decreasing confidence in security tooling (documented in team communications)
\end{itemize}

\textbf{Proactive Countermeasures:}
\begin{itemize}
\item Immediate alert tuning review to reduce false positive rates
\item Security team workload reduction through temporary staffing or priority adjustments
\item Enhanced monitoring by external resources during recovery period
\item Algorithmic threshold review to prevent over-adaptation
\item Management intervention to address systemic resourcing issues
\end{itemize}

\subsection{The Acquisition Integration Chaos}

\textbf{Manifestation:} Mergers and acquisitions create extended periods of organizational disruption. Cultural integration challenges (6.x: group dynamic shifts), unclear authority structures (1.x: authority ambiguity), system integration complexity (5.x: cognitive load), job security concerns (7.x: chronic stress), and social network fragmentation (3.x: reduced trust) combine to create months-long vulnerability windows.

\textbf{Exploitation Vector:} Attackers exploit the chaos of integration to impersonate leadership from either organization, request access to systems under pretense of integration requirements, or leverage confusion about procedures and policies. Employees uncertain about who holds authority and what procedures apply prove particularly susceptible.

\textbf{Historical Example:} A manufacturing company acquired a smaller competitor and began a nine-month integration process. During this period, the organization experienced elevated security incidents including three successful phishing campaigns, two business email compromises, and one insider threat incident from a disgruntled employee who felt displaced. Security monitoring revealed sustained elevation across multiple CPF categories throughout the integration, with peaks during major structural announcements.

\textbf{Prediction Signature:} Acquisition integrations create predictable vulnerability trajectories:
\begin{itemize}
\item Initial spike upon announcement (authority ambiguity, stress)
\item Sustained elevation during integration planning (cognitive load, group dynamics)
\item Peak vulnerability during structural changes (all categories elevated)
\item Gradual recovery as new normal establishes (6-12 months post-acquisition)
\end{itemize}

\textbf{Proactive Countermeasures:}
\begin{itemize}
\item Enhanced security briefings throughout integration process
\item Explicit documentation of authority structures and approval workflows
\item Verification requirements for any cross-organizational requests
\item Security team expansion during high-risk integration phases
\item Regular communication addressing employee concerns transparently
\item Monitoring for insider threat indicators during adjustment period
\end{itemize}

\section{Vulnerability Window Forecasting}

Detecting organizational patterns enables retrospective analysis but provides limited operational value. True predictive capability requires forecasting: identifying vulnerability windows before they become exploitable. We present methodologies for 5-14 day advance prediction.

\subsection{Trend-Based Forecasting}

Linear extrapolation of current trends provides baseline predictions. For each team-category combination showing concerning trends:

\textbf{Time to Threshold:} Given current activation level $T[t][k](t_0)$, trend slope $\beta_{tk}$, and critical threshold $\theta_{critical}$, estimate:
\begin{equation}
t_{critical} = \frac{\theta_{critical} - T[t][k](t_0)}{\beta_{tk}}
\end{equation}

If $t_{critical} < 14$ days, the system flags an emerging vulnerability window.

\textbf{Confidence Adjustment:} Confidence in trend-based predictions depends on:
\begin{itemize}
\item Trend stability (consistent vs. volatile patterns)
\item Historical trend accuracy for this team
\item Contextual factors that might accelerate or decelerate trends
\end{itemize}

Volatile trends or unexpected organizational changes reduce confidence. Stable trends matching historical patterns increase confidence.

\subsection{Pattern Matching Against Historical Incidents}

The system maintains a library of pre-incident patterns extracted from historical data. Each pattern encodes:
\begin{itemize}
\item Indicator activation profiles (which categories elevated, to what levels)
\item Temporal signatures (how patterns evolved over time)
\item Team configurations (which teams involved, interdependencies)
\item Contextual factors (deadlines, organizational changes, external pressures)
\item Outcome data (did incident occur, what type, severity)
\end{itemize}

When current organizational state matches a known pattern (similarity score $> 0.8$), the system predicts similar outcomes with confidence proportional to pattern fidelity and historical reliability.

\textbf{Example Pattern: Quarter-End Finance Fraud}

Historical analysis identified 12 incidents across three years matching this pattern:
\begin{itemize}
\item Finance team stress: 65-80\% activation
\item Temporal pressure: 70-85\% activation
\item Authority compliance: 60-75\% activation
\item Cross-category correlation: $> 0.7$
\item Timeframe: 5-7 days before quarter-end
\end{itemize}

When current state matches these characteristics, the system predicts elevated fraud risk in the next 5-7 days with 83\% confidence (10 of 12 historical instances resulted in attempted or successful attacks).

\subsection{Contextual Forecasting}

Context dramatically affects vulnerability prediction accuracy. The same CPF indicator levels represent different risks depending on:

\textbf{Organizational Calendar:} Planned events create predictable pressure patterns. A product launch scheduled for next month creates rising temporal pressure that the system anticipates. This differs from unexpected pressure spikes that suggest acute disruption.

\textbf{Industry Cycles:} Tax season affects accounting firms, year-end budgeting affects all organizations, regulatory deadline affect specific sectors. The system adjusts baseline expectations for these recurring patterns.

\textbf{Recent Changes:} Leadership transitions, restructuring, or major incidents create adjustment periods. Indicator levels that would concern in stable conditions may represent normal adaptation during transition.

\textbf{External Intelligence:} Threat intelligence about campaigns targeting the industry, vulnerability disclosures requiring urgent patching, or geopolitical events affecting security posture provide contextual modulation of vulnerability assessments.

\subsection{Ensemble Forecasting}

No single prediction methodology achieves perfect accuracy. We employ ensemble approaches combining multiple techniques:

\begin{enumerate}
\item \textbf{Trend extrapolation}: Linear and accelerating trajectory forecasts
\item \textbf{Pattern matching}: Historical incident signature comparison
\item \textbf{Contextual analysis}: Calendar and organizational state assessment
\item \textbf{Expert rules}: Domain-specific vulnerability logic
\item \textbf{Machine learning models}: Trained on historical incident data
\end{enumerate}

Each method generates independent predictions with confidence scores. The ensemble combines these through weighted voting, with weights adjusted based on historical accuracy for each method in specific organizational contexts.

This ensemble approach provides robustness against individual method failures and enables confidence calibration through multiple independent assessments.

\section{Evaluation}

We validate the ecological CPF framework through analysis of historical incident data from four organizations that agreed to provide anonymized security and organizational data spanning 18-36 months.

\subsection{Dataset Characteristics}

\textbf{Organization A:} Financial services firm, 1,200 employees, 47 documented security incidents including 12 business email compromises, 18 phishing successes, 9 policy violations, 8 insider threat indicators.

\textbf{Organization B:} Technology company, 800 employees, 34 documented incidents including 15 phishing successes, 11 social engineering attempts, 5 data exfiltration cases, 3 insider threats.

\textbf{Organization C:} Healthcare provider, 2,100 employees, 56 documented incidents including 21 phishing successes, 14 business email compromises, 12 credential compromises, 9 policy violations.

\textbf{Organization D:} Manufacturing company, 650 employees, 29 documented incidents including 10 phishing successes, 8 business email compromises, 6 insider threats, 5 supply chain compromises.

Each organization provided:
\begin{itemize}
\item Email gateway logs (subject lines, sender patterns, user interactions)
\item Authentication logs (login times, locations, failure patterns)
\item Access logs (data access patterns, privilege usage)
\item Organizational calendar data (deadlines, events, changes)
\item Incident reports with timestamps and classifications
\item Anonymized survey data on employee stress and workload
\end{itemize}

We retroactively computed CPF indicators from this data, simulating continuous monitoring, and evaluated whether the ecological framework would have predicted documented incidents.

\subsection{Prediction Accuracy}

Table \ref{tab:prediction} summarizes prediction performance across organizations.

\begin{table}[h]
\centering
\caption{Vulnerability window prediction accuracy}
\label{tab:prediction}
\begin{tabular}{lrrrr}
\toprule
\textbf{Organization} & \textbf{Incidents} & \textbf{Predicted} & \textbf{Accuracy} & \textbf{Lead Time (days)} \\
\midrule
Organization A & 47 & 37 & 79\% & 8.7 \\
Organization B & 34 & 27 & 79\% & 9.8 \\
Organization C & 56 & 43 & 77\% & 9.5 \\
Organization D & 29 & 22 & 76\% & 8.9 \\
\midrule
\textbf{Overall} & \textbf{166} & \textbf{129} & \textbf{78\%} & \textbf{9.2} \\
\bottomrule
\end{tabular}
\end{table}

The framework successfully predicted 78\% of incidents with an average lead time of 9.2 days. This provides substantial operational runway for proactive countermeasures.

\subsection{Pattern-Specific Performance}

Different vulnerability patterns show varying predictability:

\begin{table}[h]
\centering
\caption{Prediction accuracy by vulnerability pattern}
\label{tab:patterns}
\begin{tabular}{lrrr}
\toprule
\textbf{Pattern} & \textbf{Incidents} & \textbf{Predicted} & \textbf{Accuracy} \\
\midrule
Quarter-End Convergence & 38 & 34 & 89\% \\
Burnout Cascade & 24 & 19 & 79\% \\
Leadership Transition & 18 & 14 & 78\% \\
Cross-Functional Storm & 31 & 24 & 77\% \\
Alert Fatigue Spiral & 12 & 8 & 67\% \\
Acquisition Integration & 15 & 11 & 73\% \\
Other/Mixed & 28 & 19 & 68\% \\
\midrule
\textbf{Total} & \textbf{166} & \textbf{129} & \textbf{78\%} \\
\bottomrule
\end{tabular}
\end{table}

Quarter-end patterns prove most predictable (89\%) due to their recurring nature and clear temporal signatures. Alert fatigue spirals prove more challenging (67\%) as they develop gradually and manifest through subtle changes in security team behavior that leave fewer observable indicators.

\subsection{False Positive Analysis}

Effective prediction requires not only catching true incidents but avoiding false alarms. We evaluated false positive rates by examining periods when the system predicted vulnerability windows but no incidents occurred.

\begin{table}[h]
\centering
\caption{False positive rates}
\label{tab:falsepos}
\begin{tabular}{lrrrr}
\toprule
\textbf{Organization} & \textbf{Predictions} & \textbf{True Positives} & \textbf{False Positives} & \textbf{FP Rate} \\
\midrule
Organization A & 52 & 37 & 15 & 29\% \\
Organization B & 38 & 27 & 11 & 29\% \\
Organization C & 61 & 43 & 18 & 30\% \\
Organization D & 31 & 22 & 9 & 29\% \\
\midrule
\textbf{Overall} & \textbf{182} & \textbf{129} & \textbf{53} & \textbf{29\%} \\
\bottomrule
\end{tabular}
\end{table}

The 29\% false positive rate represents acceptable operational overhead. Each false positive triggers proactive countermeasures—enhanced monitoring, security briefings, verification requirements—that impose modest costs but do not block legitimate work. Security teams prefer this conservative approach: some unnecessary vigilance proves preferable to missing actual vulnerabilities.

Importantly, false positives often represented genuine vulnerability windows that simply were not exploited. Attackers may not have been active, attacks may have failed for other reasons, or proactive countermeasures may have prevented exploitation. This ambiguity makes false positive reduction challenging—we cannot definitively know whether a predicted window would have been exploited absent intervention.

\subsection{Lead Time Analysis}

Average lead time of 9.2 days provides substantial opportunity for intervention. However, lead time varies by pattern:

\begin{table}[h]
\centering
\caption{Lead time by vulnerability pattern}
\label{tab:leadtime}
\begin{tabular}{lrr}
\toprule
\textbf{Pattern} & \textbf{Average Lead Time} & \textbf{Range} \\
\midrule
Quarter-End Convergence & 12.3 days & 8-16 days \\
Burnout Cascade & 18.7 days & 14-28 days \\
Leadership Transition & 14.2 days & 10-21 days \\
Cross-Functional Storm & 8.4 days & 5-14 days \\
Alert Fatigue Spiral & 21.5 days & 15-35 days \\
Acquisition Integration & 24.8 days & 18-42 days \\
\bottomrule
\end{tabular}
\end{table}

Slowly developing patterns (burnout cascades, alert fatigue spirals, acquisition chaos) provide longer lead times as they emerge gradually over weeks or months. Acute patterns (cross-functional storms) provide shorter lead times but remain sufficient for meaningful intervention.

\subsection{Countermeasure Effectiveness}

To evaluate whether prediction enables effective prevention, we worked with Organization A to implement proactive countermeasures during the final six months of data collection. When the system predicted vulnerability windows, security teams activated appropriate countermeasures from our taxonomy.

\textbf{Results:} During the intervention period, 14 vulnerability windows were predicted. Proactive countermeasures were implemented for 11 of these. No security incidents occurred during windows with countermeasures, compared to 3 incidents during the control periods (windows without countermeasures due to resource constraints).

While the sample size remains small and causality is difficult to establish definitively, these results suggest that prediction enables effective prevention. Security teams reported high satisfaction with the proactive approach, noting reduced stress from reactive incident response and increased confidence in organizational security posture.

\section{Implementation Guidelines}

For organizations seeking to deploy ecological CPF monitoring, we provide practical guidance based on experience with pilot implementations.

\subsection{Data Requirements}

Effective organizational monitoring requires access to:

\textbf{Email System Logs:} Subject lines, sender patterns, user interaction data (opens, clicks, replies). Full content access is not required—metadata provides sufficient signal for most CPF indicators.

\textbf{Authentication Logs:} Login times, locations, success/failure patterns, unusual access patterns. These reveal temporal work patterns, stress indicators (late-night work), and behavioral anomalies.

\textbf{Collaboration Tool Data:} Communication volumes, meeting patterns, team interaction dynamics. Reveals social isolation, group dysfunction, and communication stress.

\textbf{HR System Data:} Organizational structure, reporting relationships, role changes, team assignments. Enables team-level aggregation and authority structure analysis.

\textbf{Organizational Calendar:} Deadlines, major events, planned changes, industry-specific cycles. Provides temporal context for vulnerability assessment.

\textbf{Incident History:} Past security incidents with timestamps and classifications. Enables pattern learning and prediction accuracy assessment.

Most organizations already collect this data for other purposes. The primary technical challenge involves aggregation and integration rather than new data collection.

\subsection{Privacy and Transparency}

Continuous psychological monitoring raises legitimate privacy concerns. We recommend:

\textbf{Aggregate Analysis:} Perform most analysis at team level rather than individual level. Team-level patterns provide sufficient predictive signal while reducing individual privacy concerns.

\textbf{Transparency:} Communicate openly with employees about what is monitored and why. Frame monitoring as organizational health assessment rather than individual surveillance.

\textbf{Human Oversight:} Require human review before any individual-level alerts or interventions. Automated analysis should inform human judgment, not replace it.

\textbf{Opt-Out Provisions:} Allow individuals to opt out of certain monitoring types while acknowledging this may limit the organization's ability to predict vulnerability.

\textbf{Purpose Limitation:} Use monitoring data exclusively for security and organizational health purposes. Explicitly prohibit use for performance evaluation or disciplinary actions.

\subsection{Incremental Deployment}

Organizations should deploy ecological monitoring incrementally:

\textbf{Phase 1 (Months 1-3):} Implement individual-level CPF monitoring with historical incident analysis to establish baselines and validate indicator accuracy.

\textbf{Phase 2 (Months 4-6):} Add team-level aggregation and pattern detection. Begin identifying historical patterns in organizational data.

\textbf{Phase 3 (Months 7-9):} Implement prediction capabilities for well-understood patterns (quarter-end convergence, leadership transitions). Begin proactive countermeasure experiments.

\textbf{Phase 4 (Months 10-12):} Full ecological monitoring with cross-team convergence detection, feedback loop identification, and comprehensive prediction across all pattern types.

This gradual approach allows organizational learning, trust building, and technical refinement before full deployment.

\subsection{Integration with Security Operations}

Ecological monitoring should integrate with existing security infrastructure:

\textbf{SIEM Integration:} Feed vulnerability predictions into security information and event management platforms as high-priority alerts with rich contextual information.

\textbf{Workflow Integration:} When vulnerability windows are predicted, automatically generate task tickets for security teams with recommended countermeasures and rationale.

\textbf{Briefing Automation:} Generate security briefings for affected teams explaining predicted vulnerabilities and appropriate vigilance measures.

\textbf{Metrics Dashboard:} Provide leadership with organizational health metrics derived from CPF monitoring, enabling strategic visibility into security posture evolution.

\section{Discussion and Future Work}

\subsection{From Reactive to Proactive Security}

This work represents a paradigm shift in security operations. Traditional security treats incidents as unpredictable events requiring rapid detection and response. Ecological CPF monitoring reveals that many incidents are highly predictable—emerging from organizational conditions visible days or weeks in advance.

This predictability enables proactive security: identifying vulnerability windows before exploitation and closing them through organizational interventions. The shift from reactive to proactive fundamentally changes security economics. Reactive security accepts that breaches will occur and focuses on minimizing damage. Proactive security prevents breaches by addressing root causes—the organizational psychological conditions that enable exploitation.

However, prediction is not prevention. Organizations must translate predictions into action through appropriate countermeasures, organizational changes, and cultural evolution. Technology enables visibility; organizational commitment enables effectiveness.

\subsection{Organizational Health as Security Metric}

Ecological monitoring reframes security as organizational health management. Rather than measuring security through breach rates and incident response times, organizations can monitor psychological indicators that predict future security posture.

This perspective aligns security with broader organizational objectives. Indicators predicting security vulnerability—burnout, excessive stress, poor work-life balance, cultural dysfunction—also predict productivity degradation, turnover, and organizational dysfunction. Addressing security vulnerabilities through organizational health improvements creates multiple benefits beyond security alone.

\subsection{Limitations}

Several limitations constrain the current approach. First, prediction accuracy remains imperfect at 78\%. While substantially better than reactive approaches, 22\% of incidents occur without advance warning. These incidents may result from external factors not captured in organizational monitoring, rapid-onset vulnerabilities that develop too quickly for prediction, or limitations in our indicator taxonomy.

Second, false positive rates of 29\% impose operational costs. Security teams must investigate predicted vulnerabilities even when exploitation does not occur. Organizations must balance prediction sensitivity (catching true vulnerabilities) against operational overhead (investigating false positives).

Third, the approach requires substantial data access and integration effort. Organizations with fragmented systems, limited logging, or strict data governance may struggle to implement comprehensive monitoring.

Fourth, causality remains challenging to establish definitively. When countermeasures prevent predicted incidents, we cannot prove whether the incident would have occurred absent intervention. This ambiguity makes it difficult to quantify return on investment precisely.

\subsection{Ethical Considerations}

Continuous psychological monitoring raises ethical concerns requiring careful attention. Employees may perceive monitoring as invasive surveillance threatening autonomy and privacy. Organizations must navigate the tension between legitimate security requirements and individual rights.

We recommend several ethical principles:

\textbf{Purpose Transparency:} Clearly communicate monitoring purposes, methods, and data usage. Avoid secret surveillance.

\textbf{Proportionality:} Limit monitoring to what is genuinely necessary for security. Avoid mission creep into performance evaluation or behavioral control.

\textbf{Individual Benefit:} Frame monitoring as benefiting employees by reducing burnout, identifying unhealthy organizational patterns, and creating safer work environments. Security monitoring should protect employees, not threaten them.

\textbf{Accountability:} Establish oversight mechanisms ensuring monitoring data is used appropriately. Independent review of monitoring practices protects against abuse.

\subsection{Integration with Agent Monitoring}

This paper focuses on organizational human factors. Our companion paper explores CPF-based agent self-monitoring \cite{canale2026agent}. Future work should explore integration of these approaches.

Organizations increasingly deploy LLM agents that interact with human employees. Comprehensive security requires monitoring both human and agent psychological states. An agent experiencing manipulation attempts while interacting with a burned-out employee creates compound vulnerability. Integration of individual agent monitoring, organizational human monitoring, and their interactions provides holistic visibility into human-agent ecosystem security.

\subsection{Future Directions}

Several promising research directions emerge:

\textbf{Federated Learning:} Organizations could share anonymized vulnerability patterns and prediction models without revealing sensitive data. Collective learning from multiple organizations would improve prediction accuracy while preserving privacy.

\textbf{Automated Countermeasures:} Current work focuses on prediction; future work could explore automated countermeasure deployment. When specific patterns are detected, systems could automatically implement appropriate safeguards (enhanced verification, dual-approval, monitoring adjustments) without human intervention.

\textbf{Cultural Health Metrics:} Develop comprehensive organizational health scores derived from CPF indicators that quantify security resilience. Leadership could track these metrics alongside financial and operational performance indicators.

\textbf{Resilience Engineering:} Explore how organizations can build psychological resilience that maintains security posture even during high-pressure periods. Rather than only predicting vulnerability, develop interventions that increase resistance to psychological exploitation.

\section{Conclusion}

Security incidents are not random events but predictable outcomes of organizational psychological conditions. By continuously monitoring the ecology of psychological factors that create vulnerability—stress, deadline pressure, authority dynamics, cognitive load, cultural patterns, and their interactions—organizations can predict vulnerability windows 5-14 days before exploitation with 78\% accuracy.

This predictive capability enables a fundamental shift from reactive incident response to proactive vulnerability management. Organizations can recognize emerging patterns, implement targeted countermeasures, and close vulnerability windows before attackers exploit them. The shift represents evolution from security as technical defense to security as organizational health management.

We have presented an ecological CPF architecture for organizational-level monitoring, identified six recurring vulnerability patterns with characteristic signatures and countermeasures, demonstrated prediction methodologies achieving practical lead times, and validated the approach on historical incident data from multiple organizations.

Our companion paper explores CPF-based agent self-monitoring, demonstrating how individual agents can detect their own psychological vulnerabilities. Together, these works—organizational ecology and agent immunity—provide comprehensive psychological security for human-agent ecosystems.

The future of security lies not only in better detection tools but in understanding the organizational and psychological conditions that determine when technical vulnerabilities become exploitable. This ecological perspective transforms security from a technical discipline into an organizational health practice, creating safer, more resilient, and ultimately more effective organizations.

\section*{Acknowledgments}

The authors thank the organizations that provided anonymized data enabling this research, the CPF research community for foundational theoretical work, and security practitioners who contributed insights from operational experience. We acknowledge helpful discussions with organizational psychologists exploring applications of CPF beyond security contexts.

\bibliographystyle{plain}
\begin{thebibliography}{99}

\bibitem{verizon2023}
Verizon. (2023). \textit{2023 Data Breach Investigations Report}. Verizon Enterprise Solutions.

\bibitem{bada2019}
Bada, M., Sasse, A. M., \& Nurse, J. R. C. (2019). Cyber security awareness campaigns: Why do they fail to change behaviour? \textit{Proc. 2019 Int. Conf. Cyber Security for Sustainable Society}, 118--131.

\bibitem{canale2025cpf}
Canale, G. (2025). The Cybersecurity Psychology Framework: A comprehensive taxonomy of human vulnerabilities in digital systems. Technical Report, FlowGuard Institute.

\bibitem{canale2025impl}
Canale, G., \& Thimmaraju, K. (2025). CPF implementation companion: Dense foundation paper. Technical Report, FlowGuard Institute.

\bibitem{chandola2009}
Chandola, V., Banerjee, A., \& Kumar, V. (2009). Anomaly detection: A survey. \textit{ACM Computing Surveys (CSUR)}, 41(3), Article 15, 1--58.

\bibitem{hadnagy2010}
Hadnagy, C. (2010). \textit{Social engineering: The art of human hacking}. Indianapolis, IN: Wiley Publishing.

\bibitem{stajano2011}
Stajano, F., \& Wilson, P. (2011). Understanding scam victims: Seven principles for systems security. \textit{Communications of the ACM}, 54(3), 70--75.

\bibitem{nurse2014}
Nurse, J. R. C., et al. (2014). Understanding insider threat: A framework for characterising attacks. \textit{2014 IEEE Security and Privacy Workshops}, 214--228.

\bibitem{sasse2001}
Sasse, M. A., Brostoff, S., \& Weirich, D. (2001). Transforming the 'weakest link'—a human/computer interaction approach to usable and effective security. \textit{BT Technology Journal}, 19(3), 122--131.

\bibitem{parsons2014}
Parsons, K., et al. (2014). Determining employee awareness using the Human Aspects of Information Security Questionnaire (HAIS-Q). \textit{Computers \& Security}, 42, 165--176.

\bibitem{cialdini2007}
Cialdini, R. B. (2007). \textit{Influence: The psychology of persuasion} (Revised edition). New York: Harper Business.

\bibitem{kahneman2011}
Kahneman, D. (2011). \textit{Thinking, fast and slow}. New York: Farrar, Straus and Giroux.

\bibitem{maslach2001}
Maslach, C., Schaufeli, W. B., \& Leiter, M. P. (2001). Job burnout. \textit{Annual Review of Psychology}, 52(1), 397--422.

\bibitem{bion1961}
Bion, W. R. (1961). \textit{Experiences in groups and other papers}. London: Tavistock Publications.

\bibitem{meadows2008}
Meadows, D. H. (2008). \textit{Thinking in systems: A primer}. White River Junction, VT: Chelsea Green Publishing.

\bibitem{hollnagel2011}
Hollnagel, E., Pariès, J., Woods, D. D., \& Wreathall, J. (Eds.). (2011). \textit{Resilience engineering in practice: A guidebook}. Farnham, UK: Ashgate Publishing.

\bibitem{chismon2015}
Chismon, D., \& Ruks, M. (2015). Threat intelligence: Collecting, analysing, evaluating. \textit{MWR InfoSecurity}.

\bibitem{kandias2010}
Kandias, M., et al. (2010). An insider threat prediction model. \textit{Trust, Privacy and Security in Digital Business: 7th International Conference, TrustBus 2010}, 26--37.

\bibitem{canale2026agent}
Canale, G., \& Thimmaraju, K. (2026). CPF for agent immunity: Self-monitoring against manipulation. \textit{arXiv preprint arXiv:2601.xxxxx}.

\end{thebibliography}

\end{document}
