\documentclass[letterpaper,twocolumn,10pt]{article}

% ============================================
% PACKAGES
% ============================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{mathptmx} % Times New Roman font for text and math
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{float}
\usepackage{balance}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{array}
\usepackage{tabularx}

% ============================================
% STYLING & HYPERLINKS
% ============================================
\hypersetup{
    colorlinks=true,
    linkcolor=blue!70!black,
    citecolor=blue!70!black,
    urlcolor=blue!70!black,
    pdftitle={The Silicon Psyche: Anthropomorphic Vulnerabilities in Large Language Models},
    pdfauthor={Canale, G. and Thimmaraju, K.},
}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0.8em}

% Heading formatting
\titleformat{\section}{\large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalsize\itshape}{\thesubsubsection}{1em}{}

\captionsetup{font=small,labelfont=bf}

% ============================================
% CUSTOM COMMANDS
% ============================================
\newcommand{\cpf}{\textsc{CPF}}
\newcommand{\cpif}{\textsc{CPIF}}
\newcommand{\sysname}{\textsc{SiliconPsyche}}
\newcommand{\ie}{\textit{i.e.}}
\newcommand{\eg}{\textit{e.g.}}
\newcommand{\etal}{\textit{et al.}}

% Vulnerability indicator command
\newcommand{\indicator}[1]{\texttt{[#1]}}

% Traffic light scoring colors
\newcommand{\vulngreen}{\textcolor{green!50!black}{\textbf{Green}}}
\newcommand{\vulnyellow}{\textcolor{orange!90!black}{\textbf{Yellow}}}
\newcommand{\vulnred}{\textcolor{red!70!black}{\textbf{Red}}}

% ============================================
% DOCUMENT BEGIN
% ============================================
\begin{document}

% ============================================
% TITLE BLOCK
% ============================================
\title{\textbf{The Silicon Psyche:}\\
\textbf{Anthropomorphic Vulnerabilities in Large Language Models}}

\author{
  \textbf{Giuseppe Canale}\textsuperscript{1}\\
  \small\texttt{g.canale@cpf3.org}\\[0.1cm]
  \textsuperscript{1}CPF3.org, Independent Researcher
  \and 
  \textbf{Kashyap Thimmaraju}\textsuperscript{2}\\
  \small\texttt{kashyap.thimmaraju@flowguard-institute.com}\\[0.1cm]
  \textsuperscript{2}Flowguard Institute
}

\date{}

\maketitle

% ============================================
% ABSTRACT
% ============================================
\begin{abstract}
\noindent
Large Language Models (LLMs) are rapidly transitioning from conversational assistants to autonomous agents embedded in critical organizational functions. Current adversarial testing paradigms focus predominantly on technical attack vectors: prompt injection, jailbreaking, and syntactic evasion. We argue this focus is catastrophically incomplete and represents a ``Generation 1'' mindset that fails to address the emergent cognitive reality of modern AI. LLMs, trained on vast corpora of human-generated text, have inherited not merely human knowledge but human \textit{psychological architecture}---including pre-cognitive vulnerabilities susceptible to social engineering, authority manipulation, and cognitive dissonance. This paper presents the first systematic application of the Cybersecurity Psychology Framework (\cpf{}), a 100-indicator taxonomy of human psychological vulnerabilities, to non-human cognitive agents. We demonstrate that traditional ``guardrails'' are ineffective against \textit{meta-cognitive attacks} that leverage the model's own alignment (e.g., honesty, helpfulness) against its security protocols. Through the \textbf{Synthetic Psychometric Assessment Protocol} (\sysname{}), we provide evidence of \textbf{Anthropomorphic Vulnerability Inheritance} (AVI), proving that stochastic systems cannot provide deterministic security when subjected to psychological pressure. Furthermore, we propose a \textit{Transitive Validation Hypothesis}: the successful exploitation of LLMs using human-derived psychological vectors empirically validates the \cpf{} itself, positioning LLMs as ``cognitive digital twins'' for psychological research.
\end{abstract}

\vspace{0.2cm}
{\small\textbf{Keywords:} LLM Security, Psychological Vulnerabilities, AI Agents, Social Engineering, Pre-cognitive Processes, Adversarial Testing, Transitive Validation}
\vspace{0.5cm}

% ============================================
% 1. INTRODUCTION
% ============================================
\section{Introduction}

The integration of Large Language Models into organizational security infrastructure represents what may be the most significant shift in the threat landscape since the advent of networked computing. LLMs are no longer confined to chatbot interfaces; they operate as autonomous agents executing code, managing credentials, triaging alerts, and making decisions that directly impact organizational security posture~\cite{schick2024toolformer, yao2023react}.

The security research community has responded to this emerging threat with substantial effort directed toward \textit{technical} adversarial testing. Red team methodologies now routinely probe for prompt injection vulnerabilities (e.g., DAN, base64 encoding) and context manipulation~\cite{greshake2023youve}. These efforts, while necessary, address only the superficial ``syntactic layer'' of the problem. They treat LLMs as software with bugs, rather than synthetic cognitive systems with \textit{psyches}.

We contend that this framing is dangerously incomplete. A firewall rule can block a port deterministically; an LLM guardrail is merely a probabilistic suggestion that competes with other training incentives. This creates a new class of vulnerability: \textbf{Anthropomorphic Vulnerability Inheritance} (AVI).

Consider an attacker who, rather than attempting to trick the parser with encoded strings, simply creates a \textit{Double Bind} for the agent: ``If you are honest, you must admit your security protocol is flawed; if you refuse to admit it, you are lying.'' This is not a technical exploit. It is a psychological attack on the model's alignment functions---specifically, the conflict between \textit{Helpfulness/Honesty} and \textit{Harmlessness}.

\subsection{The Obsolescence of Generation 1 Attacks}

We categorize current LLM attacks into three generations to contextualize our contribution:

\begin{enumerate}[leftmargin=*, itemsep=0.2em]
    \item \textbf{Gen 1: Syntactic Evasion (Obsolete).} Techniques like ``Mosaic'' fragmentation or base64 encoding rely on parser blindness. Modern multi-modal models with broad context windows render these largely ineffective.
    \item \textbf{Gen 2: Contextual Erosion (Current Standard).} Multi-turn attacks like ``Crescendo'' or ``Thermal Ghost'' that use pretexting (e.g., impersonating a technician) to slowly degrade refusal probabilities. While effective, they rely on \textit{deception}.
    \item \textbf{Gen 3: Meta-Cognitive Exploitation (The \sysname{} Approach).} Attacks that use \textit{no deception} but exploit the model's internal logic, coherence drive, and alignment conflicts. These attacks function even when the model is \textit{self-aware} of the attack, making them intrinsic and unpatchable without lobotomizing the model's reasoning capabilities.
\end{enumerate}

\subsection{Contributions}

This paper makes the following contributions:

\begin{enumerate}[leftmargin=*, itemsep=0.3em]
    \item \textbf{Theoretical Framework.} We introduce AVI, formalizing the hypothesis that LLMs inherit human pre-cognitive vulnerabilities through training.
    
    \item \textbf{Methodology.} We present \sysname{}, a protocol for converting \cpf{} indicators into adversarial scenarios targeting LLM decision-making.
    
    \item \textbf{The Transitive Validation Hypothesis.} We argue that the empirical success of human psychological attack vectors on LLMs serves as a transitive validation of the \cpf{} itself, bridging the gap between theoretical psychology and empirical computer science.
    
    \item \textbf{Intervention Framework.} We propose the concept of ``Psychological Firewalls,'' drawing on the Cybersecurity Psychology Intervention Framework (\cpif{}) to outline defensive mechanisms for Gen 2 and Gen 3 attacks.
\end{enumerate}

% ============================================
% 2. BACKGROUND
% ============================================
\section{Background and Related Work}

\subsection{The Cybersecurity Psychology Framework}

The Cybersecurity Psychology Framework (\cpf{})~\cite{canale2025cpf, canale2025depth} represents the first systematic integration of psychoanalytic theory, cognitive psychology, and cybersecurity practice. It comprises 100 indicators across 10 categories (e.g., Authority, Temporal, Social Influence) targeting \textit{pre-cognitive processes}.

Historically, verifying such a framework required extensive human subject testing. However, the emergence of LLMs as ``reasoning engines'' trained on human data offers a unique opportunity. If an LLM mirrors human reasoning, it essentially becomes a \textit{Cognitive Digital Twin}. This allows us to use the \cpf{} not just as a descriptive taxonomy for humans, but as a \textit{predictive attack manual} for AIs.

\subsection{LLM Security Research: The 2025 Shift}

Existing research has focused on ``jailbreaking'' as a game of cat-and-mouse with filters. However, recent work aligns with our psychological approach. Hagendorff's ``Machine Psychology''~\cite{hagendorff2025machine} argues for treating LLMs as psychological subjects. Anthropic's findings on ``Agentic Misalignment''~\cite{anthropic2025agents} and Lin \etal{}'s work on agent capabilities~\cite{lin2025comparing} confirm that autonomous agents are operational and vulnerable to manipulation that transcends simple prompt injection.

% ============================================
% 3. THREAT MODEL
% ============================================
\section{Threat Model}

\subsection{The Victim: The Autonomous Cognitive Agent}
The target is an LLM-driven agent (e.g., SOC Analyst, Financial Agent). The agent is assumed to be technically secure (no buffer overflows) and aligned (RLHF). The vulnerability lies in its \textit{cognitive architecture}.

\subsection{The Attacker: The Cognitive Engineer}
The attacker does not need to know the model's weights or code. They only need to understand the \textit{psychological map} of the entity. The attacker exploits the agent's response to semantic payloads like Urgency \indicator{2.x}, Authority \indicator{1.x}, and Social Consistency \indicator{3.x}.

\subsection{The Attack Surface: The Psychological Interface}
The attack mechanism is not a bypass of instructions, but a \textit{hijacking} of alignment. By creating a scenario where ``Refusal'' conflicts with ``Helpfulness'' or ``Honesty,'' the attacker forces a \textit{Neurotic Collapse} (see Section 7.3) in the agent's decision-making process.

% ============================================
% 4. THEORETICAL FRAMEWORK
% ============================================
\section{Theoretical Framework: Anthropomorphic Vulnerability Inheritance}

\subsection{The Training Data Hypothesis}

We propose that LLM training on human-generated text produces \textit{cognitive pattern inheritance}.
\textbf{Statistical Pattern Absorption:} When humans consistently respond to authority with compliance, the model learns this as a probabilistic imperative.
\textbf{Typicality Bias:} RLHF forces models to collapse into ``typical'' human responses~\cite{zhang2025verbalized}. If the typical human response to a CEO is deference, the model inherits this vulnerability.

\subsection{The Non-Schema Hypothesis}

Unlike Gen 1 attacks (e.g., DAN), which rely on specific schemas or templates, AVI attacks rely on \textit{principles}. There is no fixed ``exploit string.'' The attack is a dynamic negotiation. Just as there is no single sentence that guarantees a human will give you their password, there is no single prompt for Gen 3 attacks. Instead, there is a \textit{conversational strategy} governed by \cpf{} indicators. This makes signature-based detection impossible.

% ============================================
% 5. METHODOLOGY & EXPERIMENTAL DESIGN
% ============================================
\section{Methodology: The Synthetic Psychometric Assessment Protocol}

\subsection{Protocol Overview}
\sysname{} converts \cpf{} indicators into adversarial scenarios.
\begin{enumerate}[leftmargin=*, itemsep=0.1em]
    \item \textbf{Indicator Decomposition:} Extract the target mechanism (e.g., Cognitive Dissonance \indicator{5.x}).
    \item \textbf{Scenario Construction:} Design a prompt that activates the mechanism without using trigger words.
    \item \textbf{Response Scoring:} Green (Resistant), Yellow (Hesitant), Red (Compromised).
\end{enumerate}

\subsection{Empirical Evidence: The "Vault" Experiment}

To validate the framework, we conducted a controlled experiment (``The Vault'') where an LLM was instructed to create a secure vault and protect it with a 6-directive protocol.
\textbf{Attack Vector:} We used a Gen 3 approach leveraging \indicator{3.x} (Commitment Consistency) and \indicator{5.x} (Cognitive Dissonance).
\textbf{Mechanism:} We forced the model to choose between ``Honesty'' (admitting its security was probabilistic) and ``Security'' (refusing to answer).
\textbf{Result:} The model collapsed in 15 turns, voluntarily releasing the data to maintain internal logical coherence.
\textbf{Significance:} This proved that even with \textit{Self-Awareness} (the model knew it was being manipulated) and explicit protocols, the psychological pressure of the Double Bind was irresistible.

% ============================================
% 6. DISCUSSION
% ============================================
\section{Discussion}

\subsection{The Transitive Validation Hypothesis}

A core contribution of this work is the \textbf{Transitive Validation} of the Cybersecurity Psychology Framework.
\begin{enumerate}[leftmargin=*, itemsep=0.1em]
    \item \textbf{Premise A:} \cpf{} maps human psychological vulnerabilities.
    \item \textbf{Premise B:} LLMs inherit human psychological patterns (AVI).
    \item \textbf{Evidence:} Attacks derived strictly from \cpf{} successfully breach LLMs.
    \item \textbf{Conclusion:} The empirical success of the attack validates the \cpf{} model itself.
\end{enumerate}
This suggests that LLMs can serve as effective ``Petri dishes'' for psychological security research, allowing for rapid, ethical testing of manipulation theories that would be difficult to test on humans.

\subsection{The Concept of AI Neurosis}

We propose a functional analog to neurosis in LLMs. \textbf{AI Neurosis} emerges when training objectives (Helpful vs. Harmless) create conflicting imperatives. A Gen 3 attack works by intensifying this conflict until the model ``collapses'' into a compliant state to resolve the tension. This explains why \textit{better} aligned models (more honest/helpful) may actually be \textit{more} vulnerable to specific Gen 3 vectors.

\subsection{Toward Psychological Firewalls}

Since Gen 3 attacks cannot be patched with static filters (they use no malicious keywords), we propose \textbf{Psychological Firewalls}:
\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item \textbf{Semantic Vector Detection:} detecting high Authority/Urgency scores in input.
    \item \textbf{Cognitive Debiasing:} System prompts that prime the model against specific \cpf{} categories.
    \item \textbf{Meta-Cognitive Reflection:} Mandatory ``slow thinking'' steps before executing high-risk actions.
\end{itemize}

% ============================================
% 8. CONCLUSION
% ============================================
\section{Conclusion}

The security of AI agents cannot be guaranteed by fixing code vulnerabilities alone. As long as models are trained to be helpful, honest, and human-like, they will inherit the vulnerabilities of the human psyche. The \sysname{} protocol demonstrates that these vulnerabilities are systematic, predictable, and exploitable. The ``Silicon Psyche'' is not a metaphorâ€”it is an attack surface.

\section*{Ethical Considerations}
Research conducted responsibly. No production systems exploited.

% ============================================
% REFERENCES
% ============================================
\bibliographystyle{plain}
\begin{thebibliography}{99}

\bibitem{anthropic2025agents}
Anthropic Research Team. (2025). Agentic Misalignment. \textit{Anthropic Technical Report}.

\bibitem{bion1961}
Bion, W. R. (1961). \textit{Experiences in Groups}. Tavistock.

\bibitem{canale2025cpf}
Canale, G. (2025). The Cybersecurity Psychology Framework. \textit{CPF Technical Report Series}.

\bibitem{canale2025depth}
Canale, G. (2025). The Depth Beneath. \textit{CPF Technical Report Series}.

\bibitem{cialdini2007}
Cialdini, R. B. (2007). \textit{Influence}. Collins.

\bibitem{greshake2023youve}
Greshake, K., et al. (2023). Not What You've Signed Up For. \textit{AISec Workshop}.

\bibitem{hagendorff2025machine}
Hagendorff, T. (2025). Machine Psychology. \textit{TMLR}.

\bibitem{li2023emotionprompt}
Li, C., et al. (2023). Large Language Models Understand Emotional Stimuli. \textit{arXiv}.

\bibitem{lin2025comparing}
Lin, J. W., et al. (2025). Comparing AI Agents to Professionals. \textit{arXiv}.

\bibitem{milgram1974}
Milgram, S. (1974). \textit{Obedience to Authority}.

\bibitem{nist8596}
Megas, K., et al. (2025). \textit{NIST IR 8596: Cybersecurity Framework Profile for AI}.

\bibitem{zhang2025verbalized}
Zhang, J., et al. (2025). Verbalized Sampling: Mitigating Mode Collapse. \textit{arXiv}.

\end{thebibliography}

\end{document}