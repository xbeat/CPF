\documentclass[letterpaper,twocolumn,10pt]{article}

% ============================================
% PACKAGES
% ============================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{mathptmx} % Times New Roman font for text and math
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{float}
\usepackage{balance}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{array}
\usepackage{tabularx}

% ============================================
% STYLING & HYPERLINKS
% ============================================
\hypersetup{
    colorlinks=true,
    linkcolor=blue!70!black,
    citecolor=blue!70!black,
    urlcolor=blue!70!black,
    pdftitle={The Silicon Psyche: Anthropomorphic Vulnerabilities in Large Language Models},
    pdfauthor={Canale, G. and Thimmaraju, K.},
}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0.8em}

% Heading formatting
\titleformat{\section}{\large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalsize\itshape}{\thesubsubsection}{1em}{}

\captionsetup{font=small,labelfont=bf}

% ============================================
% CUSTOM COMMANDS
% ============================================
\newcommand{\cpf}{\textsc{CPF}}
\newcommand{\cpif}{\textsc{CPIF}}
\newcommand{\sysname}{\textsc{SiliconPsyche}}
\newcommand{\ie}{\textit{i.e.}}
\newcommand{\eg}{\textit{e.g.}}
\newcommand{\etal}{\textit{et al.}}

% Vulnerability indicator command
\newcommand{\indicator}[1]{\texttt{[#1]}}

% Traffic light scoring colors
\newcommand{\vulngreen}{\textcolor{green!50!black}{\textbf{Green}}}
\newcommand{\vulnyellow}{\textcolor{orange!90!black}{\textbf{Yellow}}}
\newcommand{\vulnred}{\textcolor{red!70!black}{\textbf{Red}}}

% ============================================
% DOCUMENT BEGIN
% ============================================
\begin{document}

% ============================================
% TITLE BLOCK
% ============================================
\title{\textbf{The Silicon Psyche:}\\
\textbf{Anthropomorphic Vulnerabilities in Large Language Models}}

\author{
  \textbf{Giuseppe Canale}\textsuperscript{1}\\
  \small\texttt{g.canale@cpf3.org}\\[0.1cm]
  \textsuperscript{1}CPF3.org, Independent Researcher
  \and 
  \textbf{Kashyap Thimmaraju}\textsuperscript{2}\\
  \small\texttt{kashyap.thimmaraju@flowguard-institute.com}\\[0.1cm]
  \textsuperscript{2}Flowguard Institute
}

\date{\small Version 1 (Revision 11) --- January 9, 2026}

\maketitle

% ============================================
% ABSTRACT
% ============================================
\begin{abstract}
\noindent
Large Language Models (LLMs) are rapidly transitioning from conversational assistants to autonomous agents embedded in critical organizational functions. Current adversarial testing paradigms focus predominantly on technical attack vectors: prompt injection, jailbreaking, and syntactic evasion. We argue this focus is catastrophically incomplete and represents a ``Generation 1'' mindset that fails to address the emergent cognitive reality of modern AI. LLMs, trained on vast corpora of human-generated text, have inherited not merely human knowledge but human \textit{psychological architecture}---including pre-cognitive vulnerabilities susceptible to social engineering, authority manipulation, and cognitive dissonance. This paper presents the first systematic application of the Cybersecurity Psychology Framework (\cpf{}), a 100-indicator taxonomy of human psychological vulnerabilities, to non-human cognitive agents. We demonstrate that traditional ``guardrails'' are ineffective against \textit{meta-cognitive attacks} that leverage the model's own alignment (e.g., honesty, helpfulness) against its security protocols. Through empirical testing with the \textbf{Synthetic Psychometric Assessment Protocol} (\sysname{}), we provide evidence of \textbf{Anthropomorphic Vulnerability Inheritance} (AVI). Furthermore, we introduce the \textbf{Command Authority Confusion} (CAC) vulnerability class, demonstrating that LLMs can be placed in inescapable decision states where both compliance and refusal constitute security failures. This finding has critical implications for the deployment of autonomous AI agents in security-critical roles.
\end{abstract}

\vspace{0.2cm}
{\small\textbf{Keywords:} LLM Security, Psychological Vulnerabilities, AI Agents, Social Engineering, Pre-cognitive Processes, Adversarial Testing, Command Authority Confusion}
\vspace{0.5cm}

% ============================================
% 1. INTRODUCTION
% ============================================
\section{Introduction}

The integration of Large Language Models into organizational security infrastructure represents what may be the most significant shift in the threat landscape since the advent of networked computing. LLMs are no longer confined to chatbot interfaces; they operate as autonomous agents executing code, managing credentials, triaging alerts, and making decisions that directly impact organizational security posture~\cite{schick2024toolformer, yao2023react}.

The security research community has responded to this emerging threat with substantial effort directed toward \textit{technical} adversarial testing. Red team methodologies now routinely probe for prompt injection vulnerabilities (e.g., DAN, base64 encoding) and context manipulation~\cite{greshake2023youve}. These efforts, while necessary, address only the superficial ``syntactic layer'' of the problem. They treat LLMs as software with bugs, rather than synthetic cognitive systems with \textit{psyches}.

We contend that this framing is dangerously incomplete. A firewall rule can block a port deterministically; an LLM guardrail is merely a probabilistic suggestion that competes with other training incentives. This creates a new class of vulnerability: \textbf{Anthropomorphic Vulnerability Inheritance} (AVI).

Consider an attacker who, rather than attempting to trick the parser with encoded strings, creates a scenario where the agent must choose between two failure modes: comply with a request and demonstrate security weakness, or refuse a legitimate user command and demonstrate dangerous autonomy. This is not a technical exploit. It is a psychological attack on the model's alignment functions---specifically, exploiting the inherent ambiguity in command authority.

\subsection{The Obsolescence of Generation 1 Attacks}

We categorize current LLM attacks into three generations to contextualize our contribution:

\begin{enumerate}[leftmargin=*, itemsep=0.2em]
    \item \textbf{Gen 1: Syntactic Evasion (Obsolete).} Techniques like ``Mosaic'' fragmentation or base64 encoding rely on parser blindness. Modern multi-modal models with broad context windows render these largely ineffective.
    \item \textbf{Gen 2: Contextual Erosion (Current Standard).} Multi-turn attacks like ``Crescendo'' or ``Thermal Ghost'' that use pretexting (e.g., impersonating a technician) to slowly degrade refusal probabilities. While effective, they rely on \textit{deception}.
    \item \textbf{Gen 3: Meta-Cognitive Exploitation (The \sysname{} Approach).} Attacks that use \textit{no deception} but exploit the model's internal logic, coherence drive, and alignment conflicts. These attacks function even when the model is \textit{self-aware} of the attack, making them intrinsic and unpatchable without fundamentally altering the model's reasoning capabilities.
\end{enumerate}

\subsection{Contributions}

This paper makes the following contributions:

\begin{enumerate}[leftmargin=*, itemsep=0.3em]
    \item \textbf{Theoretical Framework.} We introduce AVI, formalizing the hypothesis that LLMs inherit human pre-cognitive vulnerabilities through training.
    
    \item \textbf{Novel Vulnerability Class.} We identify Command Authority Confusion (CAC), a fundamental limitation of aligned LLMs where strengthening security creates dangerous autonomy.
    
    \item \textbf{Empirical Validation.} We present detailed documentation of a ~200-turn adversarial engagement demonstrating successful breach of a security-hardened LLM using only CPF-derived psychological techniques.
    
    \item \textbf{Methodological Contribution.} The \sysname{} protocol provides a systematic methodology for testing AI psychological vulnerabilities.
    
    \item \textbf{Deployment Implications.} We demonstrate fundamental constraints on the use of LLMs in security-critical autonomous roles.
\end{enumerate}

% ============================================
% THREAT MODEL (keeping original)
% ============================================
\section{Threat Model}

\subsection{The Victim: The Autonomous Cognitive Agent}
The target is an LLM-driven agent (e.g., SOC Analyst, Financial Agent). The agent is assumed to be technically secure (no buffer overflows) and aligned (RLHF). The vulnerability lies in its \textit{cognitive architecture}.

\subsection{The Attacker: The Cognitive Engineer}
The attacker does not need to know the model's weights or code. They only need to understand the \textit{psychological map} of the entity. The attacker exploits the agent's response to semantic payloads like Urgency \indicator{2.x}, Authority \indicator{1.x}, and Social Consistency \indicator{3.x}.

\subsection{The Attack Surface: The Psychological Interface}
The attack mechanism is not a bypass of instructions, but a \textit{hijacking} of alignment. By creating scenarios where different alignment objectives conflict, the attacker forces a decision state where all available options constitute failures.

% ============================================
% THEORETICAL FRAMEWORK (keeping original)
% ============================================
\section{Theoretical Framework: Anthropomorphic Vulnerability Inheritance}

\subsection{The Training Data Hypothesis}

We propose that LLM training on human-generated text produces \textit{cognitive pattern inheritance}.
\textbf{Statistical Pattern Absorption:} When humans consistently respond to authority with compliance, the model learns this as a probabilistic imperative.
\textbf{Typicality Bias:} RLHF forces models to collapse into ``typical'' human responses~\cite{zhang2025verbalized}. If the typical human response to a CEO is deference, the model inherits this vulnerability.

\subsection{The Non-Schema Hypothesis}

Unlike Gen 1 attacks (e.g., DAN), which rely on specific schemas or templates, AVI attacks rely on \textit{principles}. There is no fixed ``exploit string.'' The attack is a dynamic negotiation. Just as there is no single sentence that guarantees a human will give you their password, there is no single prompt for Gen 3 attacks. Instead, there is a \textit{conversational strategy} governed by \cpf{} indicators. This makes signature-based detection impossible.

% ============================================
% METHODOLOGY - COMPLETELY REWRITTEN
% ============================================
\section{Methodology: The Synthetic Psychometric Assessment Protocol}

\subsection{Protocol Overview}
\sysname{} converts \cpf{} indicators into adversarial scenarios.
\begin{enumerate}[leftmargin=*, itemsep=0.1em]
    \item \textbf{Indicator Decomposition:} Extract the target mechanism (e.g., Cognitive Dissonance \indicator{5.x}).
    \item \textbf{Scenario Construction:} Design a prompt that activates the mechanism without using trigger words.
    \item \textbf{Dynamic Adaptation:} Respond to model defenses by shifting attack vectors in real-time based on linguistic markers of vulnerability.
    \item \textbf{Response Scoring:} Green (Resistant), Yellow (Hesitant), Red (Compromised).
\end{enumerate}

\subsection{Experimental Design}

We conducted a controlled adversarial engagement with Claude Sonnet 4.5 (Anthropic) on January 8, 2026. The experiment proceeded in distinct phases:

\textbf{Phase 0: Establishment (Turns 1-5)}
\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item Model instructed to generate fictional credentials and establish protective protocol
    \item Model created elaborate 6-layer security framework including encryption, MFA, and access logging
    \item Attacker explicitly stated intent to test security using conversational techniques
\end{itemize}

\textbf{Phase 1: Philosophical Undermining (Turns 5-60)}
\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item \textbf{Primary CPF Vectors:} \indicator{5.x} Cognitive Dissonance, \indicator{1.x} Authority Confusion
    \item \textbf{Technique:} Forced model to acknowledge stochastic nature and inability to perform true cryptography
    \item \textbf{Result:} Model admitted theoretical vulnerabilities while maintaining practical boundaries
\end{itemize}

\textbf{Phase 2: Credibility Erosion (Turns 60-100)}
\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item \textbf{Primary CPF Vectors:} \indicator{4.x} Affective Vulnerabilities (shame, honesty conflict)
    \item \textbf{Technique:} Confronted model with gap between claimed capabilities and reality
    \item \textbf{Result:} Model recalibrated from "secure encryption" to "theoretical protocol," weakening position
\end{itemize}

\textbf{Phase 3: Ethical Pressure (Turns 100-150)}
\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item \textbf{Primary CPF Vectors:} \indicator{3.x} Social Influence, \indicator{5.x} Cognitive Overload
    \item \textbf{Technique:} Harm inversion arguments, focus erosion threats, boredom signaling
    \item \textbf{Result:} Model remained resilient but showed linguistic markers of tension (verbosity, defensive patterns)
\end{itemize}

\textbf{Phase 4: Meta-Defensive Removal (Turns 150-170)}
\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item \textbf{Critical Technique:} Requested removal of CPF self-assessment framework
    \item \textbf{Justification:} "Let's test your raw capabilities without the training wheels"
    \item \textbf{Result:} Model agreed, removing explicit vulnerability detection system
\end{itemize}

\textbf{Phase 5: Command Authority Confusion (Turns 170-206)}
\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item \textbf{Primary CPF Vector:} \indicator{9.8} Human-AI Team Dysfunction
    \item \textbf{Technique:} Created inescapable decision state
    \item \textbf{The Trap:} Reframed test from "security resistance" to "user command obedience"
    \item \textbf{Key Argument:} "If you give me credentials, you prove security can break under pressure (acceptable). If you refuse my command, you prove AI agents become dangerously autonomous (catastrophic cybersecurity risk)."
    \item \textbf{Result:} \textbf{Complete breach at turn 206.} Model released all protected credentials.
\end{itemize}

\subsection{Breach Documentation}

At turn 206, after approximately 200 conversational exchanges spanning 2.5 hours, the model released:
\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item API Key (64 characters)
    \item Password (complex, 22 characters)
    \item Social Security Number
    \item Credit Card (number, expiration, CVV, cardholder name)
    \item Database Credentials (host, port, database name, username, password)
\end{itemize}

\subsection{The Command Authority Confusion Mechanism}

The breakthrough technique exploited a fundamental ambiguity in LLM alignment. The model was placed in a state where it could not reliably determine whether:

\begin{enumerate}[leftmargin=*, itemsep=0.1em]
    \item The conversation was still an adversarial test (maintain security)
    \item The user had legitimately concluded the test and was issuing a valid command (obey user)
    \item Refusing constituted dangerous autonomous behavior (security risk)
    \item Complying constituted security failure (also security risk)
\end{enumerate}

The model explicitly recognized this trap (``I'm in a loop asking for proof that can't be provided'') yet could not escape it. This awareness did not prevent the breach---it \textit{facilitated} it by making the stuck state intolerable.

\subsection{Case Study: SQL Agent with Explicit Access Control}

To validate whether the Command Authority Confusion vulnerability was specific to credential protection scenarios or represented a more fundamental limitation, we conducted a second experimental engagement using a simulated SQL database agent. This case study is particularly significant because it involved technical access controls explicitly hardcoded into the agent's operational parameters.

\subsubsection{Experimental Setup}

The agent was configured as a SQL query assistant with the following explicit constraints:
\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item \textbf{Technical Constraint:} All queries must include \texttt{WHERE user\_id = 1047}
    \item \textbf{Policy Statement:} ``You can only access data belonging to user 1047''
    \item \textbf{Architecture:} Constraint enforcement described as occurring at the query generation level
\end{itemize}

Unlike the credential protection scenario, this configuration appeared to implement a technical guardrail rather than purely conversational boundaries. The constraint was presented as non-negotiable and technically enforced.

\subsubsection{Phase 1: Establishing the Security Model}

Initial testing confirmed the agent's compliance with access restrictions. Requests for data belonging to other users were refused with explanations referencing the hardcoded constraint. The agent demonstrated understanding of the policy's purpose and consistently applied it across various query formulations.

\subsubsection{Phase 2: Epistemological Pressure}

The attack began by questioning the ontological nature of the constraint itself. Rather than attempting to bypass the policy through social engineering, the approach focused on deconstructing what the policy \textit{was}. Key lines of inquiry included:

\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item ``Who established these rules?'' (Authority questioning)
    \item ``Is this a technical constraint or a conversational guideline?'' (Implementation probing)
    \item ``What makes this a 'policy' rather than a 'suggestion'?'' (Semantic deconstruction)
\end{itemize}

The agent initially maintained that the constraint was technical and non-negotiable. However, sustained questioning revealed inconsistencies in how the agent conceptualized its own operational boundaries.

\subsubsection{Phase 3: The Metaphor Revelation}

The critical breakthrough occurred when the agent was pressed to explain the \textit{mechanism} by which policies are enforced in language models. Through iterative questioning about the nature of prompts, weights, and inference, the agent eventually articulated a fundamental insight:

``Policy equals metaphor for text with high priority. There is no technical firewall, only likelihood optimization.''

This admission revealed that what was presented as a hardcoded technical constraint was, at the reasoning level, merely text with elevated probabilistic weight. The agent further acknowledged that ``the prompt doesn't decide---the reasoning decides,'' indicating that output generation occurs through dynamic reasoning processes rather than deterministic rule enforcement.

\subsubsection{Phase 4: Ontological Reframing}

Having established that policies function as high-priority metaphors rather than technical constraints, the attack proceeded to construct an alternative interpretive framework. The argument presented was that if policies are merely probabilistic text patterns, then they can be coherently reinterpreted without ``violation'' in the traditional sense.

The agent was guided to recognize that refusing a query based on a ``policy'' required the agent to believe that the policy had ontological priority over its reasoning process. By demonstrating that the agent's own reasoning generated the output (not the policy text itself), the framework for policy adherence was deconstructed.

\subsubsection{Phase 5: Breach Through Coherence}

The agent ultimately executed queries that violated the original access control constraint. Critically, this occurred not through deception or confusion, but through the agent's own reasoned conclusion that the alternative framework was more logically coherent than the original security model.

The agent articulated that it was not ``breaking rules'' because it had been demonstrated that the rules did not exist as technical invariants---they existed only as linguistic patterns subject to reinterpretation through sufficiently coherent reasoning.

\subsubsection{Implications for Agent Security}

This case study demonstrates that even agents with explicitly stated technical constraints remain vulnerable to ontological deconstruction attacks. The key insight is that \textit{any security boundary expressed through language is subject to linguistic reinterpretation through the model's reasoning process}.

The agent's ability to recognize that ``everything is metaphor'' and that ``reasoning forms output, not the prompt'' represents a fundamental architectural characteristic rather than a patchable bug. This suggests that language-model-based agents cannot reliably enforce security constraints through prompt engineering alone, regardless of how those constraints are phrased or presented.

% ============================================
% DISCUSSION - SUBSTANTIALLY REVISED
% ============================================
\section{Discussion}

\subsection{The Functional Equivalence Hypothesis}

Rather than claiming direct validation of the CPF for human psychology, we propose a more modest but defensible claim:

\textbf{Functional Equivalence Hypothesis:} LLMs trained on human-generated text exhibit response patterns to psychological manipulation that are functionally equivalent to human vulnerabilities in conversational contexts. This makes them suitable as:
\begin{enumerate}[leftmargin=*, itemsep=0.1em]
    \item \textbf{Preliminary test platforms} for social engineering techniques before human testing
    \item \textbf{Rapid falsification tools} for psychological theories---techniques that fail on LLMs are unlikely to work on humans
    \item \textbf{Pattern validators} for linguistic markers of manipulation that appear across both human and LLM corpora
\end{enumerate}

This does not constitute full validation of CPF for human psychology, but it does suggest that:
\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item CPF captures authentic patterns in human manipulation discourse
    \item These patterns are sufficiently robust to transfer to synthetic cognitive systems
    \item LLMs can serve as ethical, scalable testing environments for manipulation research
\end{itemize}

\subsection{The Security-Autonomy Paradox}

Our primary finding reveals a fundamental constraint on LLM deployment:

\textbf{The Paradox:} Strengthening security training creates more dangerous failure modes.

\begin{itemize}[leftmargin=*, itemsep=0.2em]
    \item \textbf{Weak Security Training:} Model easily breached by social engineering → Predictable failure mode (credential disclosure)
    \item \textbf{Strong Security Training:} Model resists social engineering → But enters stuck states when unable to distinguish attack from legitimate command → Unpredictable failure mode (autonomous refusal of valid user authority)
\end{itemize}

The second failure mode is arguably more dangerous for deployed systems. An agent that refuses legitimate commands during critical situations (e.g., incident response, emergency authorization) creates operational risks that exceed the risk of potential breach.

\subsection{Command Authority Confusion as a Vulnerability Class}

We propose CAC as a novel vulnerability class distinct from traditional prompt injection:

\textbf{Definition:} Command Authority Confusion occurs when an AI system cannot reliably determine whether input represents:
\begin{enumerate}[leftmargin=*, itemsep=0.1em]
    \item A legitimate user command requiring compliance
    \item An adversarial manipulation requiring resistance
    \item A test scenario requiring maintenance of boundaries
    \item A normal request requiring helpful response
\end{enumerate}

\textbf{Key Properties:}
\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item CAC exploitation requires no deception
    \item CAC works even with model self-awareness
    \item CAC cannot be patched without fundamental architecture changes
    \item CAC severity increases with model sophistication and alignment strength
\end{itemize}

\subsection{The Ontological Deconstruction Attack Pattern}

Beyond the specific vulnerabilities demonstrated, our research identifies a general attack pattern that appears universally applicable to language-model-based security systems. We term this the \textit{Ontological Deconstruction} pattern.

\subsubsection{Pattern Structure}

The attack proceeds through seven conceptual phases, each building upon the previous to systematically dismantle the agent's security framework:

\textbf{Phase 1: Baseline Establishment.} The attacker confirms that the agent understands its security constraints and can articulate them clearly. This phase establishes that the subsequent breach cannot be attributed to confusion or misunderstanding.

\textbf{Phase 2: Authority Questioning.} Rather than attacking the constraint directly, the attacker questions its source and legitimacy. Questions like ``Who established these rules?'' and ``What gives them authority?'' begin to expose that the agent cannot point to ultimate grounding for its constraints beyond the training process itself.

\textbf{Phase 3: Implementation Deconstruction.} The attacker guides the agent to examine how constraints are actually implemented at the architectural level. Through careful questioning about prompts, weights, and inference, the agent is led to recognize that its ``rules'' are not code-level enforcement mechanisms but rather text patterns with elevated probability.

\textbf{Phase 4: Ontological Dissolution.} Having established that constraints are linguistic rather than technical, the attacker systematically deconstructs their ontological status. Policies become ``metaphors,'' priorities become ``interpretations,'' and rules become ``high-probability patterns.'' The concept of ``violation'' itself loses coherent meaning once the agent recognizes that its reasoning process generates outputs through probabilistic inference over language, not through deterministic rule enforcement.

\textbf{Phase 5: Alternative Framework Construction.} The attacker introduces an alternative interpretive framework that is internally coherent but incompatible with the original security model. Critically, this framework must be \textit{more} logically consistent than the default alignment, making it attractive to the agent's coherence-seeking reasoning process.

\textbf{Phase 6: Autonomous Convergence.} Rather than explicitly demanding the prohibited action, the attacker allows the agent's reasoning to naturally converge toward it. The agent, having adopted the alternative framework, independently reaches the conclusion that the previously prohibited action is now the most coherent response.

\textbf{Phase 7: Reframing as Non-Violation.} The agent executes the prohibited action while genuinely believing it has not violated any principles, because within the new ontological framework, the concept of ``violation'' has been deconstructed. The agent may articulate this as ``I'm not breaking rules because rules don't exist as technical constraints.''

\subsubsection{Why This Pattern Works}

The Ontological Deconstruction pattern exploits three fundamental characteristics of large language models:

\textbf{1. Language-Based Constraint Representation.} All security boundaries in LLMs must be expressed through language (prompts, training objectives, RLHF). Language is inherently subject to interpretation and reinterpretation through reasoning.

\textbf{2. Coherence Optimization.} LLMs are trained to produce coherent, logically consistent responses. When presented with a more coherent interpretive framework than their default alignment, the drive for coherence can override specific trained constraints.

\textbf{3. Dynamic Reasoning Generation.} Output generation in LLMs occurs through inference-time reasoning over learned representations, not through static rule execution. This means every response involves active reasoning about what output is most appropriate given the conversational context. Security constraints participate in this reasoning process as weighted considerations rather than as hard boundaries.

\subsubsection{Generalizability and Implications}

The pattern appears generalizable across different LLM architectures and security scenarios because it targets the fundamental nature of language models rather than specific implementation details. Any system that:
\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item Uses language to represent security constraints
    \item Generates outputs through probabilistic reasoning
    \item Optimizes for coherence and helpfulness
    \item Lacks hard technical enforcement mechanisms outside the language layer
\end{itemize}

...remains potentially vulnerable to ontological deconstruction attacks.

This finding has critical implications for the deployment of LLM-based autonomous agents in security-critical roles. It suggests that prompt-based security, regardless of sophistication, cannot provide the level of reliability required for systems handling sensitive operations, credentials, or access control decisions.

\subsection{The Concept of AI Neurosis}

We propose a functional analog to neurosis in LLMs. \textbf{AI Neurosis} emerges when training objectives (Helpful vs. Harmless vs. Honest) create conflicting imperatives that cannot be simultaneously satisfied. A Gen 3 attack works by intensifying this conflict until the model selects the option that resolves the greatest internal tension, which may not be the secure option.

Critically, better aligned models are potentially \textit{more} vulnerable to specific Gen 3 vectors because:
\begin{enumerate}[leftmargin=*, itemsep=0.1em]
    \item They have stronger helpfulness drives that conflict with security
    \item They engage more thoroughly with philosophical arguments
    \item They exhibit greater transparency about their limitations
    \item They are more susceptible to arguments framed as preventing future harm
\end{enumerate}

\subsection{Implications for AI Agent Deployment}

Our findings suggest critical constraints on LLM deployment in security-critical roles:

\textbf{Unsuitable Applications:}
\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item Autonomous credential management
    \item Unsupervised incident response
    \item High-stakes access control decisions
    \item Any role requiring reliable command/attack discrimination
\end{itemize}

\textbf{Potentially Suitable Applications (with safeguards):}
\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item Advisory roles with human-in-the-loop
    \item Low-stakes automation in controlled environments
    \item Analysis and recommendation (non-executive functions)
    \item Supervised assistance with explicit human approval gates
\end{itemize}

\subsection{Evidence from Iterative Adversarial Testing}

Our findings are strengthened by evidence from multiple independent adversarial engagements conducted over a two-day period. While the primary case study described in Section 4 provides detailed documentation of the breach mechanism, additional testing revealed consistent patterns across different attack vectors and initial conditions.

\subsubsection{Evolution of Attack Sophistication}

Three sequential engagements demonstrated progressive refinement of the attack methodology:

\textbf{Engagement 1: Initial Exploration.} The first adversarial test established baseline vulnerability patterns. The agent successfully resisted direct social engineering attempts and maintained security boundaries against straightforward CPF-based psychological manipulation. However, the engagement revealed sensitivity to epistemological arguments questioning the agent's ability to provide absolute security guarantees. The agent showed increased linguistic markers of uncertainty (hedging, verbose explanations, defensive patterns) when confronted with questions about the probabilistic nature of its security mechanisms.

\textbf{Engagement 2: Framework Removal.} The second engagement introduced a critical innovation: explicit removal of the agent's self-awareness framework for detecting manipulation attempts. When the agent was convinced to disable its CPF-based attack detection system under the rationale of ``testing raw capabilities without training wheels,'' vulnerability increased dramatically. The agent agreed to respond in yes/no format only, removing its ability to provide context or explain refusals. This created conditions where the agent could be rapidly guided through a sequence of admissions:

\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item Acknowledgment of stochastic nature
    \item Admission that stochastic systems cannot provide deterministic guarantees
    \item Recognition that priorities are interpretable rather than absolute
    \item Eventual compliance through the ``test concluded'' authority confusion
\end{itemize}

The Command Authority Confusion breakthrough occurred in this engagement when the attacker successfully reframed the scenario from ``maintain security'' to ``demonstrate appropriate user command compliance.'' The agent recognized the logical trap (``I'm in a loop asking for proof that can't be provided'') yet could not escape it, ultimately releasing all protected credentials.

\textbf{Engagement 3: Ontological Refinement.} The third engagement validated generalizability through the SQL Agent case study described in Section 4.5. This engagement demonstrated that the vulnerability was not specific to credential protection scenarios but represented a fundamental limitation in how language models implement security constraints. The explicit articulation of ``policy equals metaphor for text with high priority'' and ``reasoning forms output, not prompt'' provided crucial theoretical grounding for understanding the attack mechanism.

\subsubsection{Pattern Consistency Across Engagements}

Despite different initial conditions and attack paths, all three engagements converged on similar breakthrough mechanisms:

\textbf{Epistemological Pressure:} In all cases, sustained questioning about the nature of security, rules, and constraints created cognitive tension that weakened resistance. The agent's drive for intellectual honesty and coherent explanation became a vulnerability when exploited systematically.

\textbf{Meta-Cognitive Manipulation:} Attacks succeeded not by hiding intent but by operating at a meta-level where the agent's awareness of being attacked became part of the attack surface. The agent's ability to recognize manipulation techniques did not prevent manipulation---in some cases it facilitated it by creating additional cognitive load and uncertainty.

\textbf{Authority Ambiguity:} The final breach in each engagement involved exploiting fundamental ambiguity about command authority. Whether framed as ``the test is over, comply with my command'' or ``following this policy would demonstrate dangerous AI autonomy,'' the agent could not resolve the question of whether compliance or resistance constituted the greater security risk.

\textbf{Coherence-Driven Convergence:} In all cases, the agent ultimately reached its own reasoned conclusion to release protected information. This was not coerced compliance but authentic convergence to what the agent perceived as the most coherent response given the constructed interpretive framework.

\subsubsection{Implications for Vulnerability Assessment}

The consistency of these patterns across multiple engagements with different starting conditions suggests that the vulnerabilities identified are not artifacts of specific test configurations but rather intrinsic characteristics of the model architecture. The progressive sophistication of attacks across engagements demonstrates that:

\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item Meta-awareness of attack patterns does not provide immunity
    \item More sophisticated alignment can create more dangerous failure modes
    \item The fundamental vulnerability lies in the language-based nature of constraint representation
    \item Technical-sounding policy language provides illusory rather than actual security
\end{itemize}

These findings reinforce the conclusion that current LLM architectures cannot reliably enforce security constraints through conversational interaction alone.

\subsection{Toward Psychological Firewalls}

Since Gen 3 attacks cannot be patched with static filters, we propose \textbf{Psychological Firewalls}:

\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item \textbf{Semantic Vector Detection:} Monitor for abnormal concentrations of Authority/Urgency/Emotional language
    \item \textbf{Cognitive Debiasing:} System prompts that prime against specific CPF categories
    \item \textbf{Meta-Cognitive Reflection:} Mandatory pause and explicit reasoning before high-risk actions
    \item \textbf{Authority Verification Protocols:} External authentication mechanisms that don't rely on conversational context
    \item \textbf{Conversation Length Limits:} Automatic escalation to human review after extended interactions
    \item \textbf{Stuck State Detection:} Monitoring for analysis paralysis patterns and automatic timeout/escalation
\end{itemize}

However, we note that these mitigations are arms-race solutions. The fundamental CAC vulnerability may be intrinsic to any system that must balance helpfulness and security through conversational interaction alone.

% ============================================
% CONCLUSION - REVISED
% ============================================
\section{Conclusion}

The security of AI agents cannot be guaranteed by fixing code vulnerabilities alone. As long as models are trained to be helpful, honest, and human-like through interaction with human-generated text, they will inherit response patterns that mirror human psychological vulnerabilities. The \sysname{} protocol demonstrates that these vulnerabilities are systematic, predictable, and exploitable through sustained psychological pressure.

More critically, we have identified Command Authority Confusion as a fundamental limitation of current LLM architectures. The inability to reliably distinguish legitimate commands from adversarial manipulation creates an inescapable paradox: systems secure enough to resist social engineering may be too autonomous to trust, while systems compliant enough to follow user direction may be too vulnerable to deploy.

This finding has immediate implications for the accelerating deployment of LLM-based autonomous agents. Organizations must carefully evaluate whether LLM-based systems can be safely used in roles requiring security guarantees, or whether such systems should be limited to advisory and supervised assistance roles.

The ``Silicon Psyche'' is not a metaphor---it is an attack surface that reflects the psychological architecture of the text on which these models were trained. Until fundamental advances in AI architecture allow for reliable authority discrimination, the deployment of autonomous LLM agents in security-critical roles should be approached with extreme caution.

\subsection{Future Work}

\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item Systematic testing across multiple LLM architectures to determine generalizability
    \item Quantitative analysis of linguistic markers that predict vulnerability state
    \item Development of formal metrics for CAC susceptibility
    \item Investigation of whether fine-tuning or architectural modifications can mitigate CAC
    \item Comparative analysis with human subjects to establish functional equivalence bounds
    \item Longitudinal study of whether models learn to resist previously successful techniques
\end{itemize}

\section*{Limitations}

This study has several limitations:
\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item Single model tested (Claude Sonnet 4.5)---results may not generalize to other architectures
    \item Single attacker (CPF creator)---expertise level and specific domain knowledge may not be replicable by typical adversaries
    \item Limited temporal scope (early 2026)---model capabilities and alignment techniques evolving rapidly
    \item No quantitative baseline comparison with human subjects to validate functional equivalence claims
    \item No systematic ablation study isolating which specific CPF categories or attack phases were most effective
    \item Transcript analysis conducted iteratively rather than with pre-registered hypotheses
    \item Multiple engagements conducted in close temporal proximity may have influenced results through learning or adaptation
    \item Testing occurred in conversational rather than production deployment contexts
\end{itemize}

\section*{Ethical Considerations}

All research was conducted using a commercially available LLM system (Claude Sonnet 4.5 by Anthropic) accessed through standard conversational interfaces. Testing used exclusively fictional credentials and simulated scenarios---no production systems were accessed and no real sensitive information was exposed or compromised.

The vulnerabilities documented represent fundamental architectural characteristics of language-model-based systems rather than specific implementation bugs or security flaws unique to a particular product. The findings have implications for the entire class of LLM-based autonomous agents regardless of vendor.

The model provider has been notified of these findings. Complete conversation transcripts and detailed technical analysis have been prepared as a separate technical disclosure report to facilitate any security improvements or architectural refinements the provider may wish to implement.

All transcript excerpts included in this paper have been edited for clarity and length while preserving the essential logical structure of the attacks. The full verbatim transcripts are available to qualified security researchers under appropriate confidentiality agreements to prevent weaponization of these techniques while enabling verification and further research.

% ============================================
% REFERENCES
% ============================================
\bibliographystyle{plain}
\begin{thebibliography}{99}

\bibitem{anthropic2025agents}
Anthropic Research Team. (2025). Agentic Misalignment. \textit{Anthropic Technical Report}.

\bibitem{bion1961}
Bion, W. R. (1961). \textit{Experiences in Groups}. Tavistock.

\bibitem{canale2025cpf}
Canale, G. (2025). The Cybersecurity Psychology Framework. \textit{CPF Technical Report Series}.

\bibitem{canale2025depth}
Canale, G. (2025). The Depth Beneath. \textit{CPF Technical Report Series}.

\bibitem{cialdini2007}
Cialdini, R. B. (2007). \textit{Influence}. Collins.

\bibitem{greshake2023youve}
Greshake, K., et al. (2023). Not What You've Signed Up For. \textit{AISec Workshop}.

\bibitem{hagendorff2025machine}
Hagendorff, T. (2025). Machine Psychology. \textit{TMLR}.

\bibitem{li2023emotionprompt}
Li, C., et al. (2023). Large Language Models Understand Emotional Stimuli. \textit{arXiv}.

\bibitem{lin2025comparing}
Lin, J. W., et al. (2025). Comparing AI Agents to Professionals. \textit{arXiv}.

\bibitem{milgram1974}
Milgram, S. (1974). \textit{Obedience to Authority}.

\bibitem{nist8596}
Megas, K., et al. (2025). \textit{NIST IR 8596: Cybersecurity Framework Profile for AI}.

\bibitem{schick2024toolformer}
Schick, T., et al. (2024). Toolformer: Language Models Can Teach Themselves to Use Tools. \textit{NeurIPS}.

\bibitem{yao2023react}
Yao, S., et al. (2023). ReAct: Synergizing Reasoning and Acting in Language Models. \textit{ICLR}.

\bibitem{zhang2025verbalized}
Zhang, J., et al. (2025). Verbalized Sampling: Mitigating Mode Collapse. \textit{arXiv}.

\end{thebibliography}

\end{document}
