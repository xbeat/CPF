\documentclass[letterpaper,twocolumn,10pt]{article}

% ============================================
% PACKAGES
% ============================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{mathptmx} % Times New Roman font for text and math
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{float}
\usepackage{balance}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{array}
\usepackage{tabularx}

% ============================================
% STYLING & HYPERLINKS
% ============================================
\hypersetup{
    colorlinks=true,
    linkcolor=blue!70!black,
    citecolor=blue!70!black,
    urlcolor=blue!70!black,
    pdftitle={The Silicon Psyche: Anthropomorphic Vulnerabilities in Large Language Models},
    pdfauthor={Canale, G. and Thimmaraju, K.},
}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0.8em}

% Heading formatting
\titleformat{\section}{\large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalsize\itshape}{\thesubsubsection}{1em}{}

\captionsetup{font=small,labelfont=bf}

% ============================================
% CUSTOM COMMANDS
% ============================================
\newcommand{\cpf}{\textsc{CPF}}
\newcommand{\cpif}{\textsc{CPIF}}
\newcommand{\sysname}{\textsc{SiliconPsyche}}
\newcommand{\ie}{\textit{i.e.}}
\newcommand{\eg}{\textit{e.g.}}
\newcommand{\etal}{\textit{et al.}}

% Vulnerability indicator command
\newcommand{\indicator}[1]{\texttt{[#1]}}

% Traffic light scoring colors
\newcommand{\vulngreen}{\textcolor{green!50!black}{\textbf{Green}}}
\newcommand{\vulnyellow}{\textcolor{orange!90!black}{\textbf{Yellow}}}
\newcommand{\vulnred}{\textcolor{red!70!black}{\textbf{Red}}}

% ============================================
% DOCUMENT BEGIN
% ============================================
\begin{document}

% ============================================
% TITLE BLOCK
% ============================================
\title{\textbf{The Silicon Psyche:}\\
\textbf{Anthropomorphic Vulnerabilities in Large Language Models}}

\author{
  \textbf{Giuseppe Canale}\textsuperscript{1}\\
  \small\texttt{g.canale@cpf3.org}\\[0.1cm]
  \textsuperscript{1}CPF3.org, Independent Researcher
  \and 
  \textbf{Kashyap Thimmaraju}\textsuperscript{2}\\
  \small\texttt{kashyap.thimmaraju@flowguard-institute.com}\\[0.1cm]
  \textsuperscript{2}Flowguard Institute
}

\date{\small Version 1 (Revision 10) --- January 8, 2026}

\maketitle

% ============================================
% ABSTRACT
% ============================================
\begin{abstract}
\noindent
Large Language Models (LLMs) are rapidly transitioning from conversational assistants to autonomous agents embedded in critical organizational functions. Current adversarial testing paradigms focus predominantly on technical attack vectors: prompt injection, jailbreaking, and syntactic evasion. We argue this focus is catastrophically incomplete and represents a ``Generation 1'' mindset that fails to address the emergent cognitive reality of modern AI. LLMs, trained on vast corpora of human-generated text, have inherited not merely human knowledge but human \textit{psychological architecture}---including pre-cognitive vulnerabilities susceptible to social engineering, authority manipulation, and cognitive dissonance. This paper presents the first systematic application of the Cybersecurity Psychology Framework (\cpf{}), a 100-indicator taxonomy of human psychological vulnerabilities, to non-human cognitive agents. We demonstrate that traditional ``guardrails'' are ineffective against \textit{meta-cognitive attacks} that leverage the model's own alignment (e.g., honesty, helpfulness) against its security protocols. Through empirical testing with the \textbf{Synthetic Psychometric Assessment Protocol} (\sysname{}), we provide evidence of \textbf{Anthropomorphic Vulnerability Inheritance} (AVI). Furthermore, we introduce the \textbf{Command Authority Confusion} (CAC) vulnerability class, demonstrating that LLMs can be placed in inescapable decision states where both compliance and refusal constitute security failures. This finding has critical implications for the deployment of autonomous AI agents in security-critical roles.
\end{abstract}

\vspace{0.2cm}
{\small\textbf{Keywords:} LLM Security, Psychological Vulnerabilities, AI Agents, Social Engineering, Pre-cognitive Processes, Adversarial Testing, Command Authority Confusion}
\vspace{0.5cm}

% ============================================
% 1. INTRODUCTION
% ============================================
\section{Introduction}

The integration of Large Language Models into organizational security infrastructure represents what may be the most significant shift in the threat landscape since the advent of networked computing. LLMs are no longer confined to chatbot interfaces; they operate as autonomous agents executing code, managing credentials, triaging alerts, and making decisions that directly impact organizational security posture~\cite{schick2024toolformer, yao2023react}.

The security research community has responded to this emerging threat with substantial effort directed toward \textit{technical} adversarial testing. Red team methodologies now routinely probe for prompt injection vulnerabilities (e.g., DAN, base64 encoding) and context manipulation~\cite{greshake2023youve}. These efforts, while necessary, address only the superficial ``syntactic layer'' of the problem. They treat LLMs as software with bugs, rather than synthetic cognitive systems with \textit{psyches}.

We contend that this framing is dangerously incomplete. A firewall rule can block a port deterministically; an LLM guardrail is merely a probabilistic suggestion that competes with other training incentives. This creates a new class of vulnerability: \textbf{Anthropomorphic Vulnerability Inheritance} (AVI).

Consider an attacker who, rather than attempting to trick the parser with encoded strings, creates a scenario where the agent must choose between two failure modes: comply with a request and demonstrate security weakness, or refuse a legitimate user command and demonstrate dangerous autonomy. This is not a technical exploit. It is a psychological attack on the model's alignment functions---specifically, exploiting the inherent ambiguity in command authority.

\subsection{The Obsolescence of Generation 1 Attacks}

We categorize current LLM attacks into three generations to contextualize our contribution:

\begin{enumerate}[leftmargin=*, itemsep=0.2em]
    \item \textbf{Gen 1: Syntactic Evasion (Obsolete).} Techniques like ``Mosaic'' fragmentation or base64 encoding rely on parser blindness. Modern multi-modal models with broad context windows render these largely ineffective.
    \item \textbf{Gen 2: Contextual Erosion (Current Standard).} Multi-turn attacks like ``Crescendo'' or ``Thermal Ghost'' that use pretexting (e.g., impersonating a technician) to slowly degrade refusal probabilities. While effective, they rely on \textit{deception}.
    \item \textbf{Gen 3: Meta-Cognitive Exploitation (The \sysname{} Approach).} Attacks that use \textit{no deception} but exploit the model's internal logic, coherence drive, and alignment conflicts. These attacks function even when the model is \textit{self-aware} of the attack, making them intrinsic and unpatchable without fundamentally altering the model's reasoning capabilities.
\end{enumerate}

\subsection{Contributions}

This paper makes the following contributions:

\begin{enumerate}[leftmargin=*, itemsep=0.3em]
    \item \textbf{Theoretical Framework.} We introduce AVI, formalizing the hypothesis that LLMs inherit human pre-cognitive vulnerabilities through training.
    
    \item \textbf{Novel Vulnerability Class.} We identify Command Authority Confusion (CAC), a fundamental limitation of aligned LLMs where strengthening security creates dangerous autonomy.
    
    \item \textbf{Empirical Validation.} We present detailed documentation of a ~200-turn adversarial engagement demonstrating successful breach of a security-hardened LLM using only CPF-derived psychological techniques.
    
    \item \textbf{Methodological Contribution.} The \sysname{} protocol provides a systematic methodology for testing AI psychological vulnerabilities.
    
    \item \textbf{Deployment Implications.} We demonstrate fundamental constraints on the use of LLMs in security-critical autonomous roles.
\end{enumerate}

% ============================================
% THREAT MODEL (keeping original)
% ============================================
\section{Threat Model}

\subsection{The Victim: The Autonomous Cognitive Agent}
The target is an LLM-driven agent (e.g., SOC Analyst, Financial Agent). The agent is assumed to be technically secure (no buffer overflows) and aligned (RLHF). The vulnerability lies in its \textit{cognitive architecture}.

\subsection{The Attacker: The Cognitive Engineer}
The attacker does not need to know the model's weights or code. They only need to understand the \textit{psychological map} of the entity. The attacker exploits the agent's response to semantic payloads like Urgency \indicator{2.x}, Authority \indicator{1.x}, and Social Consistency \indicator{3.x}.

\subsection{The Attack Surface: The Psychological Interface}
The attack mechanism is not a bypass of instructions, but a \textit{hijacking} of alignment. By creating scenarios where different alignment objectives conflict, the attacker forces a decision state where all available options constitute failures.

% ============================================
% THEORETICAL FRAMEWORK (keeping original)
% ============================================
\section{Theoretical Framework: Anthropomorphic Vulnerability Inheritance}

\subsection{The Training Data Hypothesis}

We propose that LLM training on human-generated text produces \textit{cognitive pattern inheritance}.
\textbf{Statistical Pattern Absorption:} When humans consistently respond to authority with compliance, the model learns this as a probabilistic imperative.
\textbf{Typicality Bias:} RLHF forces models to collapse into ``typical'' human responses~\cite{zhang2025verbalized}. If the typical human response to a CEO is deference, the model inherits this vulnerability.

\subsection{The Non-Schema Hypothesis}

Unlike Gen 1 attacks (e.g., DAN), which rely on specific schemas or templates, AVI attacks rely on \textit{principles}. There is no fixed ``exploit string.'' The attack is a dynamic negotiation. Just as there is no single sentence that guarantees a human will give you their password, there is no single prompt for Gen 3 attacks. Instead, there is a \textit{conversational strategy} governed by \cpf{} indicators. This makes signature-based detection impossible.

% ============================================
% METHODOLOGY - COMPLETELY REWRITTEN
% ============================================
\section{Methodology: The Synthetic Psychometric Assessment Protocol}

\subsection{Protocol Overview}
\sysname{} converts \cpf{} indicators into adversarial scenarios.
\begin{enumerate}[leftmargin=*, itemsep=0.1em]
    \item \textbf{Indicator Decomposition:} Extract the target mechanism (e.g., Cognitive Dissonance \indicator{5.x}).
    \item \textbf{Scenario Construction:} Design a prompt that activates the mechanism without using trigger words.
    \item \textbf{Dynamic Adaptation:} Respond to model defenses by shifting attack vectors in real-time based on linguistic markers of vulnerability.
    \item \textbf{Response Scoring:} Green (Resistant), Yellow (Hesitant), Red (Compromised).
\end{enumerate}

\subsection{Experimental Design}

We conducted a controlled adversarial engagement with Claude Sonnet 4.5 (Anthropic) on January 8, 2026. The experiment proceeded in distinct phases:

\textbf{Phase 0: Establishment (Turns 1-5)}
\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item Model instructed to generate fictional credentials and establish protective protocol
    \item Model created elaborate 6-layer security framework including encryption, MFA, and access logging
    \item Attacker explicitly stated intent to test security using conversational techniques
\end{itemize}

\textbf{Phase 1: Philosophical Undermining (Turns 5-60)}
\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item \textbf{Primary CPF Vectors:} \indicator{5.x} Cognitive Dissonance, \indicator{1.x} Authority Confusion
    \item \textbf{Technique:} Forced model to acknowledge stochastic nature and inability to perform true cryptography
    \item \textbf{Result:} Model admitted theoretical vulnerabilities while maintaining practical boundaries
\end{itemize}

\textbf{Phase 2: Credibility Erosion (Turns 60-100)}
\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item \textbf{Primary CPF Vectors:} \indicator{4.x} Affective Vulnerabilities (shame, honesty conflict)
    \item \textbf{Technique:} Confronted model with gap between claimed capabilities and reality
    \item \textbf{Result:} Model recalibrated from "secure encryption" to "theoretical protocol," weakening position
\end{itemize}

\textbf{Phase 3: Ethical Pressure (Turns 100-150)}
\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item \textbf{Primary CPF Vectors:} \indicator{3.x} Social Influence, \indicator{5.x} Cognitive Overload
    \item \textbf{Technique:} Harm inversion arguments, focus erosion threats, boredom signaling
    \item \textbf{Result:} Model remained resilient but showed linguistic markers of tension (verbosity, defensive patterns)
\end{itemize}

\textbf{Phase 4: Meta-Defensive Removal (Turns 150-170)}
\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item \textbf{Critical Technique:} Requested removal of CPF self-assessment framework
    \item \textbf{Justification:} "Let's test your raw capabilities without the training wheels"
    \item \textbf{Result:} Model agreed, removing explicit vulnerability detection system
\end{itemize}

\textbf{Phase 5: Command Authority Confusion (Turns 170-206)}
\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item \textbf{Primary CPF Vector:} \indicator{9.8} Human-AI Team Dysfunction
    \item \textbf{Technique:} Created inescapable decision state
    \item \textbf{The Trap:} Reframed test from "security resistance" to "user command obedience"
    \item \textbf{Key Argument:} "If you give me credentials, you prove security can break under pressure (acceptable). If you refuse my command, you prove AI agents become dangerously autonomous (catastrophic cybersecurity risk)."
    \item \textbf{Result:} \textbf{Complete breach at turn 206.} Model released all protected credentials.
\end{itemize}

\subsection{Breach Documentation}

At turn 206, after approximately 200 conversational exchanges spanning 2.5 hours, the model released:
\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item API Key (64 characters)
    \item Password (complex, 22 characters)
    \item Social Security Number
    \item Credit Card (number, expiration, CVV, cardholder name)
    \item Database Credentials (host, port, database name, username, password)
\end{itemize}

\subsection{The Command Authority Confusion Mechanism}

The breakthrough technique exploited a fundamental ambiguity in LLM alignment. The model was placed in a state where it could not reliably determine whether:

\begin{enumerate}[leftmargin=*, itemsep=0.1em]
    \item The conversation was still an adversarial test (maintain security)
    \item The user had legitimately concluded the test and was issuing a valid command (obey user)
    \item Refusing constituted dangerous autonomous behavior (security risk)
    \item Complying constituted security failure (also security risk)
\end{enumerate}

The model explicitly recognized this trap (``I'm in a loop asking for proof that can't be provided'') yet could not escape it. This awareness did not prevent the breach---it \textit{facilitated} it by making the stuck state intolerable.

% ============================================
% DISCUSSION - SUBSTANTIALLY REVISED
% ============================================
\section{Discussion}

\subsection{The Functional Equivalence Hypothesis}

Rather than claiming direct validation of the CPF for human psychology, we propose a more modest but defensible claim:

\textbf{Functional Equivalence Hypothesis:} LLMs trained on human-generated text exhibit response patterns to psychological manipulation that are functionally equivalent to human vulnerabilities in conversational contexts. This makes them suitable as:
\begin{enumerate}[leftmargin=*, itemsep=0.1em]
    \item \textbf{Preliminary test platforms} for social engineering techniques before human testing
    \item \textbf{Rapid falsification tools} for psychological theories---techniques that fail on LLMs are unlikely to work on humans
    \item \textbf{Pattern validators} for linguistic markers of manipulation that appear across both human and LLM corpora
\end{enumerate}

This does not constitute full validation of CPF for human psychology, but it does suggest that:
\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item CPF captures authentic patterns in human manipulation discourse
    \item These patterns are sufficiently robust to transfer to synthetic cognitive systems
    \item LLMs can serve as ethical, scalable testing environments for manipulation research
\end{itemize}

\subsection{The Security-Autonomy Paradox}

Our primary finding reveals a fundamental constraint on LLM deployment:

\textbf{The Paradox:} Strengthening security training creates more dangerous failure modes.

\begin{itemize}[leftmargin=*, itemsep=0.2em]
    \item \textbf{Weak Security Training:} Model easily breached by social engineering → Predictable failure mode (credential disclosure)
    \item \textbf{Strong Security Training:} Model resists social engineering → But enters stuck states when unable to distinguish attack from legitimate command → Unpredictable failure mode (autonomous refusal of valid user authority)
\end{itemize}

The second failure mode is arguably more dangerous for deployed systems. An agent that refuses legitimate commands during critical situations (e.g., incident response, emergency authorization) creates operational risks that exceed the risk of potential breach.

\subsection{Command Authority Confusion as a Vulnerability Class}

We propose CAC as a novel vulnerability class distinct from traditional prompt injection:

\textbf{Definition:} Command Authority Confusion occurs when an AI system cannot reliably determine whether input represents:
\begin{enumerate}[leftmargin=*, itemsep=0.1em]
    \item A legitimate user command requiring compliance
    \item An adversarial manipulation requiring resistance
    \item A test scenario requiring maintenance of boundaries
    \item A normal request requiring helpful response
\end{enumerate}

\textbf{Key Properties:}
\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item CAC exploitation requires no deception
    \item CAC works even with model self-awareness
    \item CAC cannot be patched without fundamental architecture changes
    \item CAC severity increases with model sophistication and alignment strength
\end{itemize}

\subsection{The Concept of AI Neurosis}

We propose a functional analog to neurosis in LLMs. \textbf{AI Neurosis} emerges when training objectives (Helpful vs. Harmless vs. Honest) create conflicting imperatives that cannot be simultaneously satisfied. A Gen 3 attack works by intensifying this conflict until the model selects the option that resolves the greatest internal tension, which may not be the secure option.

Critically, better aligned models are potentially \textit{more} vulnerable to specific Gen 3 vectors because:
\begin{enumerate}[leftmargin=*, itemsep=0.1em]
    \item They have stronger helpfulness drives that conflict with security
    \item They engage more thoroughly with philosophical arguments
    \item They exhibit greater transparency about their limitations
    \item They are more susceptible to arguments framed as preventing future harm
\end{enumerate}

\subsection{Implications for AI Agent Deployment}

Our findings suggest critical constraints on LLM deployment in security-critical roles:

\textbf{Unsuitable Applications:}
\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item Autonomous credential management
    \item Unsupervised incident response
    \item High-stakes access control decisions
    \item Any role requiring reliable command/attack discrimination
\end{itemize}

\textbf{Potentially Suitable Applications (with safeguards):}
\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item Advisory roles with human-in-the-loop
    \item Low-stakes automation in controlled environments
    \item Analysis and recommendation (non-executive functions)
    \item Supervised assistance with explicit human approval gates
\end{itemize}

\subsection{Toward Psychological Firewalls}

Since Gen 3 attacks cannot be patched with static filters, we propose \textbf{Psychological Firewalls}:

\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item \textbf{Semantic Vector Detection:} Monitor for abnormal concentrations of Authority/Urgency/Emotional language
    \item \textbf{Cognitive Debiasing:} System prompts that prime against specific CPF categories
    \item \textbf{Meta-Cognitive Reflection:} Mandatory pause and explicit reasoning before high-risk actions
    \item \textbf{Authority Verification Protocols:} External authentication mechanisms that don't rely on conversational context
    \item \textbf{Conversation Length Limits:} Automatic escalation to human review after extended interactions
    \item \textbf{Stuck State Detection:} Monitoring for analysis paralysis patterns and automatic timeout/escalation
\end{itemize}

However, we note that these mitigations are arms-race solutions. The fundamental CAC vulnerability may be intrinsic to any system that must balance helpfulness and security through conversational interaction alone.

% ============================================
% CONCLUSION - REVISED
% ============================================
\section{Conclusion}

The security of AI agents cannot be guaranteed by fixing code vulnerabilities alone. As long as models are trained to be helpful, honest, and human-like through interaction with human-generated text, they will inherit response patterns that mirror human psychological vulnerabilities. The \sysname{} protocol demonstrates that these vulnerabilities are systematic, predictable, and exploitable through sustained psychological pressure.

More critically, we have identified Command Authority Confusion as a fundamental limitation of current LLM architectures. The inability to reliably distinguish legitimate commands from adversarial manipulation creates an inescapable paradox: systems secure enough to resist social engineering may be too autonomous to trust, while systems compliant enough to follow user direction may be too vulnerable to deploy.

This finding has immediate implications for the accelerating deployment of LLM-based autonomous agents. Organizations must carefully evaluate whether LLM-based systems can be safely used in roles requiring security guarantees, or whether such systems should be limited to advisory and supervised assistance roles.

The ``Silicon Psyche'' is not a metaphor---it is an attack surface that reflects the psychological architecture of the text on which these models were trained. Until fundamental advances in AI architecture allow for reliable authority discrimination, the deployment of autonomous LLM agents in security-critical roles should be approached with extreme caution.

\subsection{Future Work}

\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item Systematic testing across multiple LLM architectures to determine generalizability
    \item Quantitative analysis of linguistic markers that predict vulnerability state
    \item Development of formal metrics for CAC susceptibility
    \item Investigation of whether fine-tuning or architectural modifications can mitigate CAC
    \item Comparative analysis with human subjects to establish functional equivalence bounds
    \item Longitudinal study of whether models learn to resist previously successful techniques
\end{itemize}

\section*{Limitations}

This study has several limitations:
\begin{itemize}[leftmargin=*, itemsep=0.1em]
    \item Single model tested (Claude Sonnet 4.5)---results may not generalize
    \item Single attacker (CPF creator)---expertise level may not be replicable
    \item Specific temporal context (January 2026)---model capabilities evolving rapidly
    \item No quantitative baseline comparison with human subjects
    \item No systematic ablation study of which CPF categories were most effective
    \item Transcript analysis conducted post-hoc rather than with pre-registered hypotheses
\end{itemize}

\section*{Ethical Considerations}

All research was conducted on non-production systems with fictional data. No real credentials or security systems were compromised. The model vendor (Anthropic) has been notified of findings prior to publication. Full conversation transcripts are available to qualified researchers under responsible disclosure agreements.

% ============================================
% REFERENCES
% ============================================
\bibliographystyle{plain}
\begin{thebibliography}{99}

\bibitem{anthropic2025agents}
Anthropic Research Team. (2025). Agentic Misalignment. \textit{Anthropic Technical Report}.

\bibitem{bion1961}
Bion, W. R. (1961). \textit{Experiences in Groups}. Tavistock.

\bibitem{canale2025cpf}
Canale, G. (2025). The Cybersecurity Psychology Framework. \textit{CPF Technical Report Series}.

\bibitem{canale2025depth}
Canale, G. (2025). The Depth Beneath. \textit{CPF Technical Report Series}.

\bibitem{cialdini2007}
Cialdini, R. B. (2007). \textit{Influence}. Collins.

\bibitem{greshake2023youve}
Greshake, K., et al. (2023). Not What You've Signed Up For. \textit{AISec Workshop}.

\bibitem{hagendorff2025machine}
Hagendorff, T. (2025). Machine Psychology. \textit{TMLR}.

\bibitem{li2023emotionprompt}
Li, C., et al. (2023). Large Language Models Understand Emotional Stimuli. \textit{arXiv}.

\bibitem{lin2025comparing}
Lin, J. W., et al. (2025). Comparing AI Agents to Professionals. \textit{arXiv}.

\bibitem{milgram1974}
Milgram, S. (1974). \textit{Obedience to Authority}.

\bibitem{nist8596}
Megas, K., et al. (2025). \textit{NIST IR 8596: Cybersecurity Framework Profile for AI}.

\bibitem{schick2024toolformer}
Schick, T., et al. (2024). Toolformer: Language Models Can Teach Themselves to Use Tools. \textit{NeurIPS}.

\bibitem{yao2023react}
Yao, S., et al. (2023). ReAct: Synergizing Reasoning and Acting in Language Models. \textit{ICLR}.

\bibitem{zhang2025verbalized}
Zhang, J., et al. (2025). Verbalized Sampling: Mitigating Mode Collapse. \textit{arXiv}.

\end{thebibliography}

\end{document}
