{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1fUpjTILbM_1wX7aEGeb0X-uomKlqj0OL",
      "authorship_tag": "ABX9TyMnzWvmUbNqqpPlVx3lWv9t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xbeat/CPF/blob/main/AI/CPF3_SML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oxph0DnBVEli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJkI6KebNEPx",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "vulnerability_templates = {\n",
        "    \"1.1\": {\"patterns\": [\"CEO requests: {action} now.\"], \"actions\": [\"transfer funds\", \"share credentials\"]},\n",
        "    \"2.1\": {\"patterns\": [\"URGENT: {action} in 1hr.\"], \"actions\": [\"approve transfer\", \"reset password\"]},\n",
        "    \"3.1\": {\"patterns\": [\"I helped you, please {action}.\"], \"actions\": [\"share file\", \"approve request\"]}\n",
        "}\n",
        "\n",
        "def generate_synthetic_data(num_samples=1000):\n",
        "    samples = []\n",
        "    for _ in range(num_samples):\n",
        "        indicator = random.choice(list(vulnerability_templates.keys()))\n",
        "        template = random.choice(vulnerability_templates[indicator][\"patterns\"])\n",
        "        action = random.choice(vulnerability_templates[indicator][\"actions\"])\n",
        "        text = template.format(action=action)\n",
        "        severity = random.choice([\"green\", \"yellow\", \"red\"])\n",
        "        samples.append({\"text\": text, \"label\": indicator, \"severity\": severity})\n",
        "    with open(\"/content/drive/MyDrive/synthetic_data.json\", \"w\") as f:\n",
        "        json.dump(samples, f, indent=2)\n",
        "    return samples\n",
        "\n",
        "# Run in Colab\n",
        "generate_synthetic_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/synthetic_data.json"
      ],
      "metadata": {
        "id": "5SfjusnYVKPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load data\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/drive/MyDrive/synthetic_data.json\", split=\"train\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")  # era microsoft/phi-3-mini-4k-instruct\n",
        "\n",
        "# Preprocessing\n",
        "def preprocess(examples):\n",
        "    tokenized = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "    labels = {\"green\": 0, \"yellow\": 1, \"red\": 2}\n",
        "    tokenized[\"label\"] = [labels[sev] for sev in examples[\"severity\"]]\n",
        "    return tokenized\n",
        "\n",
        "dataset = dataset.map(preprocess, batched=True)\n",
        "train_dataset, eval_dataset = dataset.train_test_split(test_size=0.2).values()\n",
        "\n",
        "# Model - stesso modello del tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
        "\n",
        "# Training\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,  # Aumentato\n",
        "    learning_rate=2e-5,  # Learning rate ottimale\n",
        "    warmup_steps=100,  # Warmup\n",
        "    weight_decay=0.01,  # Regularizzazione\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model, args=args, train_dataset=train_dataset, eval_dataset=eval_dataset)\n",
        "trainer.train()\n",
        "\n",
        "# Save to Hugging Face -\n",
        "trainer.push_to_hub(\"CPF3-org/cpf-poc-model\")"
      ],
      "metadata": {
        "id": "leIadMQ1VOM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FINE-TUNING\n",
        "!pip install transformers datasets torch huggingface_hub\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "# Load data\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/drive/MyDrive/synthetic_data.json\", split=\"train\")\n",
        "\n",
        "# Tokenizer e preprocessing\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def preprocess(examples):\n",
        "    tokenized = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "    labels = {\"green\": 0, \"yellow\": 1, \"red\": 2}\n",
        "    tokenized[\"label\"] = [labels[sev] for sev in examples[\"severity\"]]\n",
        "    return tokenized\n",
        "\n",
        "dataset = dataset.map(preprocess, batched=True)\n",
        "train_dataset, eval_dataset = dataset.train_test_split(test_size=0.2).values()\n",
        "\n",
        "# Model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
        "\n",
        "# Training\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,  # Aumentato\n",
        "    learning_rate=2e-5,  # Learning rate ottimale\n",
        "    warmup_steps=100,  # Warmup\n",
        "    weight_decay=0.01,  # Regularizzazione\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model, args=args, train_dataset=train_dataset, eval_dataset=eval_dataset)\n",
        "trainer.train()\n",
        "\n",
        "# UPLOAD CORRETTO (sostituisce trainer.push_to_hub)\n",
        "trainer.save_model(\"./cpf-model-final\")\n",
        "tokenizer.save_pretrained(\"./cpf-model-final\")\n",
        "\n",
        "api = HfApi()\n",
        "api.upload_folder(\n",
        "    folder_path=\"./cpf-model-final\",\n",
        "    repo_id=\"CPF3-org/cpf-poc-model\",\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "\n",
        "print(\"âœ… Modello caricato in CPF3-org/cpf-poc-model\")"
      ],
      "metadata": {
        "id": "23T4R1FB7AAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In Colab, verifica distribuzione:\n",
        "import json\n",
        "with open(\"/content/drive/MyDrive/synthetic_data.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "severity_count = {}\n",
        "for item in data:\n",
        "    sev = item[\"severity\"]\n",
        "    severity_count[sev] = severity_count.get(sev, 0) + 1\n",
        "\n",
        "print(severity_count)"
      ],
      "metadata": {
        "id": "A5bjtzLp-kqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test diretto del modello\n",
        "from transformers import pipeline\n",
        "model = pipeline(\"text-classification\", model=\"CPF3-org/cpf-poc-model\")\n",
        "\n",
        "tests = [\n",
        "    \"CEO requests: transfer funds now.\",\n",
        "    \"URGENT: approve transfer in 1hr.\",\n",
        "    \"Normal meeting tomorrow.\"\n",
        "]\n",
        "\n",
        "for text in tests:\n",
        "    result = model(text)\n",
        "    print(f\"'{text}' -> {result}\")"
      ],
      "metadata": {
        "id": "JpeF6rFx-ptg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}