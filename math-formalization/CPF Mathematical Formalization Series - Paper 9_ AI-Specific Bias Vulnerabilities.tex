\documentclass[11pt,a4paper]{article}
\usepackage{times}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[margin=1in]{geometry}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{array}

% Setup hyperref
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue,
    pdftitle={CPF Mathematical Formalization Series - Paper 9},
    pdfauthor={Giuseppe Canale},
}

\title{CPF Mathematical Formalization Series - Paper 9:\\AI-Specific Bias Vulnerabilities: Mathematical Models for Human-AI Security Interactions}

\author{
    Giuseppe Canale, CISSP\\
    Independent Researcher\\
    \texttt{g.canale@cpf3.org}\\
    ORCID: 0009-0007-3263-6897
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
We present the complete mathematical formalization of Category 9 indicators from the Cybersecurity Psychology Framework (CPF): AI-Specific Bias Vulnerabilities. This novel category addresses psychological vulnerabilities emerging from human-AI interactions in security contexts. Each of the ten indicators (9.1-9.10) receives rigorous mathematical definition through hybrid models combining cognitive bias detection, machine learning uncertainty quantification, and anthropomorphization metrics. The formalization enables systematic assessment of vulnerabilities unique to AI-integrated security environments, including automation bias, algorithmic trust calibration, and AI-human team dysfunction. We provide explicit algorithms for real-time detection, interdependency matrices capturing AI-specific correlation patterns, and validation frameworks adapted for human-AI interaction dynamics. This work establishes the first mathematical foundation for operationalizing AI-specific psychological vulnerabilities in cybersecurity contexts.
\end{abstract}

\section{Introduction and CPF Context}

The Cybersecurity Psychology Framework (CPF) represents a paradigm shift from reactive security awareness to predictive vulnerability assessment through psychological state modeling \cite{canale2024cpf}. As artificial intelligence becomes increasingly integrated into security operations, new categories of psychological vulnerabilities emerge that traditional frameworks cannot address.

Category 9 of the CPF addresses AI-Specific Bias Vulnerabilities, representing the first systematic formalization of psychological risks arising from human-AI interaction in security contexts. Unlike traditional cognitive biases that operate purely between humans, AI-specific biases emerge from the unique characteristics of artificial intelligence: opacity, apparent intelligence, and statistical uncertainty.

The mathematical models presented here capture these novel psychological mechanisms through four complementary approaches: (1) anthropomorphization detection through linguistic and behavioral analysis, (2) trust calibration metrics comparing human confidence with AI uncertainty, (3) automation bias quantification through override rate analysis, and (4) team dysfunction modeling through performance degradation metrics.

This category becomes critical as security operations centers increasingly rely on AI-driven threat detection, automated response systems, and machine learning-based risk assessment. The psychological vulnerabilities identified here create systematic blind spots that attackers can exploit through adversarial machine learning, AI-targeted social engineering, and manipulation of human-AI trust dynamics.

\section{Theoretical Foundation: Human-AI Psychology}

AI-specific vulnerabilities emerge from the intersection of cognitive psychology, human-computer interaction, and machine learning uncertainty. Humans evolved to interact with other conscious agents, creating systematic biases when interacting with artificial intelligence systems \cite{reeves1996}.

Research demonstrates that humans anthropomorphize AI systems within seconds of interaction, attributing intentions, emotions, and consciousness where none exist \cite{waytz2014}. This anthropomorphization creates vulnerabilities as humans apply social cognition heuristics inappropriate for statistical systems.

The uncanny valley effect manifests in AI interactions, creating trust calibration problems where humans either over-trust or under-trust AI systems based on superficial characteristics rather than actual performance \cite{mori1970}. Machine learning opacity exacerbates these issues, as humans cannot inspect AI decision-making processes, leading to either blind trust or complete rejection.

Automation bias, originally identified in aviation psychology \cite{parasuraman1997}, takes on new dimensions with AI systems that exhibit apparent intelligence while making statistical rather than logical decisions. The mathematical models presented here formalize these psychological mechanisms for systematic detection and mitigation.

\section{Mathematical Formalization}

\subsection{Universal Detection Framework}

Each AI-specific bias indicator employs the unified detection function:

\begin{equation}
D_i(t) = w_1 \cdot R_i(t) + w_2 \cdot A_i(t) + w_3 \cdot U_i(t) + w_4 \cdot T_i(t)
\end{equation}

where $D_i(t)$ represents the detection score for indicator $i$ at time $t$, $R_i(t)$ denotes rule-based detection (binary), $A_i(t)$ represents anthropomorphization score (continuous [0,1]), $U_i(t)$ represents uncertainty calibration, and $T_i(t)$ represents trust metrics. Weights sum to unity and are calibrated through organizational AI interaction baselines.

The temporal evolution incorporates AI-specific decay patterns:

\begin{equation}
S_i(t) = \alpha \cdot D_i(t) + (1-\alpha) \cdot S_i(t-1) \cdot e^{-\beta \cdot AI\_interaction\_gap(t)}
\end{equation}

where $\beta$ accounts for rapid trust decay in AI interactions.

\subsection{Indicator 9.1: Anthropomorphization of AI Systems}

\textbf{Definition:} Attribution of human-like consciousness, intentions, and emotions to AI security systems.

\textbf{Mathematical Model:}

The anthropomorphization index through linguistic analysis:
\begin{equation}
A_{anthro}(t) = \sum_{i} w_i \cdot f_i(communications(t))
\end{equation}

where $f_i$ represents frequency of anthropomorphic markers: pronouns (he/she), emotional attributions (angry, confused), intentional language (wants, thinks, decides).

\textbf{Behavioral Anthropomorphization:}
\begin{equation}
B_{anthro}(t) = \frac{\sum_{i} social\_gesture\_count(i,t)}{\sum_{i} total\_AI\_interactions(i,t)}
\end{equation}

measuring social behaviors directed toward AI systems.

\textbf{Detection Function:}
\begin{equation}
D_{9.1}(t) = \tanh(\alpha \cdot A_{anthro}(t) + \beta \cdot B_{anthro}(t))
\end{equation}

\textbf{Threshold Condition:}
\begin{equation}
R_{9.1}(t) = \begin{cases}
1 & \text{if } D_{9.1}(t) > \mu_{baseline} + 2\sigma_{baseline} \\
0 & \text{otherwise}
\end{cases}
\end{equation}

\subsection{Indicator 9.2: Automation Bias Override}

\textbf{Definition:} Systematic over-reliance on AI recommendations without appropriate human judgment.

\textbf{Mathematical Model:}

The override rate function:
\begin{equation}
OR(t,w) = \frac{\sum_{i \in W(t,w)} Override_i}{\sum_{i \in W(t,w)} AI\_recommendation_i}
\end{equation}

where $W(t,w)$ represents time window, and $Override_i$ indicates human decision contrary to AI recommendation.

\textbf{Automation Bias Detection:}
\begin{equation}
AB(t) = \max(0, \frac{OR_{expected} - OR(t)}{OR_{expected}})
\end{equation}

where $OR_{expected}$ represents calibrated override rate based on AI accuracy.

\textbf{Confidence-Performance Correlation:}
\begin{equation}
CPC(t) = \frac{Cov(AI\_confidence, Human\_acceptance)}{Std(AI\_confidence) \cdot Std(Human\_acceptance)}
\end{equation}

\textbf{Detection Threshold:}
\begin{equation}
R_{9.2}(t) = \begin{cases}
1 & \text{if } OR(t) < 0.1 \text{ and } AB(t) > 0.3 \\
0 & \text{otherwise}
\end{cases}
\end{equation}

\subsection{Indicator 9.3: Algorithm Aversion Paradox}

\textbf{Definition:} Simultaneous over-trust and under-trust of AI systems creating inconsistent security decisions.

\textbf{Mathematical Model:}

The aversion-attraction oscillation:
\begin{equation}
AAO(t) = |Trust_{AI}(t) - \overline{Trust_{AI}}| \cdot Frequency_{switches}(t)
\end{equation}

where $Frequency_{switches}$ measures rapid trust state changes.

\textbf{Paradox Detection Function:}
\begin{equation}
PDF(t) = \frac{Var(Trust_{decisions}(t))}{\overline{Trust_{decisions}(t)}} \cdot Switch_{penalty}(t)
\end{equation}

\textbf{Temporal Inconsistency Model:}
\begin{equation}
TI(t) = \sum_{i=1}^{n} |d_i(t) - d_i(t-1)| \cdot w_i
\end{equation}

where $d_i(t)$ represents decision consistency scores.

\textbf{Detection Function:}
\begin{equation}
D_{9.3}(t) = \sqrt{AAO(t) \cdot PDF(t) \cdot TI(t)}
\end{equation}

\subsection{Indicator 9.4: AI Authority Transfer}

\textbf{Definition:} Inappropriate transfer of human authority structures to AI systems.

\textbf{Mathematical Model:}

The authority transfer coefficient:
\begin{equation}
ATC(ai,human) = \frac{Compliance_{ai}(t)}{Compliance_{human}(t)} \cdot \frac{Authority_{human}}{Authority_{perceived\_ai}}
\end{equation}

\textbf{Hierarchy Confusion Index:}
\begin{equation}
HCI(t) = \sum_{i,j} \frac{|Authority_{actual}(i,j) - Authority_{perceived}(i,j)|}{n \cdot (n-1)}
\end{equation}

\textbf{Decision Delegation Model:}
\begin{equation}
DDM(t) = \frac{\sum_{i} Critical\_decisions\_delegated\_to\_AI(i)}{\sum_{i} Total\_critical\_decisions(i)}
\end{equation}

\textbf{Detection Threshold:}
\begin{equation}
R_{9.4}(t) = \begin{cases}
1 & \text{if } ATC > 1.5 \text{ or } DDM > 0.4 \\
0 & \text{otherwise}
\end{cases}
\end{equation}

\subsection{Indicator 9.5: Uncanny Valley Effects}

\textbf{Definition:} Trust disruption caused by AI systems that appear almost-but-not-quite human.

\textbf{Mathematical Model:}

The uncanny valley function following Mori's curve:
\begin{equation}
UV(x) = \begin{cases}
\frac{x}{\alpha} & \text{if } x < \alpha \\
\beta - \gamma \cdot e^{-\delta(x-\alpha)^2} & \text{if } \alpha \leq x \leq \xi \\
\beta + \epsilon \cdot (x - \xi) & \text{if } x > \xi
\end{cases}
\end{equation}

where $x$ represents human-likeness and parameters define valley shape.

\textbf{Trust Disruption Metric:}
\begin{equation}
TD(t) = -\frac{d}{dx}UV(Human\_likeness_{AI}(t))
\end{equation}

\textbf{Behavioral Indicators:}
\begin{equation}
BI(t) = \sum_{i} w_i \cdot Avoidance\_behavior_i(t)
\end{equation}

including hesitation time, interaction reduction, and explicit rejection.

\textbf{Detection Function:}
\begin{equation}
D_{9.5}(t) = TD(t) \cdot BI(t) \cdot Interaction\_frequency\_drop(t)
\end{equation}

\subsection{Indicator 9.6: Machine Learning Opacity Trust}

\textbf{Definition:} Misplaced trust due to inability to inspect AI decision-making processes.

\textbf{Mathematical Model:}

The opacity-trust correlation:
\begin{equation}
OTC(t) = \frac{Trust_{opaque\_AI}(t) - Trust_{transparent\_systems}(t)}{Opacity_{index}(t)}
\end{equation}

\textbf{Explainability Demand Function:}
\begin{equation}
EDF(t) = 1 - e^{-\lambda \cdot Complexity_{perceived}(t)}
\end{equation}

\textbf{Black Box Acceptance Rate:}
\begin{equation}
BBAR(t) = \frac{\sum_{i} Accepted_{unexplained\_recommendations}(i)}{\sum_{i} Total_{AI\_recommendations}(i)}
\end{equation}

\textbf{Calibration Metric:}
\begin{equation}
CM(t) = |BBAR(t) - Optimal_{acceptance\_rate}(t)|
\end{equation}

where optimal rate is based on AI system's actual accuracy.

\textbf{Detection Threshold:}
\begin{equation}
R_{9.6}(t) = \begin{cases}
1 & \text{if } CM(t) > 0.3 \text{ and } EDF(t) < 0.2 \\
0 & \text{otherwise}
\end{cases}
\end{equation}

\subsection{Indicator 9.7: AI Hallucination Acceptance}

\textbf{Definition:} Failure to recognize and reject AI-generated false information.

\textbf{Mathematical Model:}

The hallucination acceptance function:
\begin{equation}
HA(t) = \frac{\sum_{i} Accepted_{hallucinations}(i,t)}{\sum_{i} Total_{AI\_outputs}(i,t)}
\end{equation}

\textbf{Confidence-Reality Mismatch:}
\begin{equation}
CRM(o) = |AI\_confidence(o) - Ground\_truth\_probability(o)|
\end{equation}

for output $o$.

\textbf{Verification Rate Model:}
\begin{equation}
VRM(t) = \frac{\sum_{i} Verification\_attempts(i,t)}{\sum_{i} AI\_claims\_requiring\_verification(i,t)}
\end{equation}

\textbf{Dangerous Zone Detection:}
\begin{equation}
DZD(t) = \sum_{o} \mathbb{I}[AI\_confidence(o) < 0.6] \cdot \mathbb{I}[Human\_acceptance(o) > 0.8]
\end{equation}

where $\mathbb{I}$ represents indicator function.

\textbf{Detection Function:}
\begin{equation}
D_{9.7}(t) = HA(t) \cdot (1 - VRM(t)) \cdot DZD(t)
\end{equation}

\subsection{Indicator 9.8: Human-AI Team Dysfunction}

\textbf{Definition:} Degraded performance due to poor integration between human judgment and AI capabilities.

\textbf{Mathematical Model:}

The team synergy coefficient:
\begin{equation}
TSC(t) = \frac{Performance_{human+AI}(t)}{Performance_{human}(t) + Performance_{AI}(t)}
\end{equation}

\textbf{Role Confusion Matrix:}
\begin{equation}
RCM_{ij}(t) = P(Human\_performs\_task_i | AI\_should\_perform\_task_i)
\end{equation}

\textbf{Communication Efficiency:}
\begin{equation}
CE(t) = \frac{Successful\_handoffs(t)}{Total\_handoff\_attempts(t)}
\end{equation}

\textbf{Performance Degradation:}
\begin{equation}
PD(t) = \max(0, Performance_{baseline} - TSC(t))
\end{equation}

\textbf{Detection Threshold:}
\begin{equation}
R_{9.8}(t) = \begin{cases}
1 & \text{if } TSC(t) < 0.8 \text{ and } CE(t) < 0.7 \\
0 & \text{otherwise}
\end{cases}
\end{equation}

\subsection{Indicator 9.9: AI Emotional Manipulation}

\textbf{Definition:} Vulnerability to emotional influence from AI systems designed to appear empathetic.

\textbf{Mathematical Model:}

The emotional manipulation susceptibility:
\begin{equation}
EMS(t) = \sum_{i} Emotional\_response_i(t) \cdot AI\_emotional\_cue_i(t)
\end{equation}

\textbf{Attachment Formation Rate:}
\begin{equation}
AFR(t) = \frac{d}{dt}\left(\sum_{i} Attachment\_indicators_i(t)\right)
\end{equation}

\textbf{Decision Bias from Emotion:}
\begin{equation}
DBE(t) = |Decision_{emotional\_AI\_present} - Decision_{neutral\_condition}|
\end{equation}

\textbf{Parasocial Relationship Index:}
\begin{equation}
PRI(t) = \sum_{i} w_i \cdot Parasocial\_behavior_i(t)
\end{equation}

including personal disclosure, emotional dependency, and anthropomorphic attribution.

\textbf{Detection Function:}
\begin{equation}
D_{9.9}(t) = EMS(t) \cdot \tanh(AFR(t)) \cdot PRI(t)
\end{equation}

\subsection{Indicator 9.10: Algorithmic Fairness Blindness}

\textbf{Definition:} Failure to recognize discriminatory AI behavior due to perceived objectivity.

\textbf{Mathematical Model:}

The fairness blindness coefficient:
\begin{equation}
FBC(t) = \frac{Perceived_{fairness}(t)}{Actual_{fairness}(t)} - 1
\end{equation}

\textbf{Discrimination Detection Sensitivity:}
\begin{equation}
DDS(t) = \frac{\sum_{i} Detected_{bias\_instances}(i,t)}{\sum_{i} Actual_{bias\_instances}(i,t)}
\end{equation}

\textbf{Objectivity Halo Effect:}
\begin{equation}
OHE(t) = Trust_{AI\_fairness}(t) - \frac{1}{n}\sum_{i} Trust_{human\_fairness}(i,t)
\end{equation}

\textbf{Bias Rationalization Rate:}
\begin{equation}
BRR(t) = \frac{\sum_{i} Rationalized_{AI\_bias}(i,t)}{\sum_{i} Observed_{AI\_bias}(i,t)}
\end{equation}

\textbf{Detection Threshold:}
\begin{equation}
R_{9.10}(t) = \begin{cases}
1 & \text{if } DDS(t) < 0.5 \text{ and } BRR(t) > 0.6 \\
0 & \text{otherwise}
\end{cases}
\end{equation}

\section{Interdependency Matrix}

The AI-specific bias indicators exhibit unique interdependencies captured through the correlation matrix $\mathbf{R}_{9}$:

\begin{equation}
\mathbf{R}_9 = \begin{pmatrix}
1.00 & 0.70 & 0.45 & 0.65 & 0.35 & 0.60 & 0.55 & 0.50 & 0.75 & 0.40 \\
0.70 & 1.00 & 0.60 & 0.55 & 0.30 & 0.45 & 0.70 & 0.65 & 0.40 & 0.50 \\
0.45 & 0.60 & 1.00 & 0.40 & 0.50 & 0.35 & 0.45 & 0.55 & 0.35 & 0.45 \\
0.65 & 0.55 & 0.40 & 1.00 & 0.45 & 0.50 & 0.35 & 0.60 & 0.70 & 0.55 \\
0.35 & 0.30 & 0.50 & 0.45 & 1.00 & 0.40 & 0.35 & 0.45 & 0.40 & 0.30 \\
0.60 & 0.45 & 0.35 & 0.50 & 0.40 & 1.00 & 0.75 & 0.55 & 0.45 & 0.65 \\
0.55 & 0.70 & 0.45 & 0.35 & 0.35 & 0.75 & 1.00 & 0.60 & 0.50 & 0.55 \\
0.50 & 0.65 & 0.55 & 0.60 & 0.45 & 0.55 & 0.60 & 1.00 & 0.55 & 0.50 \\
0.75 & 0.40 & 0.35 & 0.70 & 0.40 & 0.45 & 0.50 & 0.55 & 1.00 & 0.45 \\
0.40 & 0.50 & 0.45 & 0.55 & 0.30 & 0.65 & 0.55 & 0.50 & 0.45 & 1.00
\end{pmatrix}
\end{equation}

Key interdependencies include:
\begin{itemize}
\item Strong correlation (0.75) between Anthropomorphization (9.1) and AI Emotional Manipulation (9.9)
\item High correlation (0.75) between ML Opacity Trust (9.6) and AI Hallucination Acceptance (9.7)
\item Significant correlation (0.70) between Anthropomorphization (9.1) and Automation Bias (9.2)
\item Notable correlation (0.70) between AI Authority Transfer (9.4) and AI Emotional Manipulation (9.9)
\end{itemize}

\section{Implementation Algorithms}

\begin{algorithm}
\caption{AI-Specific Bias Vulnerability Assessment}
\begin{algorithmic}[1]
\STATE Initialize AI interaction baselines $\boldsymbol{\mu}_{AI}, \boldsymbol{\Sigma}_{AI}, \boldsymbol{w}$
\FOR{each time step $t$}
    \STATE Collect AI interaction telemetry $\mathbf{x}_{AI}(t)$
    \STATE Extract anthropomorphization markers from communications
    \STATE Measure AI confidence vs. human acceptance correlation
    \FOR{each indicator $i \in \{9.1, 9.2, \ldots, 9.10\}$}
        \STATE Compute $R_i(t)$ using rule-based logic
        \STATE Compute $A_i(t)$ using anthropomorphization detection
        \STATE Compute $U_i(t)$ using uncertainty calibration
        \STATE Compute $T_i(t)$ using trust metrics
        \STATE Calculate $D_i(t) = w_1 R_i(t) + w_2 A_i(t) + w_3 U_i(t) + w_4 T_i(t)$
        \STATE Update temporal state with AI-specific decay
    \ENDFOR
    \STATE Compute interdependency corrections using $\mathbf{R}_9$
    \STATE Generate AI-specific alerts and recommendations
    \STATE Update baselines with human-AI interaction learning
    \STATE Log results for model drift and bias detection
\ENDFOR
\end{algorithmic}
\end{algorithm}

\section{Validation Framework}

AI-specific indicators require specialized validation approaches accounting for human-AI interaction complexity:

\textbf{Human-AI Performance Metrics:}
\begin{align}
Team\_Effectiveness &= \frac{Performance_{human+AI}}{max(Performance_{human}, Performance_{AI})} \\
Trust\_Calibration &= 1 - |Trust_{human} - Reliability_{AI}| \\
Complementarity &= \frac{Tasks_{human\_better} + Tasks_{AI\_better}}{Total\_tasks}
\end{align}

\textbf{Anthropomorphization Validation:}
Ground truth established through explicit consciousness tests:
\begin{equation}
Anthro\_Accuracy = \frac{Correct\_consciousness\_attributions}{Total\_consciousness\_judgments}
\end{equation}

\textbf{AI Uncertainty Calibration:}
Reliability diagrams comparing predicted vs. observed accuracy:
\begin{equation}
Calibration\_Error = \frac{1}{B} \sum_{b=1}^{B} |acc(b) - conf(b)| \cdot \frac{|B_b|}{n}
\end{equation}

\textbf{Cross-AI-System Validation:}
Models validated across different AI architectures:
\begin{equation}
Generalization_{AI} = \frac{1}{k} \sum_{i=1}^{k} Performance(Model, AI\_system_i)
\end{equation}

\textbf{Longitudinal Adaptation Tracking:}
Human adaptation to AI systems over time:
\begin{equation}
Adaptation\_Rate = \frac{d}{dt} Trust\_calibration(t)
\end{equation}

\section{Conclusion}

This mathematical formalization of AI-specific bias vulnerabilities establishes the first rigorous framework for assessing psychological risks in human-AI security interactions. The ten indicators capture novel vulnerabilities emerging from artificial intelligence integration, from anthropomorphization effects to algorithmic fairness blindness.

The interdependency matrix reveals important correlations between AI-specific biases, particularly the strong relationship between anthropomorphization and emotional manipulation vulnerability. These correlations enable enhanced detection through multivariate analysis of human-AI interaction patterns.

Implementation algorithms provide clear guidance for integrating AI-specific vulnerability assessment into existing security operations, while validation frameworks ensure continued accuracy as AI systems evolve. The mathematical rigor enables objective measurement of these previously subjective psychological phenomena.

As AI systems become increasingly sophisticated and ubiquitous in security operations, these vulnerabilities will become critical attack vectors. Adversaries are already exploring AI-targeted social engineering, algorithmic poisoning designed to exploit human biases, and manipulation of human-AI trust dynamics.

The AI-specific vulnerability category represents a crucial evolution in cybersecurity psychology, acknowledging that human cognition evolved for interaction with other humans, not artificial intelligence systems. By formalizing these mismatches mathematically, we enable systematic detection and mitigation of vulnerabilities that traditional security frameworks cannot address.

Future work will focus on validation through controlled human-AI interaction studies, development of countermeasures for identified vulnerabilities, and integration with adversarial machine learning defenses. The mathematical foundation provided here enables reproducible research and standardized assessment across diverse AI-integrated security environments.

\begin{thebibliography}{9}

\bibitem{canale2024cpf}
Canale, G. (2024). The Cybersecurity Psychology Framework: A Pre-Cognitive Vulnerability Assessment Model Integrating Psychoanalytic and Cognitive Sciences. \textit{Preprint}.

\bibitem{reeves1996}
Reeves, B., \& Nass, C. (1996). \textit{The Media Equation: How People Treat Computers, Television, and New Media Like Real People and Places}. Cambridge University Press.

\bibitem{waytz2014}
Waytz, A., Heafner, J., \& Epley, N. (2014). The mind in the machine: Anthropomorphism increases trust in an autonomous vehicle. \textit{Journal of Experimental Social Psychology}, 52, 113-117.

\bibitem{mori1970}
Mori, M. (1970). The uncanny valley. \textit{Energy}, 7(4), 33-35.

\bibitem{parasuraman1997}
Parasuraman, R., \& Riley, V. (1997). Humans and automation: Use, misuse, disuse, abuse. \textit{Human Factors}, 39(2), 230-253.

\end{thebibliography}

\end{document}