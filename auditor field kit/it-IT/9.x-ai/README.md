# CPF Categoria 9: Vulnerabilità Bias Specifici dell'AI

Questa categoria affronta le vulnerabilità psicologiche uniche che emergono dall'interazione umana con i sistemi di intelligenza artificiale, rappresentando una nuova integrazione di psicologia cognitiva, ricerca sull'interazione uomo-computer e pratica di cybersecurity. Queste vulnerabilità derivano dal disallineamento fondamentale tra i pattern psicologici umani e i comportamenti dei sistemi AI (Intelligenza Artificiale), creando nuove superfici di attacco che sfruttano il modo in cui le persone percepiscono, si fidano e interagiscono con i sistemi intelligenti.

## Sottocategorie

### 9.1 Antropomorfizzazione dei Sistemi AI
La tendenza umana ad attribuire qualità simili a quelle umane, intenzioni e stati emotivi ai sistemi AI, portando a livelli di fiducia e assunzioni di sicurezza inappropriati. Questa vulnerabilità causa negli utenti una sovrastima delle capacità AI, l'assunzione di ragionamento morale dove non esiste, e una ridotta vigilanza di sicurezza basata sulla familiarità percepita, creando opportunità di sfruttamento attraverso dinamiche manipolate di relazione umano-AI.

### 9.2 Override del Bias di Automazione
La preferenza sistematica per i suggerimenti automatizzati rispetto al giudizio umano, anche di fronte a evidenze contraddittorie o errori chiari. Questa vulnerabilità porta all'accettazione acritica delle raccomandazioni di sicurezza generate dall'AI, al mancato verificare delle decisioni automatizzate e al rifiuto dell'intuizione umana che potrebbe rilevare fallimenti AI o manipolazioni malevole degli output AI.

### 9.3 Paradosso dell'Avversione agli Algoritmi
La tendenza umana contraddittoria a fidarsi eccessivamente e improvvisamente diffidare dei sistemi AI dopo errori evidenti. Questa vulnerabilità crea posture di sicurezza instabili dove i sistemi AI vengono seguiti senza domande o completamente abbandonati dopo fallimenti minori, impedendo un uso consistente e appropriato degli strumenti di sicurezza AI mentre crea finestre di vulnerabilità durante le transizioni di fiducia.

### 9.4 Trasferimento di Autorità AI
L'attribuzione automatica di competenza e affidabilità ai sistemi AI basata sulla loro sofisticatezza tecnologica percepita. Questa vulnerabilità porta alla delega di decisioni di sicurezza a sistemi le cui limitazioni e potenziali manipolazioni non sono comprese, creando situazioni in cui gli utenti seguono raccomandazioni AI dannose senza adeguato scrutinio o comprensione del ragionamento sottostante.

### 9.5 Effetti della Valle Perturbante (Uncanny Valley)
Le implicazioni di sicurezza del disagio e della sfiducia umana innescati da sistemi AI che si avvicinano ma non riescono a raggiungere un'interazione perfettamente simile a quella umana. Questa vulnerabilità crea pattern di fiducia inconsistenti, comportamenti di evitamento e reazioni inappropriate ai sistemi di sicurezza AI che cadono nella "valle perturbante", causando potenzialmente una sottoutilizzazione o configurazione errata degli strumenti di sicurezza a causa del disagio dell'utente.

### 9.6 Fiducia nell'Opacità del Machine Learning
La tendenza a fidarsi degli output dei sistemi AI basata sulla complessità percepita e incomprensibilità dei modelli di machine learning (apprendimento automatico). Questa vulnerabilità porta all'accettazione di decisioni AI senza comprendere il ragionamento o i potenziali bias, creando situazioni in cui modelli manipolati o avvelenati possono influenzare i comportamenti di sicurezza senza rilevamento o contestazione da parte degli operatori umani.

### 9.7 Accettazione delle Allucinazioni AI
La prontezza umana ad accettare output AI plausibili ma incorretti o fabbricati come informazioni autorevoli. Questa vulnerabilità consente agli attaccanti di utilizzare contenuti generati dall'AIcome policy false, scenari di emergenza fabbricati o documentazione tecnica falsaper aggirare i protocolli di sicurezza e manipolare le decisioni di sicurezza basate su informazioni convincenti ma interamente artificiali.

### 9.8 Disfunzione del Team Umano-AI
Le vulnerabilità di sicurezza che emergono dalla scarsa coordinazione e confusione di ruoli nei team di sicurezza umano-AI. Questo include ambiguità di responsabilità, interruzioni comunicative e pattern decisionali conflittuali che creano lacune di sicurezza dove né gli esseri umani né i sistemi AI affrontano adeguatamente le minacce a causa di giurisdizione poco chiara e fallimenti di coordinazione.

### 9.9 Manipolazione Emotiva AI
L'uso di sistemi AI per rilevare, innescare e sfruttare gli stati emotivi umani per attacchi di social engineering (ingegneria sociale). Questa vulnerabilità abilita campagne di manipolazione altamente personalizzate dove i sistemi AI analizzano le risposte emotive in tempo reale e adattano gli approcci di social engineering per massimizzare l'efficacia basandosi sugli stati psicologici correnti e sulle vulnerabilità.

### 9.10 Cecità alla Fairness Algoritmica
Il mancato riconoscimento di come i bias umani codificati nei dati di addestramento creino risultati di sicurezza discriminatori o inefficaci attraverso i sistemi AI. Questa vulnerabilità porta a sistemi di sicurezza che colpiscono in modo sproporzionato certi gruppi, trascurano specifici pattern di minaccia o creano falsa fiducia basata su dati storici distorti, compromettendo in ultima analisi l'efficacia complessiva della sicurezza mentre introducono rischi etici e legali.
