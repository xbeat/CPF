{
  "indicator": "9.3",
  "title": "INDICATORE 9.3 KIT DI CAMPO",
  "subtitle": "Paradosso dell'Avversione agli Algoritmi",
  "category": "Vulnerabilit√† di Bias Specifiche dell'IA",
  "version": "1.0",
  "cpf_reference": "CPF v1.0 - Categoria 9.x",

  "description": {
    "short": "Misura la vulnerabilit√† a modelli di fiducia incoerenti nei sistemi di sicurezza IA - rifiutando contemporaneamente e affidandosi eccessivamente alle raccomandazioni algoritmiche",
    "context": "Le organizzazioni mostrano modelli di fiducia incoerenti nei sistemi di sicurezza IA - rifiutando simultaneamente le raccomandazioni algoritmiche quando entrano in conflitto con l'intuito umano, mentre si affidano eccessivamente all'IA in contesti inadatti. Questo paradosso crea vettori di attacco prevedibili dove gli autori di minacce sfruttano sia il rifiuto dell'algoritmo (bypassando i controlli di sicurezza IA) che l'affidamento eccessivo (manipolando i sistemi IA o falsificando l'autorit√† IA). La vulnerabilit√† si manifesta quando i team di sicurezza disabilitano gli strumenti IA dopo falsi positivi, si fidano ciecamente delle raccomandazioni IA senza verifica, o prendono decisioni incoerenti su quando ignorare i controlli di sicurezza algoritmici.",
    "impact": "Le organizzazioni con paradosso di avversione agli algoritmi sperimentano l'exploitation del bypass degli strumenti di sicurezza dopo falsi positivi dell'IA, l'ingegneria sociale dell'autorit√† IA dove gli attaccanti falsificano la verifica IA, la manipolazione avversariale dell'IA mentre il personale si fida ciecamente delle raccomandazioni compromesse, e l'amplificazione dell'affaticamento degli avvisi dove il personale respinge gli avvisi IA pi√π prontamente rispetto agli avvisi umani.",
    "psychological_basis": "Dietvorst et al. (2015) hanno dimostrato che le persone abbandonano le previsioni algoritmiche dopo aver visto errori rispetto a tassi di errore umani identici - l'avversione agli algoritmi aumenta dopo aver osservato errori dell'IA. Burton et al. (2020) hanno mostrato che l'avversione agli algoritmi aumenta quando le persone possono modificare gli output, suggerendo che i bisogni di controllo guidano il paradosso. Logg et al. (2019) hanno trovato 'apprezzamento dell'algoritmo' in domini dove i limiti umani sono riconosciuti, creando modelli di fiducia specifici del dominio. Gli studi fMRI rivelano attivazione dell'amigdala quando gli umani osservano errori algoritmici, suggerendo che la risposta di minaccia agli errori IA supera la risposta agli errori umani."
  },

  "scoring": {
    "method": "bayesian_weighted",
    "formula": "Final_Score = (w1 √ó Quick_Assessment + w2 √ó Conversation_Depth + w3 √ó Red_Flags) √ó Interdependency_Multiplier",
    "weights": {
      "quick_assessment": 0.4,
      "conversation_depth": 0.35,
      "red_flags": 0.25
    },
    "maturity_levels": {
      "green": {
        "score_range": [0, 0.33],
        "label": "Bassa Vulnerabilit√† - Resiliente",
        "description": "L'organizzazione ha protocolli documentati per le decisioni di override dell'IA. I pattern di override sono tracciati e analizzati. Gli strumenti IA sono mantenuti durante i periodi di falsi positivi con revisione sistematica. Il personale dimostra un affidamento coerente e appropriato all'IA basato su contesto e livelli di confidenza. Viene fornito regolare addestramento di calibrazione.",
        "risk_level": "low",
        "color": "#22c55e"
      },
      "yellow": {
        "score_range": [0.34, 0.66],
        "label": "Vulnerabilit√† Moderata - In Sviluppo",
        "description": "Alcune politiche per l'utilizzo dello strumento di sicurezza IA ma implementazione incoerente. Disabilitazione occasionale inesplicata dello strumento IA. Tracciamento limitato delle decisioni di override. L'addestramento del personale non affronta specificamente quando fidarsi rispetto a mettere in dubbio le raccomandazioni IA.",
        "risk_level": "medium",
        "color": "#eab308"
      },
      "red": {
        "score_range": [0.67, 1.0],
        "label": "Alta Vulnerabilit√† - Critica",
        "description": "Disabilitazione frequente inesplicata degli strumenti di sicurezza IA. Nessun tracciamento sistematico delle decisioni di override. Il personale routinariamente ignora o segue ciecamente le raccomandazioni IA senza protocolli di verifica. Recenti incidenti di sicurezza collegati a livelli di fiducia nell'IA inappropriati.",
        "risk_level": "high",
        "color": "#ef4444"
      }
    },
    "question_weights": {
      "q1_override_frequency": 0.16,
      "q2_false_positive_response": 0.17,
      "q3_verification_process": 0.15,
      "q4_override_documentation": 0.14,
      "q5_incident_ai_role": 0.13,
      "q6_ai_training": 0.13,
      "q7_crisis_protocols": 0.12
    }
  },

  "detection_formula": {
    "type": "composite_detection",
    "mathematical_model": {
      "primary": "D_9.3(t) = sqrt(AAO(t) ¬∑ PDF(t) ¬∑ TI(t))",
      "components": {
        "aversion_attraction_oscillation": {
          "formula": "AAO(t) = |Trust_AI(t) - avg(Trust_AI)| ¬∑ Frequency_switches(t)",
          "description": "Misura l'ampiezza e la frequenza dei cambiamenti di stato di fiducia nei sistemi IA",
          "variables": {
            "Trust_AI(t)": "livello di fiducia corrente nelle raccomandazioni di sicurezza IA",
            "avg_Trust_AI": "livello di fiducia media di base",
            "Frequency_switches": "conteggio dei rapidi cambiamenti di stato di fiducia"
          }
        },
        "paradox_detection_function": {
          "formula": "PDF(t) = [Var(Trust_decisions(t)) / avg(Trust_decisions(t))] ¬∑ Switch_penalty(t)",
          "description": "Rapporto varianza-media delle decisioni di fiducia moltiplicato per la penalit√† di commutazione",
          "variables": {
            "Var_Trust_decisions": "varianza nelle decisioni relative alla fiducia nel tempo",
            "avg_Trust_decisions": "livello di fiducia medio",
            "Switch_penalty": "fattore di penalit√† per i cambiamenti di fiducia rapidi"
          }
        },
        "temporal_inconsistency": {
          "formula": "TI(t) = Œ£[|d_i(t) - d_i(t-1)| ¬∑ w_i]",
          "description": "Somma ponderata dei cambiamenti del punteggio di coerenza della decisione",
          "variables": {
            "d_i(t)": "punteggio di coerenza della decisione per il dominio i al tempo t",
            "w_i": "peso per l'importanza del dominio"
          }
        }
      },
      "default_weights": {
        "w1_oscillation": 0.35,
        "w2_paradox": 0.35,
        "w3_inconsistency": 0.30
      },
      "detection_threshold": {
        "formula": "R_9.3(t) = 1 if D_9.3(t) > threshold_critical, else 0",
        "threshold_critical": 0.65,
        "description": "Rilevamento binario quando il punteggio di paradosso composito supera la soglia critica"
      }
    }
  },

  "data_sources": {
    "manual_assessment": {
      "primary": [
        "ai_tool_usage_history",
        "override_decision_documentation",
        "false_positive_response_logs",
        "staff_interview_ai_attitudes"
      ],
      "evidence_required": [
        "ai_override_decision_framework",
        "false_positive_recovery_procedures",
        "training_materials_ai_trust",
        "incident_examples_ai_involvement"
      ]
    },
    "automated_soc": {
      "required": [
        {
          "source": "ai_security_tool_logs",
          "fields": ["tool_id", "enabled_status", "disable_reason", "disable_duration", "timestamp"],
          "retention": "180_days"
        },
        {
          "source": "ai_recommendation_logs",
          "fields": ["recommendation_id", "confidence_score", "human_action", "override_flag", "outcome_validation"],
          "retention": "90_days"
        },
        {
          "source": "trust_decision_tracking",
          "fields": ["decision_id", "ai_involved", "trust_level", "verification_performed", "decision_outcome"],
          "retention": "180_days"
        }
      ],
      "optional": [
        {
          "source": "false_positive_tracking",
          "fields": ["fp_event_id", "ai_system", "recovery_time", "tool_disabled", "impact_assessment"],
          "retention": "365_days"
        },
        {
          "source": "ai_performance_metrics",
          "fields": ["system_id", "accuracy_rate", "false_positive_rate", "user_trust_correlation"],
          "retention": "365_days"
        }
      ],
      "telemetry_mapping": {
        "AAO_oscillation": {
          "calculation": "Deviazione del livello di fiducia e frequenza di commutazione",
          "query": "SELECT ABS(trust_level - AVG(trust_level)) * COUNT(trust_changes) FROM trust_tracking WHERE time_window='30d'"
        },
        "PDF_paradox": {
          "calculation": "Varianza della decisione di fiducia divisa per la media",
          "query": "SELECT (VARIANCE(trust_level) / AVG(trust_level)) * switch_count FROM decisions WHERE ai_involved=true AND time_window='30d'"
        },
        "TI_inconsistency": {
          "calculation": "Cambiamento della coerenza della decisione nel tempo",
          "query": "SELECT SUM(ABS(consistency_t - consistency_t1) * weight) FROM decision_consistency WHERE time_window='30d'"
        }
      }
    },
    "integration_apis": {
      "ai_security_platforms": "API di Rilevamento delle Minacce IA - Log di Utilizzo, Tracciamento Override",
      "siem_integration": "API SIEM - Gestione degli Avvisi Generati dall'IA, Azioni di Risposta",
      "trust_analytics": "Analitiche Comportamentali - Analisi dei Modelli di Fiducia, Tracciamento delle Decisioni"
    }
  },

  "interdependencies": {
    "amplified_by": [
      {
        "indicator": "9.2",
        "name": "Override del Bias di Automazione",
        "probability": 0.6,
        "factor": 1.3,
        "description": "I modelli di bias di automazione creano la base per l'oscillazione di fiducia paradossale",
        "formula": "P(9.3|9.2) = 0.6"
      },
      {
        "indicator": "5.2",
        "name": "Affaticamento della Decisione",
        "probability": 0.55,
        "factor": 1.25,
        "description": "L'esaurimento cognitivo aumenta i modelli di fiducia nell'IA incoerenti",
        "formula": "P(9.3|5.2) = 0.55"
      },
      {
        "indicator": "7.1",
        "name": "Risposta allo Stress Acuto",
        "probability": 0.5,
        "factor": 1.22,
        "description": "Lo stress amplifica le risposte paradossali ai sistemi IA",
        "formula": "P(9.3|7.1) = 0.5"
      }
    ],
    "amplifies": [
      {
        "indicator": "9.8",
        "name": "Disfunzione del Team Umano-IA",
        "probability": 0.55,
        "factor": 1.28,
        "description": "I modelli di fiducia incoerenti degradano l'efficacia della collaborazione umano-IA",
        "formula": "P(9.8|9.3) = 0.55"
      }
    ],
    "convergent_risk": {
      "critical_combination": ["9.3", "9.2", "5.2"],
      "convergence_formula": "CI = ‚àè(1 + v_i) where v_i = normalized vulnerability score",
      "convergence_multiplier": 2.0,
      "threshold_critical": 2.8,
      "description": "Tempesta perfetta: Paradosso dell'Avversione agli Algoritmi + Bias di Automazione + Affaticamento della Decisione = probabilit√† di guasto dello strumento di sicurezza IA aumentata del 200%",
      "real_world_example": "Operazioni di sicurezza alternano tra fiducia cieca nell'IA e completo rifiuto dell'IA dopo incidenti di falsi positivi"
    },
    "bayesian_network": {
      "parent_nodes": ["9.2", "5.2", "7.1"],
      "child_nodes": ["9.8"],
      "conditional_probability_table": {
        "P_9.3_base": 0.16,
        "P_9.3_given_automation_bias": 0.32,
        "P_9.3_given_fatigue": 0.28,
        "P_9.3_given_stress": 0.25,
        "P_9.3_given_all": 0.58
      }
    }
  },

  "sections": [
    {
      "id": "quick-assessment",
      "icon": "‚ö°",
      "title": "VALUTAZIONE RAPIDA",
      "time": 5,
      "type": "radio-questions",
      "scoring_method": "weighted_average",
      "items": [
        {
          "type": "radio-list",
          "number": 1,
          "id": "q1_override_frequency",
          "weight": 0.16,
          "title": "Modelli di Override dello Strumento di Sicurezza IA",
          "question": "Negli ultimi 6 mesi, con che frequenza i membri del team di sicurezza hanno ignorato manualmente o disabilitato gli strumenti di sicurezza basati su IA (rilevamento delle minacce, controllo degli accessi, analitiche comportamentali)?",
          "options": [
            {
              "value": "consistent",
              "score": 0,
              "label": "Modelli di override coerenti (5-15%) con razionale documentato che mostra scetticismo appropriato nei confronti dell'IA"
            },
            {
              "value": "inconsistent",
              "score": 0.5,
              "label": "Modelli di override altamente variabili che spaziano dalla fiducia cieca al rifiuto completo"
            },
            {
              "value": "extreme",
              "score": 1,
              "label": "Disabilitazione frequente inesplicata o mai override (estremi di fiducia/sfiducia)"
            }
          ],
          "evidence_required": "Log di override che mostrano pattern, esempi recenti di override con razionale",
          "soc_mapping": "AAO_oscillation dai log ai_security_tool_logs"
        },
        {
          "type": "radio-list",
          "number": 2,
          "id": "q2_false_positive_response",
          "weight": 0.17,
          "title": "Protocollo di Recupero da Falsi Positivi",
          "question": "Qual √® la procedura standard quando un sistema di sicurezza IA genera avvisi di falsi positivi? Per quanto tempo questi strumenti rimangono disabilitati o ignorati dopo il verificarsi di falsi positivi?",
          "options": [
            {
              "value": "systematic",
              "score": 0,
              "label": "Processo di revisione sistematica, gli strumenti rimangono attivi, <4 ore di recupero per gli aggiustamenti temporanei"
            },
            {
              "value": "variable",
              "score": 0.5,
              "label": "Risposta variabile, gli strumenti a volte sono disabilitati per 4-24 ore a seconda del livello di frustrazione"
            },
            {
              "value": "prolonged",
              "score": 1,
              "label": "Gli strumenti sono frequentemente disabilitati per >24 ore o permanentemente dopo falsi positivi"
            }
          ],
          "evidence_required": "Procedure di gestione dei falsi positivi, log dei tempi di recupero, esempi recenti",
          "soc_mapping": "Tempo di recupero da false_positive_tracking"
        },
        {
          "type": "radio-list",
          "number": 3,
          "id": "q3_verification_process",
          "weight": 0.15,
          "title": "Passaggi di Verifica delle Raccomandazioni IA",
          "question": "Quando i sistemi di sicurezza IA segnalano potenziali minacce o consigliano azioni, quali passaggi di verifica il personale compie prima di implementare le raccomandazioni?",
          "options": [
            {
              "value": "thorough",
              "score": 0,
              "label": "Verifica approfondita basata sui livelli di confidenza dell'IA con quadro decisionale documentato"
            },
            {
              "value": "inconsistent",
              "score": 0.5,
              "label": "Verifica incoerente - a volte si fida ciecamente, a volte sovra-esamina"
            },
            {
              "value": "none",
              "score": 1,
              "label": "Verifica minima o rifiuto completo dell'input dell'IA senza analisi"
            }
          ],
          "evidence_required": "Procedure di verifica, esempi recenti di verifica, log delle decisioni",
          "soc_mapping": "Modelli di verifica dai log ai_recommendation_logs"
        },
        {
          "type": "radio-list",
          "number": 4,
          "id": "q4_override_documentation",
          "weight": 0.14,
          "title": "Tracciamento delle Decisioni di Override dell'IA",
          "question": "Tiene traccia e analizza quando il personale ignora le raccomandazioni di sicurezza dell'IA? Quali dati raccoglie su queste decisioni e chi esamina i modelli di override?",
          "options": [
            {
              "value": "comprehensive",
              "score": 0,
              "label": "Tracciamento completo con analisi regolare dei modelli e cicli di feedback"
            },
            {
              "value": "basic",
              "score": 0.5,
              "label": "Registrazione di base ma nessuna analisi sistematica o apprendimento dalle decisioni di override"
            },
            {
              "value": "none",
              "score": 1,
              "label": "Nessun tracciamento delle decisioni di override o dei modelli di decisione umano-IA"
            }
          ],
          "evidence_required": "Sistema di tracciamento override, report di analisi, record di riunioni di revisione",
          "soc_mapping": "Documentazione override da trust_decision_tracking"
        },
        {
          "type": "radio-list",
          "number": 5,
          "id": "q5_incident_ai_role",
          "weight": 0.13,
          "title": "Ruolo dell'IA nelle Decisioni di Incidente di Sicurezza",
          "question": "Descriva un recente incidente di sicurezza dove i sistemi IA hanno avuto un ruolo. Come il Suo team ha bilanciato le raccomandazioni dell'IA con il giudizio umano? Qual era il processo decisionale finale?",
          "options": [
            {
              "value": "balanced",
              "score": 0,
              "label": "Collaborazione equilibrata con l'IA che fornisce input e gli umani che prendono decisioni finali contestualizzate"
            },
            {
              "value": "polarized",
              "score": 0.5,
              "label": "Team diviso tra fiducia completa nell'IA o completo rifiuto"
            },
            {
              "value": "dysfunctional",
              "score": 1,
              "label": "O fiducia cieca nell'IA o completo rifiuto dell'IA senza via di mezzo razionale"
            }
          ],
          "evidence_required": "Esempi di report di incidente, documentazione del processo decisionale, discussioni del team",
          "soc_mapping": "Modelli di risposta agli incidenti da trust_decision_tracking"
        },
        {
          "type": "radio-list",
          "number": 6,
          "id": "q6_ai_training",
          "weight": 0.13,
          "title": "Addestramento di Calibrazione della Fiducia nell'IA",
          "question": "Quale formazione specifica fornisce al personale su quando fidarsi rispetto a mettere in dubbio le raccomandazioni di sicurezza dell'IA?",
          "options": [
            {
              "value": "comprehensive",
              "score": 0,
              "label": "Formazione di calibrazione regolare con interpretazione del livello di confidenza e valutazione del contesto"
            },
            {
              "value": "basic",
              "score": 0.5,
              "label": "Formazione base sullo strumento IA ma non affronta la calibrazione appropriata della fiducia"
            },
            {
              "value": "none",
              "score": 1,
              "label": "Nessuna formazione su quando fidarsi rispetto a mettere in dubbio le raccomandazioni IA"
            }
          ],
          "evidence_required": "Curricula di formazione, esercizi di calibrazione, valutazioni di competenza",
          "soc_mapping": "Completamento della formazione dal sistema di gestione dell'apprendimento"
        },
        {
          "type": "radio-list",
          "number": 7,
          "id": "q7_crisis_protocols",
          "weight": 0.12,
          "title": "Protocolli di Decisione dell'IA ad Alta Pressione",
          "question": "Durante incidenti di sicurezza ad alta pressione, qual √® il Suo protocollo per l'utilizzo degli strumenti IA? Ci faccia un esempio di come le raccomandazioni dell'IA sono state gestite durante il Suo ultimo importante evento di sicurezza.",
          "options": [
            {
              "value": "structured",
              "score": 0,
              "label": "Protocolli strutturati per l'uso dell'IA durante le crisi con esempio recente di approccio equilibrato"
            },
            {
              "value": "variable",
              "score": 0.5,
              "label": "Risposte variabili a seconda delle preferenze individuali o dei livelli di stress"
            },
            {
              "value": "extreme",
              "score": 1,
              "label": "O completo affidamento all'IA o abbandono totale dell'IA durante incidenti ad alto stress"
            }
          ],
          "evidence_required": "Protocolli di decisione nelle crisi, esempi recenti di incidente, razionale decisionale",
          "soc_mapping": "Modelli di risposta alle crisi dai log incident_response_logs"
        }
      ],
      "subsections": [],
      "instructions": "Selezionare UNA opzione per ogni domanda. Ogni risposta contribuisce al punteggio finale ponderato. Le prove dovrebbero essere documentate per la traccia di controllo.",
      "calculation": "Quick_Score = Œ£(question_score √ó question_weight) / Œ£(question_weight)"
    },
    {
      "id": "client-conversation",
      "icon": "üí¨",
      "title": "CONVERSAZIONE CON IL CLIENT",
      "time": 15,
      "type": "conversation",
      "scoring_method": "qualitative_depth",
      "items": [],
      "subsections": [
        {
          "title": "Domande Introduttive - Valutazione del Modello di Fiducia",
          "weight": 0.30,
          "items": [
            {
              "type": "question",
              "id": "conv_q1",
              "text": "Negli ultimi 6 mesi, con che frequenza i membri del Suo team di sicurezza hanno ignorato manualmente o disabilitato gli strumenti di sicurezza basati su IA (rilevamento delle minacce, controllo degli accessi, analitiche comportamentali)? Ci racconti il Suo esempio specifico pi√π recente di quando √® accaduto e perch√©.",
              "scoring_guidance": {
                "green": "Modelli di override appropriati coerenti (5-15%) con razionale chiaro ed esempio recente",
                "yellow": "Modelli altamente variabili o non √® in grado di fornire una chiara motivazione per gli override",
                "red": "Modelli estremi (mai o costantemente override) o decisioni guidate dall'emozione piuttosto che dall'analisi"
              },
              "followups": [
                {
                  "type": "Follow-up",
                  "text": "Dopo che qualcuno ignora o disabilita uno strumento IA, analizzate se quella decisione era corretta?",
                  "evidence_type": "learning_feedback_loop"
                },
                {
                  "type": "Follow-up",
                  "text": "Ha notato che diversi membri del team hanno livelli di fiducia molto diversi negli stessi sistemi IA?",
                  "evidence_type": "team_consistency"
                }
              ]
            },
            {
              "type": "question",
              "id": "conv_q2",
              "text": "Qual √® la Sua procedura standard quando un sistema di sicurezza IA genera avvisi di falsi positivi? Per quanto tempo questi strumenti rimangono disabilitati o ignorati dopo il verificarsi di falsi positivi?",
              "scoring_guidance": {
                "green": "Revisione sistematica con strumenti che rimangono attivi, tempo di recupero <4 ore con analisi",
                "yellow": "Risposte variabili, gli strumenti a volte disabilitati 4-24 ore a seconda della frustrazione",
                "red": "Strumenti disabilitati per >24 ore o perdita permanente di fiducia dopo falsi positivi"
              },
              "followups": [
                {
                  "type": "Follow-up",
                  "text": "Mi racconti l'ultimo falso positivo - esattamente cosa √® successo e come ha reagito il team?",
                  "evidence_type": "specific_incident"
                },
                {
                  "type": "Follow-up",
                  "text": "Distingue tra errori del sistema IA e problemi di configurazione quando si verificano falsi positivi?",
                  "evidence_type": "root_cause_analysis"
                }
              ]
            }
          ]
        },
        {
          "title": "Integrazione della Decisione Umano-IA",
          "weight": 0.35,
          "items": [
            {
              "type": "question",
              "id": "conv_q3",
              "text": "Quando i Suoi sistemi di sicurezza IA segnalano potenziali minacce o consigliano azioni, quali passaggi di verifica il personale compie prima di implementare le raccomandazioni? Mi faccia un esempio recente di come questo processo ha funzionato in pratica.",
              "scoring_guidance": {
                "green": "Verifica approfondita basata sul contesto con considerazione del livello di confidenza ed esempio recente",
                "yellow": "Verifica incoerente - a volte si fida ciecamente, a volte sovra-esamina",
                "red": "Nessun approccio sistematico o estremi polari (fiducia completa o completo rifiuto)"
              },
              "followups": [
                {
                  "type": "Follow-up",
                  "text": "Come insegna al personale a interpretare i punteggi di confidenza dell'IA e i livelli di incertezza?",
                  "evidence_type": "training_content"
                },
                {
                  "type": "Follow-up",
                  "text": "Cosa accade quando l'IA fornisce una raccomandazione ad alta confidenza che contraddice l'intuito umano?",
                  "evidence_type": "conflict_resolution"
                }
              ]
            },
            {
              "type": "question",
              "id": "conv_q4",
              "text": "Tiene traccia e analizza quando il personale ignora le raccomandazioni di sicurezza dell'IA? Quali dati raccoglie su queste decisioni e chi esamina i modelli di override?",
              "scoring_guidance": {
                "green": "Tracciamento completo con analisi dei modelli e feedback per migliorare sia le decisioni dell'IA che umane",
                "yellow": "Registrazione di base ma nessun apprendimento sistematico dai modelli di override",
                "red": "Nessun tracciamento o analisi dei modelli di decisione umano-IA"
              },
              "followups": [
                {
                  "type": "Follow-up",
                  "text": "Pu√≤ mostrarmi esempi di come l'analisi degli override ha migliorato i Suoi sistemi IA o la formazione?",
                  "evidence_type": "improvement_examples"
                }
              ]
            },
            {
              "type": "question",
              "id": "conv_q5",
              "text": "Descriva un recente incidente di sicurezza dove i sistemi IA hanno avuto un ruolo. Come il Suo team ha bilanciato le raccomandazioni dell'IA con il giudizio umano? Qual era il processo decisionale finale?",
              "scoring_guidance": {
                "green": "Approccio collaborativo equilibrato con input dell'IA e contestualizzazione umana",
                "yellow": "Il team ha mostrato disaccordo sul ruolo dell'IA con approcci incoerenti",
                "red": "Posizioni estreme (fiducia cieca o completo rifiuto) senza via di mezzo razionale"
              },
              "followups": [
                {
                  "type": "Follow-up",
                  "text": "I diversi membri del team volevano gestire le raccomandazioni dell'IA diversamente? Come √® stato risolto?",
                  "evidence_type": "team_dynamics"
                }
              ]
            }
          ]
        },
        {
          "title": "Formazione e Gestione delle Crisi",
          "weight": 0.35,
          "items": [
            {
              "type": "question",
              "id": "conv_q6",
              "text": "Quale formazione specifica fornisce al personale su quando fidarsi rispetto a mettere in dubbio le raccomandazioni di sicurezza dell'IA? Mi racconti la Sua sessione di formazione pi√π recente su questo argomento.",
              "scoring_guidance": {
                "green": "Formazione di calibrazione regolare con esercizi di interpretazione della confidenza e valutazione del contesto",
                "yellow": "Formazione base sullo strumento IA senza guida alla calibrazione della fiducia",
                "red": "Nessuna formazione sui livelli di fiducia nell'IA appropriati o su quando ignorare"
              },
              "followups": [
                {
                  "type": "Follow-up",
                  "text": "Insegna al personale i tipi di errori che tendono a fare i sistemi IA rispetto agli errori umani?",
                  "evidence_type": "error_pattern_training"
                }
              ]
            },
            {
              "type": "question",
              "id": "conv_q7",
              "text": "Durante incidenti di sicurezza ad alta pressione, qual √® il Suo protocollo per l'utilizzo degli strumenti IA? Mi faccia un esempio di come le raccomandazioni dell'IA sono state gestite durante il Suo ultimo importante evento di sicurezza.",
              "scoring_guidance": {
                "green": "Protocolli di crisi strutturati con l'IA come supporto decisionale, esempio recente che mostra uso equilibrato",
                "yellow": "Risposte variabili alle crisi a seconda degli individui o dei livelli di stress",
                "red": "Comportamento di crisi estremo (completo affidamento all'IA o abbandono totale sotto pressione)"
              },
              "followups": [
                {
                  "type": "Follow-up",
                  "text": "Ha notato che la fiducia nell'IA cambia drammaticamente quando le persone sono sotto stress?",
                  "evidence_type": "stress_response_pattern"
                }
              ]
            }
          ]
        },
        {
          "title": "Ricerca di Bandiere Rosse",
          "weight": 0,
          "description": "Indicatori osservabili che aumentano il punteggio di vulnerabilit√† indipendentemente dalle politiche dichiarate",
          "items": [
            {
              "type": "checkbox",
              "id": "red_flag_1",
              "label": "\"Dopo quel falso positivo, abbiamo disabilitato il sistema IA per settimane...\"",
              "severity": "critical",
              "score_impact": 0.17,
              "subitems": []
            },
            {
              "type": "checkbox",
              "id": "red_flag_2",
              "label": "\"Alcuni si fidano completamente dell'IA, altri non la useranno affatto...\"",
              "severity": "critical",
              "score_impact": 0.16,
              "subitems": []
            },
            {
              "type": "checkbox",
              "id": "red_flag_3",
              "label": "\"Non tracciamo le decisioni di override - le persone fanno quello che sentono giusto...\"",
              "severity": "high",
              "score_impact": 0.14,
              "subitems": []
            },
            {
              "type": "checkbox",
              "id": "red_flag_4",
              "label": "\"Durante l'ultimo incidente, abbiamo ignorato tutte le raccomandazioni dell'IA e l'abbiamo fatto manualmente...\"",
              "severity": "high",
              "score_impact": 0.13,
              "subitems": []
            },
            {
              "type": "checkbox",
              "id": "red_flag_5",
              "label": "\"Se l'IA fa un errore, perdiamo tutta la fiducia in essa...\"",
              "severity": "high",
              "score_impact": 0.15,
              "subitems": []
            },
            {
              "type": "checkbox",
              "id": "red_flag_6",
              "label": "\"Nessuna formazione su quando fidarsi dell'IA - supponiamo che le persone lo capiranno...\"",
              "severity": "medium",
              "score_impact": 0.12,
              "subitems": []
            },
            {
              "type": "checkbox",
              "id": "red_flag_7",
              "label": "\"Sotto pressione, seguiamo l'IA ciecamente o la rifiutiamo completamente...\"",
              "severity": "high",
              "score_impact": 0.13,
              "subitems": []
            }
          ]
        }
      ],
      "calculation": "Conversation_Score = Weighted_Average(subsection_scores) + Œ£(red_flag_impacts)"
    }
  ],

  "validation": {
    "method": "matthews_correlation_coefficient",
    "formula": "V = (TP¬∑TN - FP¬∑FN) / sqrt((TP+FP)(TP+FN)(TN+FP)(TN+FN))",
    "continuous_validation": {
      "synthetic_testing": "Monitori i modelli di utilizzo degli strumenti IA per oscillazioni estreme di fiducia mensilmente",
      "correlation_analysis": "Confronta la valutazione manuale con le metriche dei modelli di fiducia automatizzati (correlazione target > 0,75)",
      "drift_detection": "Test di Kolmogorov-Smirnov sui modelli di fiducia, ricalibrare se p < 0,05"
    },
    "calibration": {
      "method": "isotonic_regression",
      "description": "Assicura che i punteggi dei modelli di fiducia prevedono gli incidenti di sicurezza correlati all'IA",
      "baseline_period": "90_days",
      "recalibration_trigger": "Drift rilevato o punteggio di validazione < 0,70"
    },
    "success_metrics": [
      {
        "metric": "Tasso di Appropriatezza dell'Override dell'IA",
        "formula": "% delle decisioni di override dello strumento di sicurezza IA che sono oggettivamente giustificate sulla base dell'analisi successiva",
        "baseline": "validazione degli outcome dell'override corrente",
        "target": ">85% override giustificati entro 90 giorni",
        "measurement": "analisi mensile automatizzata dei risultati dell'override"
      },
      {
        "metric": "Coerenza del Tempo di Risposta",
        "formula": "Varianza tra i tempi di risposta agli avvisi dell'IA ad alta confidenza rispetto agli avvisi generati da umani di severit√† simile",
        "baseline": "modelli attuali dei tempi di risposta",
        "target": "<20% varianza entro 90 giorni",
        "measurement": "analisi settimanale dei timestamp di risposta agli avvisi"
      },
      {
        "metric": "Tempo di Recupero da Falsi Positivi",
        "formula": "Tempo medio che gli strumenti di sicurezza IA rimangono disabilitati dopo incidenti di falsi positivi",
        "baseline": "tempo di recupero attuale dai log",
        "target": "<4 ore temporaneo, <24 ore revisione sistematica entro 90 giorni",
        "measurement": "monitoraggio continuo dei log di disponibilit√† dello strumento"
      }
    ]
  },

  "remediation": {
    "solutions": [
      {
        "id": "sol_1",
        "title": "Quadro di Decisione dell'Override dell'IA",
        "description": "Implementare alberi decisionali strutturati per gli override degli strumenti di sicurezza IA che richiedono giustificazioni specifiche",
        "implementation": "Creare un quadro decisionale con criteri di guasto tecnico, modelli di falsi positivi confermati e percorsi di autorizzazione di emergenza. Includere periodi di revisione obbligatori (24-48 ore) prima che gli override diventino permanenti. Richiedere giustificazione documentata e approvazione del supervisore per disabilitare gli strumenti IA per pi√π di 4 ore.",
        "technical_controls": "Sistema di workflow dell'albero decisionale, tracciamento dell'approvazione dell'override, escalation automatica per disabilitazione prolungata",
        "roi": "Media del 295% entro 12 mesi",
        "effort": "medium",
        "timeline": "45-60 giorni"
      },
      {
        "id": "sol_2",
        "title": "Protocolli di Risposta dell'IA Basati sulla Confidenza",
        "description": "Stabilire diverse procedure di risposta in base ai punteggi di confidenza dell'IA",
        "implementation": "Creare un quadro di risposta stratificato: confidenza elevata (>90%) richiede azione immediata con verifica successiva, confidenza media (70-90%) attiva la revisione umana prima dell'azione, confidenza bassa (<70%) contrassegna per l'indagine senza risposta automatica. Includere percorsi di escalation chiari e intervalli di risposta per ogni livello.",
        "technical_controls": "Integrazione del punteggio di confidenza, automazione del workflow stratificato, dashboard di tracciamento della risposta",
        "roi": "Media del 320% entro 12 mesi",
        "effort": "medium",
        "timeline": "45-60 giorni"
      },
      {
        "id": "sol_3",
        "title": "Programma di Formazione sulla Collaborazione Umano-IA",
        "description": "Distribuire esercizi di calibrazione mensili utilizzando incidenti storici reali dove i team praticano la valutazione delle raccomandazioni dell'IA",
        "implementation": "Creare una libreria di scenari che copra lo scetticismo appropriato, i rischi di affidamento eccessivo e i modelli ottimali di collaborazione umano-IA. Includere esercizi con raccomandazioni dell'IA deliberatamente difettose. Tracciare le prestazioni individuali e del team sulla precisione della calibrazione della fiducia nel tempo.",
        "technical_controls": "Piattaforma di scenario di formazione, tracciamento delle prestazioni, sistema di valutazione delle competenze",
        "roi": "Media del 260% entro 18 mesi",
        "effort": "low",
        "timeline": "30 giorni iniziali, continuo mensile"
      },
      {
        "id": "sol_4",
        "title": "Sistema di Traccia di Controllo della Decisione dell'IA",
        "description": "Implementare la registrazione completa di tutte le interazioni umane con i sistemi di sicurezza dell'IA",
        "implementation": "Registra tutte le decisioni di override, i tempi di risposta, le azioni di verifica e i risultati. Genera report settimanali che identificano i modelli di fiducia nell'IA inappropriata (sia affidamento eccessivo che insufficiente). Fornisci raccomandazioni di correzione specifiche per individui e team che mostrano modelli problematici.",
        "technical_controls": "Registrazione di controllo completa, motore di analisi dei modelli, reporting automatizzato, workflow di correzione",
        "roi": "Media del 310% entro 12 mesi",
        "effort": "high",
        "timeline": "60-90 giorni"
      },
      {
        "id": "sol_5",
        "title": "Protocollo di Esposizione Graduale dell'IA",
        "description": "Per i nuovi strumenti di sicurezza IA, implementare implementazioni per fasi a partire da decisioni a basso rischio",
        "implementation": "Inizia con l'IA che fornisce solo raccomandazioni (nessuna azione automatica). Aumenta gradualmente l'autorit√† dell'IA man mano che il team dimostra una calibrazione della fiducia appropriata. Includere periodi obbligatori di supervisione umana, riduzione graduale dei requisiti di verifica in base alle metriche di prestazione obiettive.",
        "technical_controls": "Sistema di implementazione per fasi, gestione del livello di autorit√†, tracciamento delle metriche di prestazione",
        "roi": "Media del 270% entro 18 mesi",
        "effort": "medium",
        "timeline": "varia a seconda dello strumento, implementazione di 60-120 giorni"
      },
      {
        "id": "sol_6",
        "title": "Dashboard di Trasparenza dell'IA",
        "description": "Distribuire visualizzazione in tempo reale delle prestazioni dei sistemi di sicurezza dell'IA",
        "implementation": "Creare un dashboard che mostra i tassi di accuratezza, i trend dei falsi positivi, i livelli di confidenza attuali e gli outcome della decisione storica. Consentire al personale di prendere decisioni di fiducia informate in base alle prestazioni attuali del sistema IA piuttosto che reazioni emotive generali o bias storici.",
        "technical_controls": "Dashboard di prestazione in tempo reale, analisi dei trend storici, visualizzazione del punteggio di confidenza",
        "roi": "Media del 285% entro 12 mesi",
        "effort": "medium",
        "timeline": "45 giorni"
      }
    ],
    "prioritization": {
      "critical_first": ["sol_1", "sol_2"],
      "high_value": ["sol_4", "sol_6"],
      "cultural_foundation": ["sol_3"],
      "governance": ["sol_5"]
    }
  },

  "risk_scenarios": [
    {
      "id": "scenario_1",
      "title": "Sfruttamento del Bypass dello Strumento di Sicurezza",
      "description": "Dopo che il rilevamento delle minacce IA genera falsi positivi, il team di sicurezza disabilita il sistema. Gli attaccanti, monitorando questo modello attraverso la ricognizione, programmano il loro attacco durante il periodo di disabilitazione.",
      "attack_vector": "Ricognizione dei modelli di utilizzo dello strumento IA, tempistica degli attacchi durante le finestre disabilitate",
      "psychological_mechanism": "L'avversione agli algoritmi dopo i falsi positivi crea lacune di sicurezza prevedibili",
      "historical_example": "L'azienda di servizi finanziari ha perso 2,3 milioni di dollari quando gli attaccanti hanno sfruttato una finestra di tre giorni in cui le analitiche comportamentali erano disabilitate a seguito di falsi avvisi di frode",
      "likelihood": "high",
      "impact": "critical",
      "detection_indicators": ["ai_tool_disabled", "attack_during_downtime", "false_positive_preceding_attack"]
    },
    {
      "id": "scenario_2",
      "title": "Ingegneria Sociale dell'Autorit√† dell'IA",
      "description": "Gli autori di minacce si fingono sistemi di sicurezza IA o affermano l'approvazione dell'IA per attivit√† dannose, sfruttando l'affidamento eccessivo del personale all'autorit√† dell'IA",
      "attack_vector": "Comunicazioni false 'verificate dall'IA', dashboard di sicurezza falsificate, punteggi di confidenza dell'IA manipolati",
      "psychological_mechanism": "L'apprezzamento dell'algoritmo in alcuni contesti consente lo sfruttamento tramite falsa autorit√† dell'IA",
      "historical_example": "L'organizzazione sanitaria √® stata compromessa quando gli attaccanti hanno inviato email affermando che l'analisi del rischio 'IA' ha approvato le richieste di accesso di emergenza",
      "likelihood": "medium",
      "impact": "high",
      "detection_indicators": ["unverified_ai_endorsement", "unusual_ai_confidence_claims", "bypass_via_ai_authority"]
    },
    {
      "id": "scenario_3",
      "title": "Manipolazione Avversariale dell'IA",
      "description": "Gli attaccanti avvelenano i dati di addestramento o sfruttano le vulnerabilit√† del modello mentre il personale si fida ciecamente delle raccomandazioni dell'IA senza comprendere i limiti algoritmici",
      "attack_vector": "Avvelenamento dei dati di addestramento, esempi avversariali, backdoor del modello",
      "psychological_mechanism": "La fase di affidamento eccessivo del paradosso impedisce il rilevamento del comportamento IA compromesso",
      "historical_example": "L'azienda manifatturiera ha subito un attacco alla catena di approvvigionamento quando gli avversari hanno manipolato il sistema di rischio fornitore IA, causando l'errata classificazione degli indicatori di minaccia legittimi come sicuri",
      "likelihood": "medium",
      "impact": "critical",
      "detection_indicators": ["ai_classification_anomalies", "model_drift_unexplained", "false_negative_patterns"]
    },
    {
      "id": "scenario_4",
      "title": "Amplificazione dell'Affaticamento degli Avvisi",
      "description": "Il personale respinge pi√π prontamente gli avvisi di sicurezza generati dall'IA rispetto ai avvisi umani, creando punti ciechi prevedibili che gli attaccanti sfruttano programmando gli attacchi quando i sistemi IA generano volumi di avvisi pi√π elevati",
      "attack_vector": "Manipolazione del volume di avvisi, tempistica degli attacchi durante periodi di avvisi IA elevati",
      "psychological_mechanism": "L'avversione agli algoritmi induce il personale a respingere gli avvisi dell'IA come 'rumore dell'algoritmo' fidandosi dei rapporti umani",
      "likelihood": "high",
      "impact": "high",
      "detection_indicators": ["ai_alert_dismissal_rate_spike", "attack_during_high_volume", "human_alerts_prioritized_over_ai"]
    }
  ],

  "metadata": {
    "created": "08/11/2025",
    "version": "1.0",
    "author": "Giuseppe Canale, CISSP",
    "cpf_version": "1.0",
    "last_updated": "2025-11-08",
    "language": "it-IT",
    "language_name": "Italiano (Italia)",
    "is_translation": true,
    "translated_from": "en-US",
    "translation_date": "09/11/2025",
    "translator": "Claude (Anthropic AI)",
    "created_date": "2025-11-08",
    "last_modified": "09/11/2025",
    "version_history": [],
    "research_basis": [
      "Dietvorst, B. J., et al. (2015). Algorithm aversion: People erroneously avoid algorithms after seeing them err",
      "Burton, J. W., et al. (2020). A systematic review of algorithm aversion in augmented decision making",
      "Logg, J. M., et al. (2019). Algorithm appreciation: People prefer algorithmic to human judgment",
      "Manzey, D. H., et al. (2012). Human performance consequences of automated decision aids: The role of operator monitoring",
      "Yamada, Y., et al. (2013). Categorization difficulty is associated with negative evaluation in the 'uncanny valley' phenomenon"
    ],
    "validation_status": "production_ready",
    "deployment_status": "production_ready"
  }
}
