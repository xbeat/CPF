{
  "indicator": "9.6",
  "title": "INDICATORE 9.6 KIT DI VALUTAZIONE",
  "subtitle": "Fiducia nell'Opacit√† del Machine Learning",
  "category": "Vulnerabilit√† di Bias Specifiche dell'IA",
  "version": "1.0",
  "cpf_reference": "CPF v1.0 - Categoria 9.x",

  "description": {
    "short": "Misura la vulnerabilit√† allo sviluppo di modelli di fiducia inappropriati verso sistemi IA i cui processi decisionali sono fondamentalmente opachi o incomprensibili",
    "context": "La Fiducia nell'Opacit√† del Machine Learning rappresenta una vulnerabilit√† critica dove gli esseri umani sviluppano modelli di fiducia inappropriati verso sistemi IA i cui processi decisionali sono fondamentalmente opachi o incomprensibili. Questa vulnerabilit√† opera attraverso fenomeni di trasferimento di fiducia, ricerca di chiusura cognitiva e sostituzione di competenza. Gli utenti sostituiscono inconsciamente le valutazioni della competenza di un sistema IA in aree che possono valutare (design dell'interfaccia, velocit√†, sofisticazione tecnica) con la competenza in aree che non possono valutare (accuratezza dei processi decisionali nascosti, qualit√† dei dati, bias algoritmico). Questo crea rischi di sicurezza quando il personale accetta raccomandazioni IA senza verifica, rendendo le organizzazioni vulnerabili ad attacchi mediati da IA, avvelenamento di modelli e manipolazione decisionale.",
    "impact": "Le organizzazioni con vulnerabilit√† di fiducia nell'opacit√† sperimentano attacchi mediati da IA dove gli aggressori manipolano sistemi IA opachi, avvelenamento di modelli attraverso dati di addestramento compromessi che sfruttano la fiducia cieca, manipolazione decisionale attraverso raccomandazioni IA non verificate, e erosione graduale della vigilanza della sicurezza poich√© il personale si affida eccessivamente a sistemi di rilevamento minacce basati su IA.",
    "psychological_basis": "La fiducia nell'opacit√† del machine learning emerge dalla teoria dell'elaborazione euristica dove gli utenti sostituiscono gli indicatori di competenza osservabili con l'effettiva capacit√† di valutare l'accuratezza. Il pregiudizio di automazione (Parasuraman & Manzey, 2010) documenta che gli umani si fidano eccessivamente dei sistemi automatizzati anche quando ricevono prove di errori. La ricerca sulla chiusura cognitiva (Kruglanski & Webster, 1996) mostra che gli individui preferiscono raccomandazioni definitive da sistemi opachi piuttosto che incertezza da processi trasparenti. Gli studi di neuroimaging rivelano che l'interazione con IA opachi attiva circuiti neuronali simili alla fiducia interpersonale, bypassando l'analisi critica normalmente applicata alle decisioni di sicurezza."
  },

  "scoring": {
    "method": "bayesian_weighted",
    "formula": "Punteggio_Finale = (w1 √ó Valutazione_Rapida + w2 √ó Profondit√†_Conversazione + w3 √ó Segnali_Rossi) √ó Moltiplicatore_Interdipendenza",
    "weights": {
      "quick_assessment": 0.4,
      "conversation_depth": 0.35,
      "red_flags": 0.25
    },
    "maturity_levels": {
      "green": {
        "score_range": [0, 0.33],
        "label": "Vulnerabilit√† Bassa - Resiliente",
        "description": "Revisioni regolari delle decisioni IA con ragionamento documentato. Il personale sovraccarica frequentemente l'IA quando il giudizio umano differisce. Procedure formali richiedono spiegazioni prima dell'implementazione. Requisiti sistematici di trasparenza dei fornitori applicati. Processi di verifica indipendente per raccomandazioni ad alto rischio.",
        "risk_level": "low",
        "color": "#22c55e"
      },
      "yellow": {
        "score_range": [0.34, 0.66],
        "label": "Vulnerabilit√† Moderata - In Sviluppo",
        "description": "Documentazione sporadica delle decisioni IA con revisione incoerente. Sovrascritture occasionali dell'IA ma non sistematiche. Guida informale sulla ricerca di spiegazioni. Alcune domande di trasparenza ai fornitori ma non formalizzate. Processi di verifica esistono ma applicati in modo incoerente.",
        "risk_level": "medium",
        "color": "#eab308"
      },
      "red": {
        "score_range": [0.67, 1.0],
        "label": "Vulnerabilit√† Alta - Critica",
        "description": "Documentazione minima o assente del ragionamento delle decisioni IA. Rari o assenti casi di personale che sovrascrive le raccomandazioni IA. Nessuna procedura definita quando il personale non pu√≤ comprendere le raccomandazioni IA. Nessun requisito sistematico di trasparenza dei fornitori. Verifica limitata o assente delle decisioni IA.",
        "risk_level": "high",
        "color": "#ef4444"
      }
    },
    "question_weights": {
      "q1_ai_decision_documentation": 0.17,
      "q2_ai_override_frequency": 0.16,
      "q3_ai_explanation_requirements": 0.15,
      "q4_vendor_transparency": 0.14,
      "q5_ai_decision_validation": 0.16,
      "q6_staff_confidence_patterns": 0.12,
      "q7_ai_failure_response": 0.10
    }
  },

  "sections": [
    {
      "id": "quick-assessment",
      "icon": "üéØ",
      "title": "VALUTAZIONE RAPIDA",
      "time": 5,
      "type": "radio-questions",
      "scoring_method": "weighted_average",
      "items": [
        {
          "type": "radio-list",
          "number": 1,
          "id": "q1_ai_decision_documentation",
          "weight": 0.17,
          "title": "Documentazione delle Decisioni IA",
          "question": "Come la Sua organizzazione documenta e rivede il ragionamento dietro le raccomandazioni dei sistemi IA, specialmente per le decisioni relative alla sicurezza?",
          "options": [
            {
              "value": "comprehensive",
              "score": 0,
              "label": "Revisioni regolari delle decisioni IA con ragionamento documentato, processi di validazione specifici per decisioni di sicurezza"
            },
            {
              "value": "sporadic",
              "score": 0.5,
              "label": "Documentazione sporadica delle decisioni IA, processi di revisione incoerenti"
            },
            {
              "value": "minimal",
              "score": 1,
              "label": "Documentazione minima o assente del ragionamento delle decisioni IA, nessuna revisione sistematica"
            }
          ],
          "evidence_required": "Politica di documentazione decisioni IA, esempi di revisioni recenti",
          "soc_mapping": "AI decision logging from security system audit trails"
        },
        {
          "type": "radio-list",
          "number": 2,
          "id": "q2_ai_override_frequency",
          "weight": 0.16,
          "title": "Frequenza di Sovrascrittura IA",
          "question": "Negli ultimi 6 mesi, con quale frequenza i membri del personale hanno sovrascritto o messo in discussione le raccomandazioni degli strumenti di sicurezza basati su IA?",
          "options": [
            {
              "value": "frequent",
              "score": 0,
              "label": "Il personale sovrascrive frequentemente l'IA quando il giudizio umano differisce (tasso di sovrascrittura 15-25%)"
            },
            {
              "value": "occasional",
              "score": 0.5,
              "label": "Sovrascritture IA occasionali ma incoerenti (tasso di sovrascrittura 5-15%)"
            },
            {
              "value": "rare",
              "score": 1,
              "label": "Rari o assenti casi di personale che sovrascrive le raccomandazioni IA (tasso di sovrascrittura <5%)"
            }
          ],
          "evidence_required": "Log di sovrascrittura IA, analisi del tasso di sovrascrittura",
          "soc_mapping": "AI override events from security decision logs"
        },
        {
          "type": "radio-list",
          "number": 3,
          "id": "q3_ai_explanation_requirements",
          "weight": 0.15,
          "title": "Requisiti di Spiegazione IA",
          "question": "Qual √® la procedura della Sua organizzazione quando il personale non pu√≤ comprendere perch√© un sistema IA ha fatto una raccomandazione di sicurezza specifica?",
          "options": [
            {
              "value": "formal",
              "score": 0,
              "label": "Procedure formali che richiedono spiegazioni prima dell'implementazione, processo di escalation per raccomandazioni poco chiare"
            },
            {
              "value": "informal",
              "score": 0.5,
              "label": "Guida informale sulla ricerca di spiegazioni, applicata in modo incoerente"
            },
            {
              "value": "none",
              "score": 1,
              "label": "Nessuna procedura definita, il personale √® tenuto ad accettare le raccomandazioni IA indipendentemente dalla comprensione"
            }
          ],
          "evidence_required": "Politica di spiegazione IA, procedure di escalation",
          "soc_mapping": "Escalation requests from AI recommendation queries"
        },
        {
          "type": "radio-list",
          "number": 4,
          "id": "q4_vendor_transparency",
          "weight": 0.14,
          "title": "Trasparenza dei Fornitori",
          "question": "Quando si procurano strumenti di sicurezza IA, quali domande specifiche la Sua organizzazione pone ai fornitori riguardo alla spiegabilit√† e alle modalit√† di guasto?",
          "options": [
            {
              "value": "systematic",
              "score": 0,
              "label": "Requisiti sistematici di trasparenza dei fornitori applicati, criteri di valutazione standardizzati incluso il testing di spiegabilit√†"
            },
            {
              "value": "some",
              "score": 0.5,
              "label": "Alcune domande di trasparenza ai fornitori ma non sistematiche, nessun punteggio formale"
            },
            {
              "value": "none",
              "score": 1,
              "label": "Nessun requisito sistematico di trasparenza dei fornitori, focus principalmente su funzionalit√† e costo"
            }
          ],
          "evidence_required": "Criteri di approvvigionamento fornitori IA, checklist di valutazione",
          "soc_mapping": "Vendor evaluation records from procurement system"
        },
        {
          "type": "radio-list",
          "number": 5,
          "id": "q5_ai_decision_validation",
          "weight": 0.16,
          "title": "Validazione delle Decisioni IA",
          "question": "Quali processi esistono per verificare indipendentemente le raccomandazioni di sicurezza generate da IA prima dell'implementazione?",
          "options": [
            {
              "value": "consistent",
              "score": 0,
              "label": "Processi di verifica indipendente utilizzati costantemente per decisioni ad alto rischio, trigger e procedure documentati"
            },
            {
              "value": "inconsistent",
              "score": 0.5,
              "label": "Processi di verifica esistono ma applicati in modo incoerente, trigger poco chiari"
            },
            {
              "value": "limited",
              "score": 1,
              "label": "Verifica limitata o assente delle decisioni IA"
            }
          ],
          "evidence_required": "Procedure di verifica IA, esempi di validazione recenti",
          "soc_mapping": "Validation events from AI decision audit logs"
        },
        {
          "type": "radio-list",
          "number": 6,
          "id": "q6_staff_confidence_patterns",
          "weight": 0.12,
          "title": "Modelli di Fiducia del Personale",
          "question": "Con quale frequenza i membri del team di sicurezza cercano seconde opinioni quando i sistemi IA forniscono raccomandazioni controintuitive?",
          "options": [
            {
              "value": "regular",
              "score": 0,
              "label": "Evidenza di scetticismo sano, consultazione regolare su raccomandazioni IA inusuali"
            },
            {
              "value": "mixed",
              "score": 0.5,
              "label": "Modelli misti di fiducia e scetticismo IA, dipendenti dalla situazione"
            },
            {
              "value": "high_trust",
              "score": 1,
              "label": "Alta fiducia nell'IA nonostante l'opacit√†, raccomandazioni controintuitive tipicamente accettate senza consultazione"
            }
          ],
          "evidence_required": "Pattern di consultazione, esempi di richieste di seconda opinione",
          "soc_mapping": "Consultation requests from collaboration logs"
        },
        {
          "type": "radio-list",
          "number": 7,
          "id": "q7_ai_failure_response",
          "weight": 0.10,
          "title": "Risposta ai Guasti IA",
          "question": "Cosa √® successo l'ultima volta che uno strumento di sicurezza IA ha fornito analisi o raccomandazioni errate? Come √® stato identificato?",
          "options": [
            {
              "value": "documented",
              "score": 0,
              "label": "Guasto recente identificato attraverso processi di verifica, risposta documentata e aggiustamenti del sistema"
            },
            {
              "value": "occasional",
              "score": 0.5,
              "label": "Guasti occasionali identificati, risposta informale senza miglioramento sistematico del processo"
            },
            {
              "value": "none",
              "score": 1,
              "label": "Nessun guasto recente identificato (suggerisce mancanza di verifica), o guasti non documentati/analizzati"
            }
          ],
          "evidence_required": "Report di guasti IA, documentazione delle lezioni apprese",
          "soc_mapping": "AI failure incidents from security incident database"
        }
      ],
      "subsections": []
    },
    {
      "id": "client-conversation",
      "icon": "üí¨",
      "title": "GUIDA CONVERSAZIONE CLIENTE",
      "time": 30,
      "items": [],
      "subsections": []
    }
  ],

  "data_sources": {
    "manual_assessment": {
      "primary": [
        "ai_decision_documentation_policy",
        "ai_override_frequency_logs",
        "ai_explanation_procedures",
        "vendor_transparency_requirements"
      ],
      "evidence_required": [
        "ai_decision_review_policy",
        "recent_ai_override_examples",
        "ai_explanation_escalation_procedures",
        "vendor_evaluation_criteria"
      ]
    }
  },

  "validation": {
    "required_evidence": [
      "ai_decision_documentation_policy",
      "ai_override_rate_analysis",
      "vendor_transparency_checklist",
      "ai_failure_incident_reports"
    ]
  },

  "remediation": {
    "priority": "high",
    "solutions": [
      {
        "id": "rem_9.6_01",
        "title": "Implementare Documentazione delle Decisioni IA",
        "description": "Stabilire requisiti formali per documentare il ragionamento delle decisioni IA, specialmente per raccomandazioni di sicurezza critiche"
      },
      {
        "id": "rem_9.6_02",
        "title": "Sviluppare Processi di Verifica Indipendente",
        "description": "Creare procedure per la verifica umana delle raccomandazioni IA ad alto rischio prima dell'implementazione"
      },
      {
        "id": "rem_9.6_03",
        "title": "Stabilire Requisiti di Trasparenza dei Fornitori",
        "description": "Implementare criteri formali di approvvigionamento che richiedono spiegabilit√† IA e documentazione delle modalit√† di guasto"
      }
    ]
  }
}
