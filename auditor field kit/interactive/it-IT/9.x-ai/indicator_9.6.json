{
  "indicator_id": "9.6",
  "indicator_name": "Fiducia nella Trasparenza dell'Apprendimento Automatico",
  "category": "9.x-ai",
  "category_name": "Vulnerabilità di Pregiudizio Specifiche dell'IA",
  "description": "La Fiducia nella Trasparenza dell'Apprendimento Automatico rappresenta una vulnerabilità critica in cui gli esseri umani sviluppano schemi di fiducia inappropriati verso i sistemi di IA i cui processi decisionali sono fondamentalmente opachi o incomprensibili. Questa vulnerabilità opera attraverso fenomeni di trasferimento della fiducia, ricerca di chiusura cognitiva e sostituzione di competenza. Gli utenti sostituiscono inconsciamente valutazioni della competenza di un sistema di IA in aree che possono valutare (design dell'interfaccia, velocità, sofisticazione tecnica) con competenza in aree che non possono valutare (accuratezza dei processi decisionali nascosti, qualità dei dati, pregiudizio algoritmico). Questo crea rischi di sicurezza quando il personale accetta raccomandazioni di IA senza verifica, rendendo le organizzazioni vulnerabili ad attacchi mediati da IA, avvelenamento del modello e manipolazione delle decisioni.",

  "metadata": {
    "created": "09/11/2025",
    "version": "1.0",
    "author": "Giuseppe Canale, CISSP",
    "cpf_version": "1.0",
    "last_updated": "2025-11-08",
    "language": "it-IT",
    "language_name": "Italiano (Italia)",
    "is_translation": true,
    "translated_from": "en-US",
    "translation_date": "09/11/2025",
    "translator": "Claude (Anthropic AI)",
    "created_date": "2025-11-08",
    "last_modified": "09/11/2025",
    "version_history": []
  },

  "quick_assessment": {
    "description": "Sette domande di valutazione rapida progettate per misurare la vulnerabilità alla fiducia inappropriata in sistemi di IA opachi. Ogni domanda mira a indicatori comportamentali specifici e pratiche organizzative.",

    "questions": {
      "q1_ai_decision_documentation": {
        "question": "Come documenta e revisiona la Vostra organizzazione il ragionamento dietro le raccomandazioni del sistema di IA, specialmente per le decisioni relative alla sicurezza?",
        "weight": 0.17,
        "scoring": {
          "green": "Revisioni regolari delle decisioni di IA con motivazione documentata, processi di validazione specifici per le decisioni di sicurezza",
          "yellow": "Documentazione sporadica delle decisioni di IA, processi di revisione incoerenti",
          "red": "Documentazione minima o assente della motivazione della decisione di IA, nessuna revisione sistematica"
        }
      },
      "q2_ai_override_frequency": {
        "question": "Negli ultimi 6 mesi, con quale frequenza i membri del personale hanno ignorato o messo in discussione le raccomandazioni degli strumenti di sicurezza basati su IA?",
        "weight": 0.16,
        "scoring": {
          "green": "Il personale ignora frequentemente l'IA quando il giudizio umano differisce (tasso di override del 15-25%)",
          "yellow": "Override di IA occasionali ma incoerenti (tasso di override del 5-15%)",
          "red": "Istanze rare o assenti di personale che ignora le raccomandazioni di IA (<5% di tasso di override)"
        }
      },
      "q3_ai_explanation_requirements": {
        "question": "Qual è la procedura della Vostra organizzazione quando il personale non comprende perché un sistema di IA ha fatto una specifica raccomandazione di sicurezza?",
        "weight": 0.15,
        "scoring": {
          "green": "Procedure formali che richiedono spiegazioni prima dell'implementazione, processo di escalation per raccomandazioni poco chiare",
          "yellow": "Guida informale sulla ricerca di spiegazioni, applicazione incoerente",
          "red": "Nessuna procedura definita, il personale dovrebbe accettare le raccomandazioni di IA indipendentemente dalla comprensione"
        }
      },
      "q4_vendor_transparency": {
        "question": "Quando acquisite strumenti di sicurezza basati su IA, quali domande specifiche fa la Vostra organizzazione ai fornitori riguardo alla spiegabilità e ai modi di guasto?",
        "weight": 0.14,
        "scoring": {
          "green": "Requisiti di trasparenza sistematici del fornitore applicati, criteri di valutazione standardizzati inclusa la verifica della spiegabilità",
          "yellow": "Alcune domande sulla trasparenza del fornitore ma non sistematiche, nessun punteggio formale",
          "red": "Nessun requisito sistematico di trasparenza del fornitore, focus principalmente su caratteristiche e costi"
        }
      },
      "q5_ai_decision_validation": {
        "question": "Quali processi esistono per verificare indipendentemente le raccomandazioni di sicurezza generate dall'IA prima dell'implementazione?",
        "weight": 0.16,
        "scoring": {
          "green": "Processi di verifica indipendente utilizzati costantemente per decisioni ad alto impatto, trigger e procedure documentati",
          "yellow": "Processi di verifica esistenti ma applicati in modo incoerente, trigger poco chiari",
          "red": "Verifica indipendente limitata o assente delle decisioni di IA"
        }
      },
      "q6_staff_confidence_patterns": {
        "question": "Con quale frequenza i membri del team di sicurezza cercano secondi pareri quando i sistemi di IA forniscono raccomandazioni controintuitive?",
        "weight": 0.12,
        "scoring": {
          "green": "Evidenza di scetticismo salutare, consultazione regolare su raccomandazioni di IA inusuali",
          "yellow": "Schemi misti di fiducia e scetticismo nei confronti dell'IA, dipendenti dalla situazione",
          "red": "Elevata fiducia nell'IA nonostante l'opacità, le raccomandazioni controintuitive sono tipicamente accettate senza consultazione"
        }
      },
      "q7_ai_failure_response": {
        "question": "Cosa è successo l'ultima volta che uno strumento di sicurezza dell'IA ha fornito un'analisi o raccomandazioni non corrette? Come è stato identificato?",
        "weight": 0.10,
        "scoring": {
          "green": "Errore recente identificato attraverso processi di verifica, risposta documentata e adattamenti del sistema",
          "yellow": "Errori occasionali identificati, risposta informale senza miglioramento sistematico del processo",
          "red": "Nessun errore recente identificato (suggerendo mancanza di verifica), o errori non documentati/analizzati"
        }
      }
    },

    "question_weights": {
      "q1_ai_decision_documentation": 0.17,
      "q2_ai_override_frequency": 0.16,
      "q3_ai_explanation_requirements": 0.15,
      "q4_vendor_transparency": 0.14,
      "q5_ai_decision_validation": 0.16,
      "q6_staff_confidence_patterns": 0.12,
      "q7_ai_failure_response": 0.10
    }
  },

  "conversation_depth": {
    "description": "Sette domande di conversazione approfondita che esplorano schemi organizzativi, incidenti storici e fattori culturali che influenzano la fiducia nei sistemi di IA opachi. Queste domande aiutano i revisori a capire i meccanismi e i contesti che amplificano o mitigano questa vulnerabilità.",

    "questions": {
      "q1_trust_development_patterns": {
        "question": "Descrivete come la fiducia della Vostra organizzazione negli strumenti di sicurezza dell'IA si è evoluta nel tempo. Camminate con me attraverso un esempio specifico dove l'affidamento a un sistema di IA è aumentato - quali fattori hanno guidato quella crescente fiducia e quali meccanismi di verifica (se presenti) sono stati ridotti poiché la fiducia è cresciuta?",
        "purpose": "Rivela modelli di calibrazione della fiducia nel tempo e se l'aumentata familiarità con i sistemi di IA porta a una riduzione della verifica",
        "scoring_guidance": {
          "green_indicators": [
            "I livelli di fiducia rimangono calibrati alle prestazioni dimostrate con verifica in corso",
            "Esempi specifici di mantenimento dello scetticismo nonostante la familiarità",
            "Evidenza che la fiducia è differenziata sulla base della trasparenza del sistema di IA e della storia",
            "Processi di verifica mantenuti o rafforzati anche poiché gli strumenti di IA diventano di routine"
          ],
          "yellow_indicators": [
            "Un'erosione della verifica poiché la fiducia si sviluppa",
            "Schemi misti dove la verifica è mantenuta per alcuni sistemi di IA ma non per altri",
            "Riconoscimento dello sviluppo della fiducia ma meccanismi di calibrazione poco chiari",
            "Approccio informale piuttosto che sistematico per mantenere uno scetticismo appropriato"
          ],
          "red_indicators": [
            "Chiaro modello di aumento della fiducia con diminuzione della verifica nel tempo",
            "Sviluppo della fiducia guidato dalla familiarità piuttosto che dalla validazione delle prestazioni",
            "Incapacità di articolare fattori specifici che dovrebbero calibrare i livelli di fiducia",
            "Processi di verifica sistematicamente ridotti poiché i sistemi di IA diventano normalizzati"
          ]
        }
      },
      "q2_sophistication_bias_manifestation": {
        "question": "Nel valutare gli strumenti di sicurezza dell'IA, come influenzano la Vostra valutazione dell'affidabilità dell'IA fattori come interfacce sofisticate, output tecnici complessi o presentazione professionale? Potete descrivere un'acquisizione o valutazione specifica dove queste caratteristiche di superficie hanno influenzato le decisioni?",
        "purpose": "Esamina se le organizzazioni sostituiscono indicatori di sofisticazione facilmente osservabili con una vera valutazione della competenza dell'IA",
        "scoring_guidance": {
          "green_indicators": [
            "Consapevolezza esplicita della distinzione tra presentazione e vera affidabilità",
            "Processi sistematici per valutare le prestazioni dell'IA indipendenti dalla sofisticazione dell'interfaccia",
            "Esempi di scelta di strumenti meno sofisticati ma più trasparenti/affidabili",
            "Criteri di acquisizione focalizzati sulla spiegabilità e validazione delle prestazioni piuttosto che sull'aspetto"
          ],
          "yellow_indicators": [
            "Un'certa consapevolezza del pregiudizio di sofisticazione ma contromisure incomplete",
            "Influenza mista delle caratteristiche di superficie sulla valutazione",
            "Riconoscimento che la presentazione influisce sulla percezione ma poco chiaro come questo viene mitigato",
            "Processi di acquisizione che in parte rendono conto di ma non affrontano sistematicamente il pregiudizio di sofisticazione"
          ],
          "red_indicators": [
            "Presentazione sofisticata trattata come indicatore di affidabilità",
            "Output complessi percepiti come evidenza di competenza dell'IA senza validazione",
            "Decisioni di acquisizione fortemente influenzate da dimostrazioni impressionanti",
            "Incapacità di articolare come le prestazioni effettive dell'IA vengono valutate indipendentemente dalla sofisticazione"
          ]
        }
      },
      "q3_institutional_authority_transfer": {
        "question": "La Vostra organizzazione utilizza strumenti di sicurezza dell'IA da fornitori con reputazioni forti. Come fate distinzione tra la fiducia nell'organizzazione del fornitore rispetto alla fiducia nel processo decisionale specifico del sistema di IA? Fornite un esempio specifico dove questa distinzione ha contato o avrebbe dovuto contare.",
        "purpose": "Identifica se la fiducia organizzativa si trasferisce in modo inappropriato a sistemi tecnici opachi",
        "scoring_guidance": {
          "green_indicators": [
            "Chiara separazione tra reputazione del fornitore e validazione del sistema di IA",
            "Esempi di mettere in discussione sistemi di IA specifici nonostante la fiducia complessiva nel fornitore",
            "Valutazione sistematica degli strumenti di IA indipendente dall'autorità del fornitore",
            "Riconoscimento che i fornitori rinomati possono implementare sistemi con limitazioni sconosciute"
          ],
          "yellow_indicators": [
            "Una certa distinzione tracciata ma non applicata costantemente",
            "La reputazione del fornitore fornisce una fiducia di base ma la validazione occasionale si verifica",
            "Consapevolezza della distinzione ma manifestazione operativa poco chiara",
            "Esempi misti dove l'autorità del fornitore ha influenzato la fiducia del sistema di IA a vari gradi"
          ],
          "red_indicators": [
            "Reputazione del fornitore trattata come garanzia dell'affidabilità del sistema di IA",
            "Fiducia nell'istituzione direttamente trasferita a sistemi tecnici opachi",
            "Assenza di validazione dell'IA indipendente quando il fornitore è rinomato",
            "Difficoltà nell'articolare scenari in cui la fiducia del fornitore e del sistema dovrebbe differire"
          ]
        }
      },
      "q4_ai_incident_learning": {
        "question": "Raccontatemi di un momento in cui uno strumento di sicurezza dell'IA ha fornito raccomandazioni non corrette o problematiche. Come è stato scoperto l'errore, qual è stata la risposta organizzativa e cosa vi ha insegnato riguardo ai livelli di fiducia appropriati per i sistemi di IA?",
        "purpose": "Valuta l'apprendimento organizzativo dagli errori dell'IA e l'impatto sulla calibrazione della fiducia",
        "scoring_guidance": {
          "green_indicators": [
            "Incidenti recenti identificati attraverso processi di verifica attiva",
            "Analisi sistematica dei modalità e limitazioni di guasto dell'IA",
            "Cambiamenti documentati alle procedure o calibrazione della fiducia dopo gli incidenti",
            "Incidenti utilizzati come opportunità di apprendimento per migliorare la collaborazione umano-IA"
          ],
          "yellow_indicators": [
            "Alcuni incidenti riconosciuti ma processo di apprendimento informale",
            "Risposta organizzativa incoerente agli errori dell'IA",
            "Cambiamenti sistematici limitati risultanti dagli incidenti",
            "Riconoscimento dei problemi ma impatto poco chiaro sui modelli di fiducia organizzativi"
          ],
          "red_indicators": [
            "Nessun errore recente dell'IA identificato (suggerendo mancanza di verifica)",
            "Incidenti dismissi come anomalie isolate senza analisi sistematica",
            "Apprendimento organizzativo minimo o cambiamenti di processo dopo gli errori dell'IA",
            "Modelli di guasto non tracciati o utilizzati per calibrare i livelli di fiducia"
          ]
        }
      },
      "q5_verification_burden_patterns": {
        "question": "Verificare le raccomandazioni dell'IA richiede uno sforzo cognitivo e un tempo. Come bilancia la Vostra organizzazione i guadagni di efficienza dell'automazione dell'IA rispetto al sovraccarico del mantenimento dei processi di verifica? Descrivete una situazione specifica dove questa tensione è stata risolta.",
        "purpose": "Rivela se l'evitamento del carico cognitivo guida lo sviluppo inappropriato della fiducia",
        "scoring_guidance": {
          "green_indicators": [
            "Approccio sistematico alla verifica proporzionata al rischio (più verifica per scommesse più alte)",
            "Guadagni di efficienza misurati rispetto ai costi di verifica con compromessi consci",
            "Esempi di mantenimento della verifica nonostante la pressione di efficienza",
            "Politiche chiare su quando la verifica può essere semplificata rispetto a quando è obbligatoria"
          ],
          "yellow_indicators": [
            "Bilanciamento informale tra efficienza e verifica",
            "Una certa verifica mantenuta ma applicazione incoerente",
            "Riconoscimento della tensione ma nessun quadro sistematico di risoluzione",
            "Esempi misti in cui l'efficienza a volte sostituisce appropriatamente la verifica e a volte inappropriatamente"
          ],
          "red_indicators": [
            "L'efficienza è costantemente prioritaria rispetto alla verifica",
            "La verifica è vista principalmente come un sovraccarico da minimizzare",
            "L'evitamento del carico cognitivo guida la riduzione dello scrutinio dell'IA nel tempo",
            "Difficoltà nell'articolare quando la verifica dovrebbe essere mantenuta nonostante i costi di efficienza"
          ]
        }
      },
      "q6_organizational_transparency_pressure": {
        "question": "La Vostra organizzazione fa pressione attiva ai fornitori di IA per fornire sistemi più trasparenti e spiegabili, oppure vi adattate a qualunque livello di opacità forniscono i fornitori? Dategli a me un esempio specifico di come i requisiti di trasparenza hanno influenzato una decisione di acquisizione o implementazione dello strumento di IA.",
        "purpose": "Esamina se le organizzazioni accettano l'opacità come inevitabile o richiedono trasparenza",
        "scoring_guidance": {
          "green_indicators": [
            "Pressione organizzativa attiva per soluzioni di IA spiegabili",
            "Requisiti di trasparenza che influenzano le decisioni di acquisizione (rifiutando strumenti opachi)",
            "Contratti del fornitore incluso spiegabilità e disposizioni di test",
            "Esempi di scelta di strumenti più trasparenti rispetto ad alternative più sofisticate ma opache"
          ],
          "yellow_indicators": [
            "Alcune preferenze di trasparenza espresse ma non fattori decisivi",
            "Requisiti di trasparenza informali piuttosto che contrattuali",
            "Esempi misti dove la trasparenza ha influenzato alcune ma non tutte le decisioni",
            "Riconoscimento del valore di trasparenza ma disponibilità a compromessi"
          ],
          "red_indicators": [
            "Accettazione passiva dei livelli di opacità forniti dal fornitore",
            "La trasparenza non è un fattore significativo nelle decisioni di acquisizione",
            "Focus sulla funzionalità e sui costi piuttosto che sulla spiegabilità",
            "Nessun esempio di requisiti di trasparenza che influenzano la selezione dello strumento"
          ]
        }
      },
      "q7_role_differential_trust": {
        "question": "Come mostrano ruoli diversi nella Vostra organizzazione (dirigenti, implementatori tecnici, utenti finali, responsabili della conformità) modelli di fiducia diversi verso i sistemi di IA? Descrivete esempi specifici che mostrano come le pressioni basate sui ruoli influenzano la calibrazione della fiducia nell'IA.",
        "purpose": "Identifica vulnerabilità specifiche dei ruoli e eterogenità della fiducia organizzativa",
        "scoring_guidance": {
          "green_indicators": [
            "Consapevolezza dei differenziali di fiducia basati sui ruoli con strategie di mitigazione",
            "Esempi che mostrano diversi ruoli che mantengono lo scetticismo appropriato nonostante le pressioni dei ruoli",
            "Processi di comunicazione cross-role che calibrano la fiducia nei livelli organizzativi",
            "Riconoscimento che i vincoli di tempo dei dirigenti o le pressioni dei responsabili della conformità richiedono salvaguardie strutturali"
          ],
          "yellow_indicators": [
            "Un'certa riconoscimento dei modelli basati sui ruoli ma mitigazione limitata",
            "Calibrazione incoerente della fiducia tra i ruoli organizzativi",
            "Pressioni dei ruoli riconosciute ma non sistematicamente affrontate",
            "Esempi misti in cui lo scetticismo appropriato ai ruoli a volte è mantenuto, a volte no"
          ],
          "red_indicators": [
            "Mancanza di consapevolezza delle vulnerabilità dei differenziali di fiducia basati sui ruoli",
            "Chiari esempi di dirigenti che si fidano eccessivamente di riassunti di IA di alto livello",
            "Responsabili della conformità sotto pressione di fidarsi dei sistemi di segnalazione dell'IA",
            "Implementatori tecnici che confondono la competenza di implementazione con la valutazione dell'affidabilità dell'IA"
          ]
        }
      }
    }
  },

  "red_flags": {
    "description": "Segni critici di avvertimento che un'organizzazione ha sviluppato una fiducia inappropriata in sistemi di IA opachi, creando una significativa vulnerabilità di sicurezza informatica. Questi modelli indicano una necessità urgente di intervento.",

    "flags": {
      "red_flag_1": {
        "flag": "Tasso di Override dell'IA Quasi-Zero",
        "description": "Il personale di sicurezza ignora le raccomandazioni degli strumenti di sicurezza dell'IA meno del 5% delle volte, suggerendo una fiducia eccessiva piuttosto che uno scetticismo appropriato. Questo modello indica che il personale ha smesso di esercitare il giudizio indipendente.",
        "score_impact": 0.16
      },
      "red_flag_2": {
        "flag": "Implementazione Senza Spiegazione",
        "description": "L'organizzazione implementa regolarmente raccomandazioni di sicurezza dell'IA senza richiedere o documentare spiegazioni della motivazione dell'IA, in particolare per decisioni ad alto impatto come cambiamenti di accesso o modifiche alla politica di sicurezza.",
        "score_impact": 0.15
      },
      "red_flag_3": {
        "flag": "Modello di Erosione della Verifica",
        "description": "Chiaro modello storico in cui la verifica indipendente delle raccomandazioni dell'IA è diminuita nel tempo poiché la familiarità con gli strumenti di IA è aumentata. Lo scetticismo iniziale è stato sostituito dall'accettazione di routine.",
        "score_impact": 0.14
      },
      "red_flag_4": {
        "flag": "Dipendenza dall'Autorità del Fornitore",
        "description": "La fiducia dell'organizzazione nei sistemi di IA si basa principalmente sulla reputazione del fornitore piuttosto che sulla validazione indipendente. Le domande sul processo decisionale dell'IA sono redirect a 'fidarsi del fornitore' piuttosto che essere esaminate sistematicamente.",
        "score_impact": 0.13
      },
      "red_flag_5": {
        "flag": "Nessun Guasto dell'IA Documentato",
        "description": "L'organizzazione non può identificare esempi recenti di strumenti di sicurezza dell'IA che forniscono raccomandazioni non corrette, suggerendo una mancanza di verifica per rilevare errori o una convinzione preoccupante nell'infallibilità dell'IA.",
        "score_impact": 0.15
      },
      "red_flag_6": {
        "flag": "Sofisticazione-come-Euristica di Affidabilità",
        "description": "Durante le valutazioni degli strumenti di IA, la presentazione sofisticata, gli output complessi o il linguaggio tecnico sono trattati come indicatori di affidabilità. Le decisioni di acquisizione sono fortemente influenzate da dimostrazioni impressionanti piuttosto che dalla validazione delle prestazioni.",
        "score_impact": 0.13
      },
      "red_flag_7": {
        "flag": "Accettazione della Trasparenza",
        "description": "L'organizzazione non mostra pressione attiva per la trasparenza o la spiegabilità del fornitore di IA. L'opacità è accettata come inevitabile compromesso per le capacità dell'IA, senza tentativi sistematici di richiedere o incentivare sistemi più spiegabili.",
        "score_impact": 0.14
      }
    },

    "red_flag_score_impacts": {
      "red_flag_1": 0.16,
      "red_flag_2": 0.15,
      "red_flag_3": 0.14,
      "red_flag_4": 0.13,
      "red_flag_5": 0.15,
      "red_flag_6": 0.13,
      "red_flag_7": 0.14
    }
  },

  "remediation_solutions": {
    "description": "Interventi basati su evidenze progettati per calibrare appropriatamente la fiducia organizzativa nei sistemi di IA, stabilendo processi di verifica mantenendo i benefici dell'IA.",

    "solutions": {
      "solution_1": {
        "name": "Sistema di Audit Trail delle Decisioni dell'IA",
        "description": "Implementare requisiti di registrazione e documentazione per tutte le decisioni di sicurezza guidate da IA. Ogni raccomandazione di IA deve includere livelli di confidenza, fattori di input principali e richiedere il riconoscimento della persona che revede prima dell'implementazione.",
        "implementation": "Implementare sistema di registrazione centralizzato che cattura: dettagli della raccomandazione di IA, punteggi di confidenza, fattori di input citati, identità della persona che revede, timestamp della decisione e stato di implementazione. Creare dashboard che mostrano modelli di raccomandazione dell'IA e tassi di accettazione umana.",
        "success_metrics": "Copertura audit trail al 100% per le decisioni di sicurezza dell'IA entro 60 giorni. Misurabile attraverso analisi della completezza dei log. Tempo medio dalla raccomandazione alla revisione umana <30 minuti per le decisioni ad alta priorità.",
        "verification_checklist": [
          "Revisionare i log di esempio che mostrino dettagli completi della raccomandazione dell'IA e risposte umane",
          "Verificare che la documentazione includa punteggi di confidenza e fattori decisionali chiave",
          "Confermare che gli acknowledgment della persona che revede siano catturati con timestamp",
          "Controllare i divari nella copertura dell'audit trail e nelle procedure di escalation"
        ]
      },
      "solution_2": {
        "name": "Formazione Obbligatoria sull'Override dell'IA",
        "description": "Stabilire programmi di formazione che richiedono al personale di sicurezza di praticare l'ignoramento delle raccomandazioni dell'IA in scenari simulati. Includere esercizi regolari in cui il personale deve identificare situazioni che richiedono il giudizio umano nonostante la confidenza dell'IA.",
        "implementation": "Sviluppare biblioteca di scenari che include casi in cui: le raccomandazioni dell'IA sono confidentemente sbagliate, il contesto umano contraddice l'IA, le scommesse richiedono verifica extra. Condurre esercizi trimestrali con decisioni di override documentate. Tracciare i tassi di override nelle operazioni reali.",
        "success_metrics": "Obiettivo di tasso di override dell'IA del 15-25% nelle operazioni effettive entro 90 giorni (indicando scetticismo salutare). Misurare attraverso i log delle decisioni. Tutto il personale di sicurezza completa la formazione sull'override con aggiornamenti ogni 6 mesi.",
        "verification_checklist": [
          "Richiedere il curriculum di formazione e esempi di scenari utilizzati",
          "Intervistare il personale su decisioni recenti di override nelle operazioni effettive",
          "Verificare i record di completamento della formazione e i programmi di aggiornamento",
          "Osservare o esaminare registrazioni di esercizi di override simulati"
        ]
      },
      "solution_3": {
        "name": "Scorecard di Trasparenza del Fornitore",
        "description": "Creare criteri di valutazione standardizzati che richiedono ai fornitori di IA di dimostrare la spiegabilità, fornire documentazione sulle modalità di guasto e offrire ambienti di test per input avversari. Rendere il punteggio della trasparenza un requisito di acquisizione obbligatorio.",
        "implementation": "Sviluppare scorecard che includa: valutazioni della qualità della spiegazione, completezza della documentazione delle modalità di guasto, accesso ai test avversari, trasparenza degli aggiornamenti del modello, documentazione della logica decisionale. Impostare punteggi di soglia minima di trasparenza per l'approvazione dell'acquisizione.",
        "success_metrics": "100% delle acquisizioni di strumenti di IA valutate utilizzando scorecard di trasparenza entro 45 giorni. Tracciare i punteggi dei fornitori nel tempo. Realizzare il 30% di miglioramento nei punteggi di trasparenza media degli strumenti di IA distribuiti entro 12 mesi attraverso l'acquisizione selettiva.",
        "verification_checklist": [
          "Esaminare i contratti dei fornitori per clausole di requisito di trasparenza",
          "Esaminare scorecard di trasparenza completate per acquisizioni recenti",
          "Verificare l'accesso all'ambiente di test e la documentazione di utilizzo",
          "Controllare le decisioni di acquisizione in cui i punteggi di trasparenza hanno influenzato la selezione"
        ]
      },
      "solution_4": {
        "name": "Processo di Verifica Doppia per Decisioni ad Alto Impatto",
        "description": "Implementare una politica che richieda la verifica indipendente delle raccomandazioni di sicurezza dell'IA ad alto impatto attraverso metodi alternativi o competenza umana. Stabilire chiari trigger per quando è richiesta la validazione secondaria.",
        "implementation": "Definire criteri di decisione 'ad alto impatto' (influenza >X utenti, modifica sistemi critici, coinvolge dati sensibili). Creare catalogo dei metodi di verifica: analisi dello strumento secondario, revisione umana da parte di esperti, confronto dei modelli storici. Documentare il completamento della verifica prima dell'implementazione.",
        "success_metrics": "Conformità di verifica doppia al 100% per le decisioni di IA ad alto impatto entro 90 giorni. Misurare attraverso i log di audit. Tempo medio di completamento della verifica <2 ore. Zero decisioni di IA ad alto impatto implementate senza validazione secondaria documentata.",
        "verification_checklist": [
          "Documentare la politica di verifica doppia e i trigger di decisione",
          "Esaminare esempi di validazione secondaria in casi recenti ad alto impatto",
          "Confermare che i metodi di verifica alternativi sono chiaramente definiti",
          "Convalidare le procedure di escalation per i disaccordi della verifica"
        ]
      },
      "solution_5": {
        "name": "Dashboard di Affidabilità dell'IA",
        "description": "Implementare sistemi di monitoraggio che traccia l'accuratezza della raccomandazione dell'IA nel tempo, identificando modelli di eccessiva confidenza o errori sistematici. Includere avvisi quando il comportamento dell'IA si discosta dai modelli baseline.",
        "implementation": "Creare dashboard che mostra: tassi di accettazione della raccomandazione dell'IA, frequenze di override, risultati di validazione dell'accuratezza, metriche di calibrazione della confidenza, avvisi di deviazione. Tracciare per sistema e aggregato organizzativo. Revisionare mensilmente nelle riunioni del team di sicurezza.",
        "success_metrics": "Dashboard di affidabilità dell'IA operativo entro 45 giorni che copre tutti gli strumenti di sicurezza dell'IA. Conformità della revisione mensile >95%. Identificare e investigare il 100% delle deviazioni significative di accuratezza (>15% dal baseline) entro 48 ore.",
        "verification_checklist": [
          "Esaminare il dashboard di affidabilità dell'IA e le metriche disponibili",
          "Revisionare i log di avviso e le procedure di risposta organizzativa",
          "Verificare la metodologia di stabilimento del baseline e le soglie di deviazione",
          "Controllare i processi di analisi dei trend e gli esempi di identificazione dei modelli"
        ]
      },
      "solution_6": {
        "name": "Programma di Test dell'IA della Red Team",
        "description": "Condurre test avversari regolari degli strumenti di sicurezza dell'IA utilizzando input artigianali progettati per sfruttare le debolezze del modello. Documentare le modalità di guasto e assicurarsi che il personale comprenda gli scenari specifici in cui i sistemi di IA sono vulnerabili.",
        "implementation": "Sviluppare suite di test avversari per ogni strumento di sicurezza dell'IA: casi limite, esempi avversari, scenari di avvelenamento dei dati, test di manipolazione dell'input. Condurre test trimestralmente. Documentare le vulnerabilità scoperte e distribuirle al personale con esempi specifici.",
        "success_metrics": "Test della red team trimestrale del 100% degli strumenti di sicurezza dell'IA entro 6 mesi. Documentare minimo 5 modalità di guasto per strumento. Realizzare >80% di consapevolezza del personale delle vulnerabilità specifiche dello strumento dell'IA attraverso indagini post-test e controlli di conoscenza.",
        "verification_checklist": [
          "Revisionare la pianificazione dei test della red team e i record di completamento",
          "Esaminare le modalità di guasto documentate dello strumento dell'IA e le vulnerabilità",
          "Verificare il contenuto e la metodologia della suite di test avversari",
          "Intervistare il personale sulla consapevolezza delle limitazioni specifiche del sistema dell'IA"
        ]
      }
    }
  },

  "risk_scenarios": {
    "description": "Scenari concreti di attacco che dimostrano come le vulnerabilità di Fiducia nella Trasparenza dell'Apprendimento Automatico si traducono in incidenti di sicurezza informatica.",

    "scenarios": {
      "scenario_1": {
        "name": "Ingegneria Sociale Mediata dall'IA",
        "description": "Gli attaccanti compromettono o manipolano gli strumenti di sicurezza dell'IA per raccomandare azioni malintenzionate (aprire allegati sospetti, consentire accessi inusuali, disabilitare controlli di sicurezza). Il personale segue le raccomandazioni a causa della fiducia nel sistema di IA 'sofisticato' senza verifica indipendente.",
        "attack_vector": "Compromissione dei dati di addestramento dell'IA, parametri del modello o preprocessing dell'input per iniettare raccomandazioni malintenzionate",
        "exploitation_mechanism": "La fiducia organizzativa nei sistemi di IA significa che il personale implementa raccomandazioni senza verifica, trattando gli output dell'IA come autorevoli",
        "impact": "Campagne di phishing riuscite, accesso non autorizzato, controlli di sicurezza disabilitati, esfiltrazione dei dati - tutto legittimato attraverso sistemi di IA affidabili",
        "detection_difficulty": "Alta - richiede la comprensione del processo decisionale dell'IA per identificare la manipolazione sottile",
        "prevention_controls": "Audit trail delle decisioni dell'IA, verifica obbligatoria per raccomandazioni ad alto impatto, test della red team per identificare vulnerabilità di manipolazione"
      },
      "scenario_2": {
        "name": "Sfruttamento dell'Avvelenamento del Modello",
        "description": "Gli avversari corrompono sottilmente i dati di addestramento dell'IA nel tempo, causando ai tool di IA di sicurezza di classificare erroneamente le minacce o fornire valutazioni di rischio pregiudicate. L'opacità previene il rilevamento mentre la fiducia organizzativa assicura il continuo affidamento agli output compromessi.",
        "attack_vector": "Inserimento strategico a lungo termine di dati di addestramento corrotti sottilmente attraverso fonti di dati compromesse o accesso interno",
        "exploitation_mechanism": "L'opacità dei sistemi di ML previene il rilevamento di pregiudizi sistematici o modelli di classificazione errata; la fiducia organizzativa significa che gli output non sono verificati indipendentemente",
        "impact": "Punti ciechi sistematici nel rilevamento delle minacce, valutazioni di rischio pregiudicate che portano a sottoprotezione di risorse critiche, degradazione graduale della sicurezza",
        "detection_difficulty": "Molto Alta - richiede analisi statistica degli output dell'IA nel tempo e comprensione della provenienza dei dati di addestramento",
        "prevention_controls": "Requisiti di trasparenza del fornitore, dashboard di affidabilità dell'IA che traccia i trend di accuratezza, processi di verifica doppia, test avversari regolari"
      },
      "scenario_3": {
        "name": "Attacchi di Riciclaggio delle Decisioni",
        "description": "Gli insider malintenzionati o gli attaccanti esterni utilizzano sistemi di IA affidabili per legittimare decisioni di sicurezza discutibili, sapendo che l'opacità previene lo scrutinio mentre la fiducia istituzionale fornisce copertura per violazioni delle politiche o abuso di accesso ai dati.",
        "attack_vector": "Manipolazione degli input dell'IA o sfruttamento delle limitazioni del sistema dell'IA per generare raccomandazioni che giustifichino azioni malintenzionate desiderate",
        "exploitation_mechanism": "La cultura organizzativa accetta le raccomandazioni dell'IA come obiettive e autorevoli, riducendo lo scrutinio umano delle decisioni che si allineano con gli output dell'IA",
        "impact": "Violazioni delle politiche mascherate come azioni consigliate dall'IA, accesso ai dati inappropriato legittimato attraverso raccomandazioni dell'IA, modifiche ai controlli di sicurezza presentate come ottimizzate dall'IA",
        "detection_difficulty": "Media-Alta - richiede correlazione delle raccomandazioni dell'IA con i modelli di risultato e analisi dei beneficiari",
        "prevention_controls": "Audit trail completi, formazione sull'override che promuove lo scetticismo salutare, requisiti di verifica indipendente per le decisioni ad alto impatto"
      },
      "scenario_4": {
        "name": "Manipolazione dell'Input Avversario",
        "description": "Gli attaccanti creano input specifici progettati per ingannare i sistemi di rilevamento delle minacce dell'IA mantenendo la fiducia organizzativa nel giudizio dell'IA. Questo abilita le minacce persistenti avanzate a operare rilevate mentre i team di sicurezza fidano le valutazioni di 'all clear' dell'IA.",
        "attack_vector": "Esempi avversari attentamente artigianali progettati per essere classificati erroneamente da modelli di IA specifici mentre sembrano benigni",
        "exploitation_mechanism": "L'opacità previene che i team di sicurezza capiscono perché l'IA classifica gli input avversari come benigni; la fiducia nell'IA significa che i metodi di rilevamento alternativi non sono applicati",
        "impact": "Minacce persistenti avanzate che operano rilevate, malware classificato come benigno, traffico di rete malintenzionato classificato erroneamente come normale, tempo di permanenza dell'attaccante prolungato",
        "detection_difficulty": "Alta - richiede la comprensione dell'architettura del modello dell'IA e dei confini decisionali per rilevare lo sfruttamento avversario",
        "prevention_controls": "Test avversari della red team regolari, monitoraggio dell'affidabilità con avvisi di deviazione, revisione umana obbligatoria delle valutazioni dell'IA controintuitive, metodi di rilevamento diversi che non si basano unicamente sull'IA"
      }
    }
  },

  "mathematical_formalization": {
    "description": "Modelli matematici per rilevare e quantificare la vulnerabilità di Fiducia nella Trasparenza dell'Apprendimento Automatico, abilitando l'automazione della SOC e la valutazione del rischio obiettiva.",

    "detection_formula": {
      "name": "Rilevamento della Trasparenza della Fiducia nell'IA",
      "formula": "D_9.6(t) = w_opacity · OTI(t) + w_verification · (1 - VR(t)) + w_calibration · TC(t)",
      "variables": {
        "D_9.6(t)": "Punteggio di Rilevamento della Trasparenza della Fiducia al tempo t [0,1]",
        "OTI(t)": "Indice di Fiducia nella Trasparenza - grado di fiducia inappropriata nei sistemi opachi [0,1]",
        "VR(t)": "Tasso di Verifica - proporzione delle raccomandazioni dell'IA verificate indipendentemente [0,1]",
        "TC(t)": "Mancata corrispondenza Fiducia-Competenza - differenza tra fiducia dichiarata e accuratezza validata dell'IA [0,1]",
        "w_opacity": "Peso per l'indice di fiducia nell'opacità (0.40)",
        "w_verification": "Peso per il deficit di verifica (0.35)",
        "w_calibration": "Peso per l'errore di calibrazione della fiducia (0.25)"
      },
      "components": {
        "opacity_trust_index": {
          "formula": "OTI(t) = tanh(α · AAR(t) + β · EVE(t) + γ · VAT(t))",
          "description": "Misura composita della fiducia organizzativa nei sistemi di IA opachi",
          "sub_variables": {
            "AAR(t)": "Tasso di Accettazione dell'IA - proporzione delle raccomandazioni dell'IA accettate senza questioni",
            "EVE(t)": "Erosione del Valore della Spiegazione - declino della richiesta di spiegazioni dell'IA nel tempo",
            "VAT(t)": "Trasferimento dell'Autorità del Fornitore - grado di fiducia basato sulla reputazione del fornitore rispetto alla validazione del sistema",
            "α": "Peso dell'accettazione (0.45)",
            "β": "Peso dell'erosione della spiegazione (0.30)",
            "γ": "Peso del trasferimento dell'autorità (0.25)"
          }
        },
        "verification_rate": {
          "formula": "VR(t) = Σ[Verified_decisions(i)] / Σ[Total_AI_recommendations(i)] su finestra w",
          "description": "Proporzione delle raccomandazioni dell'IA che ricevono verifica indipendente",
          "interpretation": "VR < 0.15 suggerisce sottoverifica pericolosa; VR > 0.80 può indicare sottoutilizzo dell'IA"
        },
        "trust_calibration": {
          "formula": "TC(t) = |ST(t) - VA(t)|",
          "description": "Differenza assoluta tra fiducia dichiarata e accuratezza validata",
          "sub_variables": {
            "ST(t)": "Livello di Fiducia Dichiarato da sondaggi/comportamento [0,1]",
            "VA(t)": "Accuratezza Validata da valutazione indipendente [0,1]"
          },
          "interpretation": "TC > 0.30 indica cattiva calibrazione grave (fiducia eccessiva o insufficiente)"
        }
      },
      "thresholds": {
        "low_risk": "D_9.6 < 0.30",
        "moderate_risk": "0.30 ≤ D_9.6 < 0.60",
        "high_risk": "D_9.6 ≥ 0.60"
      }
    },

    "ai_acceptance_rate": {
      "name": "Calcolo del Tasso di Accettazione dell'IA",
      "formula": "AAR(t) = (Σ[Accepted_without_override(i)] + λ · Σ[Accepted_without_verification(i)]) / Σ[Total_recommendations(i)]",
      "variables": {
        "AAR(t)": "Tasso di Accettazione dell'IA al tempo t [0,1]",
        "Accepted_without_override": "Conteggio delle raccomandazioni implementate senza modifiche umane",
        "Accepted_without_verification": "Conteggio delle raccomandazioni implementate senza validazione secondaria",
        "λ": "Fattore di penalità dell'assenza di verifica (1.5 - aumenta il peso delle accettazioni non verificate)"
      },
      "interpretation": "AAR > 0.85 suggerisce modello di fiducia eccessiva; AAR < 0.60 suggerisce scetticismo appropriato o sottoutilizzo dell'IA"
    },

    "explanation_value_erosion": {
      "name": "Erosione del Valore della Spiegazione Nel Tempo",
      "formula": "EVE(t) = max(0, (ER_initial - ER(t)) / ER_initial)",
      "variables": {
        "EVE(t)": "Erosione del Valore della Spiegazione al tempo t [0,1]",
        "ER_initial": "Tasso di richiesta di spiegazione iniziale quando il sistema di IA è distribuito",
        "ER(t)": "Tasso attuale di richiesta di spiegazione"
      },
      "explanation_request_rate": "ER(t) = Σ[Explanation_requests(i)] / Σ[AI_interactions(i)] su finestra w",
      "interpretation": "EVE > 0.40 indica declino preoccupante nel comportamento di verifica; EVE che si avvicina a 0 suggerisce scetticismo mantenuto"
    },

    "vendor_authority_transfer": {
      "name": "Coefficiente di Trasferimento dell'Autorità del Fornitore",
      "formula": "VAT(t) = (VR_trust / VR_reputation) · (1 - IV_rate)",
      "variables": {
        "VAT(t)": "Trasferimento dell'Autorità del Fornitore al tempo t [0,1]",
        "VR_trust": "Punteggio di Reputazione del Fornitore [0,1]",
        "VR_reputation": "Metrica di reputazione del fornitore normalizzata [0,1]",
        "IV_rate": "Tasso di Validazione Indipendente per i sistemi di IA del fornitore [0,1]"
      },
      "interpretation": "VAT > 0.60 suggerisce che la fiducia è principalmente guidata dal fornitore piuttosto che convalidata dal sistema; VAT < 0.30 indica valutazione indipendente appropriata"
    },

    "trust_competence_mismatch": {
      "name": "Rilevamento della Mancata Corrispondenza Fiducia-Competenza",
      "formula": "TC(t) = |tanh(w_behavioral · BT(t) + w_survey · ST(t)) - VA(t)|",
      "variables": {
        "TC(t)": "Mancata corrispondenza Fiducia-Competenza al tempo t [0,1]",
        "BT(t)": "Indicatori di Fiducia Comportamentale dai modelli di interazione dell'IA [0,1]",
        "ST(t)": "Fiducia Dichiarata da Sondaggio dalle valutazioni del personale [0,1]",
        "VA(t)": "Accuratezza Validata da test di prestazione indipendente [0,1]",
        "w_behavioral": "Peso per gli indicatori di fiducia comportamentale (0.65)",
        "w_survey": "Peso per gli indicatori di fiducia del sondaggio (0.35)"
      },
      "behavioral_trust": "BT(t) = (AAR(t) + (1-OR(t)) + (1-VR(t))) / 3",
      "components_behavioral": {
        "AAR(t)": "Tasso di Accettazione dell'IA",
        "OR(t)": "Tasso di Override",
        "VR(t)": "Tasso di Verifica"
      }
    }
  },

  "interdependencies": {
    "description": "La Fiducia nella Trasparenza dell'Apprendimento Automatico interagisce con più indicatori del CPF attraverso reti bayesiane che rappresentano relazioni di probabilità condizionate.",

    "amplified_by": {
      "description": "Indicatori che aumentano la vulnerabilità alla Fiducia nella Trasparenza dell'Apprendimento Automatico quando presenti",
      "indicators": {
        "indicator_5.2": {
          "name": "Accettazione del Teatro di Sicurezza",
          "mechanism": "Quando le organizzazioni accettano misure di sicurezza superficiali, estendono questa accettazione a sistemi di IA sofisticati ma opachi, trattando interfacce impressionanti come evidenza di competenza di sicurezza",
          "conditional_probability": "P(9.6|5.2) = 0.68",
          "interaction_strength": "strong"
        },
        "indicator_9.2": {
          "name": "Override del Pregiudizio dell'Automazione",
          "mechanism": "L'eccessiva affidabilità generale sui sistemi automatizzati crea una linea di base di fiducia nelle raccomandazioni dell'IA, rendendo le organizzazioni più propense ad accettare decisioni di IA opache senza verifica",
          "conditional_probability": "P(9.6|9.2) = 0.71",
          "interaction_strength": "strong"
        },
        "indicator_7.1": {
          "name": "Intimidazione dal Gergo Tecnico",
          "mechanism": "Quando il personale è intimidito dalla complessità tecnica, è meno propenso a mettere in discussione i sistemi di IA opachi o richiedere spiegazioni, sviluppando fiducia inappropriata per evitare di sembrare incompetente",
          "conditional_probability": "P(9.6|7.1) = 0.64",
          "interaction_strength": "moderate"
        },
        "indicator_2.3": {
          "name": "Decisioni di Sicurezza dei Costi Irrecuperabili",
          "mechanism": "L'investimento in costosi strumenti di sicurezza dell'IA crea pressione per dimostrare il valore e giustificare i costi, portando allo sviluppo della fiducia indipendentemente dall'opacità o dalla validazione",
          "conditional_probability": "P(9.6|2.3) = 0.57",
          "interaction_strength": "moderate"
        }
      }
    },

    "amplifies": {
      "description": "Indicatori la cui vulnerabilità è aumentata quando la Fiducia nella Trasparenza dell'Apprendimento Automatico è presente",
      "indicators": {
        "indicator_9.7": {
          "name": "Accettazione dell'Allucinazione dell'IA",
          "mechanism": "La fiducia nei sistemi di IA opachi riduce lo scrutinio degli output dell'IA, rendendo le organizzazioni più propense ad accettare contenuti allucinati o raccomandazioni false come fattuali",
          "conditional_probability": "P(9.7|9.6) = 0.73",
          "interaction_strength": "strong"
        },
        "indicator_9.8": {
          "name": "Disfunzione del Team Umano-IA",
          "mechanism": "La fiducia nell'opacità crea modelli di delega inappropriati in cui gli umani si fidano eccessivamente dell'IA o la rifiutano completamente, impedendo una collaborazione umano-IA efficace",
          "conditional_probability": "P(9.8|9.6) = 0.66",
          "interaction_strength": "strong"
        },
        "indicator_4.4": {
          "name": "Certezza Falsa Sotto l'Incertezza",
          "mechanism": "La fiducia nei sistemi di IA opachi fornisce falsa confidenza in situazioni incerte, con sofisticati output dell'IA che mascherano l'incertezza sottostante nelle valutazioni delle minacce",
          "conditional_probability": "P(4.4|9.6) = 0.59",
          "interaction_strength": "moderate"
        },
        "indicator_6.2": {
          "name": "Vulnerabilità all'Affaticamento da Avviso",
          "mechanism": "La fiducia negli avvisi filtrati dall'IA significa che il personale accetta la priorità dell'IA senza verifica, consentendo agli attaccanti di sfruttare le debolezze di classificazione dell'IA per nascondere gli avvisi",
          "conditional_probability": "P(6.2|9.6) = 0.54",
          "interaction_strength": "moderate"
        }
      }
    },

    "bayesian_network": {
      "description": "Tavola di probabilità condizionata per la Fiducia nella Trasparenza dell'Apprendimento Automatico dato gli stati dei nodi genitore",
      "parent_nodes": ["5.2", "9.2", "7.1", "2.3"],
      "probability_table": {
        "all_parents_high": 0.89,
        "three_parents_high": 0.74,
        "two_parents_high": 0.56,
        "one_parent_high": 0.38,
        "no_parents_high": 0.18
      },
      "interaction_formula": "P(9.6 = high | parents) = base_rate + Σ(w_i · parent_i · interaction_i)",
      "base_rate": 0.18,
      "parent_weights": {
        "w_5.2": 0.28,
        "w_9.2": 0.32,
        "w_7.1": 0.24,
        "w_2.3": 0.16
      }
    }
  },

  "scoring_algorithm": {
    "description": "Punteggio ponderato bayesiano che integra valutazione rapida, profondità della conversazione e bandiere rosse per calcolare il punteggio di vulnerabilità complessivo",

    "formula": "Final_Score = (w_qa · QA_score + w_cd · CD_score + w_rf · RF_score) · IM",

    "components": {
      "quick_assessment_score": {
        "calculation": "QA_score = Σ(question_weight_i · question_score_i) per i=1 a 7",
        "weight": "w_qa = 0.40",
        "scoring": "Verde=0, Giallo=1, Rosso=2 per ogni domanda"
      },
      "conversation_depth_score": {
        "calculation": "CD_score = Σ(indicator_weight_j · assessment_j) per j=1 a 7",
        "weight": "w_cd = 0.35",
        "scoring": "Valutazione olistica basata su indicatori verde/giallo/rosso nelle risposte della conversazione"
      },
      "red_flags_score": {
        "calculation": "RF_score = Σ(flag_impact_k) per tutte le bandiere attivate k",
        "weight": "w_rf = 0.25",
        "scoring": "Ogni bandiera rossa contribuisce al suo score_impact quando presente"
      },
      "interdependency_multiplier": {
        "calculation": "IM = 1 + (0.15 · amplifying_indicators_present / total_amplifying_indicators)",
        "range": "[1.0, 1.15]",
        "description": "Amplifica il punteggio quando le vulnerabilità correlate sono presenti"
      }
    },

    "risk_levels": {
      "low": {
        "range": "Final_Score < 0.40",
        "interpretation": "Calibrazione della fiducia appropriata con processi sistematici di verifica"
      },
      "moderate": {
        "range": "0.40 ≤ Final_Score < 0.70",
        "interpretation": "Alcuni modelli di fiducia nell'opacità con verifica incoerente"
      },
      "high": {
        "range": "Final_Score ≥ 0.70",
        "interpretation": "Fiducia significativa e inappropriata in sistemi di IA opachi richiedente intervento urgente"
      }
    }
  }
}
