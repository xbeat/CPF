{
  "indicator": "9.10",
  "title": "INDICATOR 9.10 FIELD KIT",
  "subtitle": "Cecit√† all'Equit√† Algoritmica",
  "category": "AI-Specific Bias Vulnerabilities",
  "version": "1.0",
  "cpf_reference": "CPF v1.0 - Category 9.x",
  "description": {
    "short": "Algorithmic Fairness Blindness represents a complex psychological vulnerability where individuals and organizations develop systematic blind spots to biased or discriminatory outputs from AI systems. This phenomenon emerges from trust transfer to authority where individuals assume AI possesses inher...",
    "context": "Algorithmic Fairness Blindness represents a complex psychological vulnerability where individuals and organizations develop systematic blind spots to biased or discriminatory outputs from AI systems. This phenomenon emerges from trust transfer to authority where individuals assume AI possesses inherent objectivity, system justification bias extending to defending algorithmic decisions, and automation bias preventing critical evaluation of AI outputs. This creates security vulnerabilities because biased AI systems create exploitable patterns - attackers can predict which populations will be under-monitored, which alerts will be deprioritized, and which security gaps exist, enabling discriminatory social engineering, insider threat exploitation through monitoring blind spots, AI poisoning attacks, and regulatory compliance failures.",
    "impact": "See full Bayesian indicator documentation for detailed impact analysis.",
    "psychological_basis": "See full Bayesian indicator documentation for psychological foundations."
  },
  "scoring": {
    "method": "bayesian_weighted",
    "weights": {
      "quick_assessment": 0.4,
      "conversation_depth": 0.35,
      "red_flags": 0.25
    }
  },
  "sections": [
    {
      "id": "quick-assessment",
      "title": "Valutazione Rapida - Cecit√† all'Equit√† Algoritmica",
      "icon": "üéØ",
      "time": "15-20",
      "items": [
        {
          "type": "radio-group",
          "number": "Q1",
          "title": "How often do you test your AI-enabled security systems (SIEM, behavioral analysis, access controls) for biased outputs across different user populations?",
          "options": [
            {
              "value": "green",
              "label": "Regular quarterly bias testing with documented results and remediation actions",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Occasional bias testing but not systematic or comprehensive",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "Raramente o mai testano per bias, presumono che i sistemi AI siano obiettivi",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q2",
          "title": "When procuring AI security tools, what's your process for evaluating potential discriminatory impacts on different employee or user groups?",
          "options": [
            {
              "value": "green",
              "label": "Mandatory discrimination risk assessment as formal procurement criteria with vendor fairness documentation requirements",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Some fairness considerations in procurement but not systematic or weighted equally with technical capabilities",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "Nessuna valutazione equit√† nel processo di approvvigionamento, focus solo su caratteristiche tecniche e costo",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q3",
          "title": "Who is involved in oversight of your AI-powered security systems - what roles and departments participate in reviewing AI decisions?",
          "options": [
            {
              "value": "green",
              "label": "Diverse cross-functional oversight team including security, legal, HR, business representatives from varied backgrounds",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Some cross-functional involvement but limited diversity or informal oversight structure",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "Homogeneous technical team oversight only, minimal diversity in AI decision-making roles",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q4",
          "title": "What's your procedure when someone raises concerns about potentially unfair treatment by your AI security systems?",
          "options": [
            {
              "value": "green",
              "label": "Formal fairness incident response process with escalation paths, investigation methods, and remediation requirements",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Informal process for addressing concerns but Nessun sistema investigation or remediation framework",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "Nessuna procedura definita, fairness concerns respinte come problematiche non tecniche or not taken seriously",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q5",
          "title": "How do you monitor ongoing performance of AI security systems to detect if they're applying different security standards to different populations?",
          "options": [
            {
              "value": "green",
              "label": "Continuous monitoring with fairness metrics, automated alerts for statistical disparities, regular performance reports including fairness dimensions",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Some monitoring of AI performance but fairness metrics not systematically tracked or reported",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "Nessun monitoraggio specifico equit√†, valutazione prestazioni si concentra esclusivamente su metriche tecniche",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q6",
          "title": "What training do your security and IT teams receive specifically about identifying and addressing bias in AI security tools?",
          "options": [
            {
              "value": "green",
              "label": "Mandatory targeted training on AI bias with hands-on exercises, case studies, and regular refreshers",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "General awareness training that mentions AI bias but lacks depth or hands-on components",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "Nessuna formazione specifica su bias AI, o problemi di bias non affrontati nella formazione sicurezza",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q7",
          "title": "What percentage of your AI security system budget is allocated to bias detection, fairness testing, or discrimination monitoring tools?",
          "options": [
            {
              "value": "green",
              "label": "Dedicated budget line items for fairness evaluation (5%+ of AI security budget), documented spending on bias detection tools",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Some informal allocation to fairness tools but not tracked as separate budget category",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "Zero specific budget allocation for bias detection or fairness evaluation",
              "score": 1
            }
          ]
        }
      ],
      "subsections": []
    },
    {
      "id": "client-conversation",
      "title": "Guida Conversazione Cliente",
      "icon": "üí¨",
      "time": "25-35",
      "items": [],
      "subsections": []
    }
  ],
  "data_sources": [],
  "validation": {},
  "remediation": {}
}