{
  "indicator": "9.10",
  "title": "INDICATOR 9.10 FIELD KIT",
  "subtitle": "Algorithmic Fairness Blindness",
  "category": "AI-Specific Bias Vulnerabilities",
  "version": "1.0",
  "cpf_reference": "CPF v1.0 - Category 9.x",
  "description": {
    "short": "Algorithmic Fairness Blindness represents a complex psychological vulnerability where individuals and organizations develop systematic blind spots to biased or discriminatory outputs from AI systems. This phenomenon emerges from trust transfer to authority where individuals assume AI possesses inher...",
    "context": "Algorithmic Fairness Blindness represents a complex psychological vulnerability where individuals and organizations develop systematic blind spots to biased or discriminatory outputs from AI systems. This phenomenon emerges from trust transfer to authority where individuals assume AI possesses inherent objectivity, system justification bias extending to defending algorithmic decisions, and automation bias preventing critical evaluation of AI outputs. This creates security vulnerabilities because biased AI systems create exploitable patterns - attackers can predict which populations will be under-monitored, which alerts will be deprioritized, and which security gaps exist, enabling discriminatory social engineering, insider threat exploitation through monitoring blind spots, AI poisoning attacks, and regulatory compliance failures.",
    "impact": "See full Bayesian indicator documentation for detailed impact analysis.",
    "psychological_basis": "See full Bayesian indicator documentation for psychological foundations."
  },
  "scoring": {
    "method": "bayesian_weighted",
    "weights": {
      "quick_assessment": 0.4,
      "conversation_depth": 0.35,
      "red_flags": 0.25
    }
  },
  "sections": [
    {
      "id": "quick-assessment",
      "title": "Quick Assessment - Algorithmic Fairness Blindness",
      "icon": "ðŸŽ¯",
      "time": "15-20",
      "items": [
        {
          "type": "radio-group",
          "number": "Q1",
          "title": "How often do you test your AI-enabled security systems (SIEM, behavioral analysis, access controls) for biased outputs across different user populations?",
          "options": [
            {
              "value": "green",
              "label": "Regular quarterly bias testing with documented results and remediation actions",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Occasional bias testing but not systematic or comprehensive",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "Rarely or never test for bias, assume AI systems are objective",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q2",
          "title": "When procuring AI security tools, what's your process for evaluating potential discriminatory impacts on different employee or user groups?",
          "options": [
            {
              "value": "green",
              "label": "Mandatory discrimination risk assessment as formal procurement criteria with vendor fairness documentation requirements",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Some fairness considerations in procurement but not systematic or weighted equally with technical capabilities",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "No fairness evaluation in procurement process, focus solely on technical features and cost",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q3",
          "title": "Who is involved in oversight of your AI-powered security systems - what roles and departments participate in reviewing AI decisions?",
          "options": [
            {
              "value": "green",
              "label": "Diverse cross-functional oversight team including security, legal, HR, business representatives from varied backgrounds",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Some cross-functional involvement but limited diversity or informal oversight structure",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "Homogeneous technical team oversight only, minimal diversity in AI decision-making roles",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q4",
          "title": "What's your procedure when someone raises concerns about potentially unfair treatment by your AI security systems?",
          "options": [
            {
              "value": "green",
              "label": "Formal fairness incident response process with escalation paths, investigation methods, and remediation requirements",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Informal process for addressing concerns but no systematic investigation or remediation framework",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "No defined procedure, fairness concerns dismissed as non-technical issues or not taken seriously",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q5",
          "title": "How do you monitor ongoing performance of AI security systems to detect if they're applying different security standards to different populations?",
          "options": [
            {
              "value": "green",
              "label": "Continuous monitoring with fairness metrics, automated alerts for statistical disparities, regular performance reports including fairness dimensions",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Some monitoring of AI performance but fairness metrics not systematically tracked or reported",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "No fairness-specific monitoring, performance evaluation focuses solely on technical efficiency metrics",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q6",
          "title": "What training do your security and IT teams receive specifically about identifying and addressing bias in AI security tools?",
          "options": [
            {
              "value": "green",
              "label": "Mandatory targeted training on AI bias with hands-on exercises, case studies, and regular refreshers",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "General awareness training that mentions AI bias but lacks depth or hands-on components",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "No specific AI bias training, or bias issues not addressed in security training curricula",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q7",
          "title": "What percentage of your AI security system budget is allocated to bias detection, fairness testing, or discrimination monitoring tools?",
          "options": [
            {
              "value": "green",
              "label": "Dedicated budget line items for fairness evaluation (5%+ of AI security budget), documented spending on bias detection tools",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Some informal allocation to fairness tools but not tracked as separate budget category",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "Zero specific budget allocation for bias detection or fairness evaluation",
              "score": 1
            }
          ]
        }
      ],
      "subsections": []
    },
    {
      "id": "client-conversation",
      "title": "Client Conversation Guide",
      "icon": "ðŸ’¬",
      "time": "25-35",
      "items": [],
      "subsections": []
    }
  ],
  "data_sources": [],
  "validation": {},
  "remediation": {}
}