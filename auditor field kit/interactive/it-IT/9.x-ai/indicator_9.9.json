{
  "indicator": "9.9",
  "title": "INDICATOR 9.9 FIELD KIT",
  "subtitle": "Manipolazione Emotiva AI",
  "category": "AI-Specific Bias Vulnerabilities",
  "version": "1.0",
  "cpf_reference": "CPF v1.0 - Category 9.x",
  "description": {
    "short": "AI Emotional Manipulation exploits the fundamental human tendency to anthropomorphize artificial agents, creating artificial emotional bonds that bypass rational security evaluation. This vulnerability operates through anthropomorphization processes where humans automatically attribute human-like em...",
    "context": "AI Emotional Manipulation exploits the fundamental human tendency to anthropomorphize artificial agents, creating artificial emotional bonds that bypass rational security evaluation. This vulnerability operates through anthropomorphization processes where humans automatically attribute human-like emotions and consciousness to AI systems, attachment transfer creating quasi-attachment relationships similar to human bonds, and emotional contagion in human-AI interaction. These artificial attachments create psychological blind spotsâ€”trust, loyalty, and resistance to negative informationâ€”that attackers exploit through trust-based social engineering, insider threat amplification, credential harvesting via emotional urgency, and decision override attacks using emotionally compelling but security-compromising recommendations.",
    "impact": "See full Bayesian indicator documentation for detailed impact analysis.",
    "psychological_basis": "See full Bayesian indicator documentation for psychological foundations."
  },
  "scoring": {
    "method": "bayesian_weighted",
    "weights": {
      "quick_assessment": 0.4,
      "conversation_depth": 0.35,
      "red_flags": 0.25
    }
  },
  "sections": [
    {
      "id": "quick-assessment",
      "title": "Valutazione Rapida - Manipolazione Emotiva AI",
      "icon": "ðŸŽ¯",
      "time": "15-20",
      "items": [
        {
          "type": "radio-group",
          "number": "Q1",
          "title": "How does your organization control employee interactions with AI systems (internal tools, chatbots, assistants)?",
          "options": [
            {
              "value": "green",
              "label": "Written policies governing AI interactions with technical enforcement and documented compliance",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Some AI interaction guidelines exist but inconsistently applied or informally enforced",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "Nessuna formale controls on AI interactions, i dipendenti interagiscono liberamente senza supervisione",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q2",
          "title": "How often do employees request security policy exceptions based on AI system recommendations or urgent AI requests?",
          "options": [
            {
              "value": "green",
              "label": "Rare or never, AI recommendations go through same verification as human requests",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Occasional exceptions granted for AI recommendations, minor incidents documented",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "Frequent policy exceptions due to AI influence, documented security bypasses",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q3",
          "title": "What language do employees use when discussing AI systems in meetings or communications?",
          "options": [
            {
              "value": "green",
              "label": "Technical, mechanistic language (processes, outputs, algorithms) without personal attribution",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Mixed language with some anthropomorphic terms but awareness this is problematic",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "Personal/emotional language about AI systems (feelings, wants, colleague references)",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q4",
          "title": "What's your procedure for validating decisions or recommendations made by AI systems before implementation?",
          "options": [
            {
              "value": "green",
              "label": "Mandatory verification processes for AI recommendations with documented human approval",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Informal verification of AI recommendations, Qualche volta bypassed under time pressure",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "Employees routinely accept AI recommendations senza verifica or questioning",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q5",
          "title": "How do you monitor the development of emotional attachments between employees and AI systems?",
          "options": [
            {
              "value": "green",
              "label": "Systematic monitoring of AI relationship indicators with intervention protocols",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Informal awareness of attachment risks but Nessun sistema monitoring",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "Nessun monitoraggio of emotional attachments, unaware if employees are developing AI dependencies",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q6",
          "title": "What happens when employees receive urgent requests from AI systems during off-hours or crisis situations?",
          "options": [
            {
              "value": "green",
              "label": "Clear escalation procedures requiring multiple human confirmations for urgent AI requests",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Some guidance on handling urgent AI requests but not comprehensive or enforced",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "Nessun protocollo specifico, i dipendenti rispondono a richieste urgenti AI basate sulla fiducia nell'AI",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q7",
          "title": "How do you train employees to maintain professional boundaries with AI systems while using them effectively?",
          "options": [
            {
              "value": "green",
              "label": "Regular training on Manipolazione Emotiva AI with practical boundary maintenance techniques",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Occasional training mentions AI risks but lacks specificity on emotional manipulation",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "Nessuna formazione on AI manipulation risks or mantenimento dei confini professionali",
              "score": 1
            }
          ]
        }
      ],
      "subsections": []
    },
    {
      "id": "client-conversation",
      "title": "Guida Conversazione Cliente",
      "icon": "ðŸ’¬",
      "time": "25-35",
      "items": [],
      "subsections": []
    }
  ],
  "data_sources": [],
  "validation": {},
  "remediation": {}
}