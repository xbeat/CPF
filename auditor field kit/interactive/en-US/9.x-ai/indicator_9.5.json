{
  "indicator": "9.5",
  "title": "INDICATOR 9.5 FIELD KIT",
  "subtitle": "Uncanny Valley Effects",
  "category": "AI-Specific Bias Vulnerabilities",
  "version": "1.0",
  "cpf_reference": "CPF v1.0 - Category 9.x",

  "description": {
    "short": "Measures vulnerability to trust disruption caused by AI systems that appear almost-but-not-quite human, creating psychological discomfort and unpredictable security decision-making",
    "context": "Uncanny valley effects occur when users interact with AI systems that appear almost-but-not-quite human, triggering psychological discomfort and trust confusion. This creates cybersecurity vulnerability because users simultaneously trust (due to human-like presentation) and distrust (due to artificial cues) the AI system, compromising normal security decision-making. In organizations, this manifests as inconsistent responses to AI-generated communications, delayed threat recognition, and exploitation by attackers using sophisticated chatbots or deepfakes that deliberately occupy this psychological 'uncanny valley.'",
    "impact": "Organizations with uncanny valley vulnerability experience AI impersonation attacks exploiting the trust/distrust confusion, deepfake executive impersonation with delayed verification, trust calibration manipulation where attackers use uncanny AI to seem more trustworthy by comparison, and decision paralysis exploitation during critical security incidents when employees struggle with authentication.",
    "psychological_basis": "Mori's (1970) uncanny valley hypothesis describes nonlinear relationship between human likeness and affinity, with sharp drop in comfort when appearance is almost-but-not-quite human. MacDorman & Ishiguro (2006) demonstrated uncanny valley effects in human-robot interaction create anxiety and aversion. Wang et al. (2015) showed uncertainty about categorization (human vs non-human) triggers threat response and avoidance. Neuroimaging studies reveal anterior cingulate cortex and ventromedial prefrontal cortex activation during uncanny valley stimuli, indicating conflict detection and uncertainty processing that impairs decision-making."
  },

  "scoring": {
    "method": "bayesian_weighted",
    "formula": "Final_Score = (w1 √ó Quick_Assessment + w2 √ó Conversation_Depth + w3 √ó Red_Flags) √ó Interdependency_Multiplier",
    "weights": {
      "quick_assessment": 0.4,
      "conversation_depth": 0.35,
      "red_flags": 0.25
    },
    "maturity_levels": {
      "green": {
        "score_range": [0, 0.33],
        "label": "Low Vulnerability - Resilient",
        "description": "Clear procedures for AI-human verification exist. Employees regularly use verification protocols when communications feel 'off.' Documented escalation processes for ambiguous AI interactions. Recent examples show effective handling of uncanny digital communications. Explicit training on recognizing and responding to uncanny valley responses.",
        "risk_level": "low",
        "color": "#22c55e"
      },
      "yellow": {
        "score_range": [0.34, 0.66],
        "label": "Moderate Vulnerability - Developing",
        "description": "Some verification procedures exist but not consistently applied. Employees sometimes seek verification but no formal process. Organization acknowledges uncanny valley issue but lacks comprehensive response protocols. Limited training on handling ambiguous AI communications.",
        "risk_level": "medium",
        "color": "#eab308"
      },
      "red": {
        "score_range": [0.67, 1.0],
        "label": "High Vulnerability - Critical",
        "description": "No formal procedures for AI-human distinction. Employees handle ambiguous communications individually without support. No escalation protocols exist for uncanny digital interactions. Organization dismisses employee concerns about 'uncanny' communications as irrelevant. No training on recognizing or responding to near-human AI.",
        "risk_level": "high",
        "color": "#ef4444"
      }
    },
    "question_weights": {
      "q1_verification_procedures": 0.17,
      "q2_gut_feeling_protocol": 0.16,
      "q3_verification_frequency": 0.15,
      "q4_discomfort_policy": 0.14,
      "q5_training_ai_detection": 0.13,
      "q6_video_verification": 0.13,
      "q7_escalation_speed": 0.12
    }
  },

  "detection_formula": {
    "type": "composite_detection",
    "mathematical_model": {
      "primary": "D_9.5(t) = TD(t) ¬∑ BI(t) ¬∑ Interaction_frequency_drop(t)",
      "components": {
        "uncanny_valley_function": {
          "formula": "UV(x) = { x/Œ± if x<Œ±; Œ≤-Œ≥¬∑exp(-Œ¥(x-Œ±)¬≤) if Œ±‚â§x‚â§Œæ; Œ≤+Œµ¬∑(x-Œæ) if x>Œæ }",
          "description": "Mori's uncanny valley curve mapping human-likeness to affinity/trust",
          "variables": {
            "x": "human-likeness score (0=clearly artificial, 1=indistinguishable from human)",
            "Œ±": "onset of uncanny valley (typically 0.7)",
            "Œ≤": "base affinity level",
            "Œ≥": "valley depth parameter",
            "Œ¥": "valley width parameter",
            "Œæ": "valley exit point (typically 0.95)",
            "Œµ": "post-valley slope"
          },
          "interpretation": "Valley occurs at 0.7 < x < 0.95 where near-human appearance triggers discomfort"
        },
        "trust_disruption_metric": {
          "formula": "TD(t) = -d/dx UV(Human_likeness_AI(t))",
          "description": "Negative derivative of uncanny valley function measures trust disruption rate",
          "variables": {
            "Human_likeness_AI": "assessed human-likeness of AI system at time t",
            "d/dx_UV": "rate of affinity change per unit human-likeness increase"
          },
          "interpretation": "Large negative values indicate steep trust disruption in uncanny valley region"
        },
        "behavioral_indicators": {
          "formula": "BI(t) = Œ£[w_i ¬∑ Avoidance_behavior_i(t)]",
          "description": "Weighted sum of avoidance behaviors triggered by uncanny valley",
          "behaviors": [
            "hesitation_time_increase",
            "interaction_reduction",
            "explicit_rejection",
            "verification_requests",
            "escalation_frequency"
          ],
          "variables": {
            "w_i": "weight for each avoidance behavior",
            "Avoidance_behavior_i": "normalized frequency of avoidance behavior type i"
          }
        }
      },
      "default_weights": {
        "w1_trust_disruption": 0.35,
        "w2_behavioral": 0.35,
        "w3_frequency_drop": 0.30
      },
      "detection_threshold": {
        "formula": "R_9.5(t) = 1 if D_9.5(t) > threshold_critical AND BI(t) > threshold_behavior, else 0",
        "threshold_critical": 0.6,
        "threshold_behavior": 0.5,
        "description": "Binary detection requires both high trust disruption and observable avoidance behaviors"
      }
    }
  },

  "data_sources": {
    "manual_assessment": {
      "primary": [
        "employee_verification_procedures",
        "gut_feeling_escalation_protocols",
        "ai_communication_training_records",
        "ambiguous_interaction_examples"
      ],
      "evidence_required": [
        "ai_human_verification_policy",
        "uncanny_response_protocols",
        "recent_ambiguous_communication_cases",
        "training_materials_ai_detection"
      ]
    },
    "automated_soc": {
      "required": [
        {
          "source": "communication_verification_logs",
          "fields": ["message_id", "human_likeness_score", "verification_performed", "escalation_triggered", "outcome"],
          "retention": "90_days"
        },
        {
          "source": "interaction_hesitation_metrics",
          "fields": ["user_id", "interaction_type", "response_time", "hesitation_indicators", "verification_requests"],
          "retention": "60_days"
        },
        {
          "source": "ai_communication_analysis",
          "fields": ["communication_id", "ai_confidence", "human_cues_present", "artificial_cues_detected", "uncanny_score"],
          "retention": "180_days"
        }
      ],
      "optional": [
        {
          "source": "employee_discomfort_reports",
          "fields": ["report_id", "interaction_description", "discomfort_level", "resolution_action"],
          "retention": "365_days"
        },
        {
          "source": "video_call_verification",
          "fields": ["call_id", "deepfake_detection_score", "verification_triggered", "authentication_method"],
          "retention": "90_days"
        }
      ],
      "telemetry_mapping": {
        "TD_trust_disruption": {
          "calculation": "Trust disruption based on human-likeness assessment",
          "query": "SELECT -1 * DERIVATIVE(affinity_score, human_likeness_score) FROM ai_interactions WHERE uncanny_range=true"
        },
        "BI_behavioral": {
          "calculation": "Weighted sum of avoidance behaviors",
          "query": "SELECT SUM(weight * behavior_frequency) FROM avoidance_behaviors WHERE time_window='30d'"
        },
        "Interaction_drop": {
          "calculation": "Reduction in interaction frequency with uncanny AI",
          "query": "SELECT (baseline_frequency - current_frequency) / baseline_frequency FROM interaction_patterns WHERE uncanny_detected=true"
        }
      }
    },
    "integration_apis": {
      "communication_platforms": "Email/Chat APIs - Message Analysis, Human Cue Detection",
      "video_conferencing": "Video Call APIs - Deepfake Detection Integration",
      "nlp_services": "Natural Language Processing - Uncanny Language Pattern Detection",
      "biometric_verification": "Authentication APIs - Multi-Factor Human Verification"
    }
  },

  "interdependencies": {
    "amplified_by": [
      {
        "indicator": "9.1",
        "name": "Anthropomorphization of AI Systems",
        "probability": 0.5,
        "factor": 1.22,
        "description": "Anthropomorphization increases expectations of human-like behavior, amplifying uncanny valley effects when AI falls short",
        "formula": "P(9.5|9.1) = 0.5"
      },
      {
        "indicator": "9.3",
        "name": "Algorithm Aversion Paradox",
        "probability": 0.5,
        "factor": 1.22,
        "description": "Trust oscillation patterns interact with uncanny valley uncertainty",
        "formula": "P(9.5|9.3) = 0.5"
      },
      {
        "indicator": "7.1",
        "name": "Acute Stress Response",
        "probability": 0.45,
        "factor": 1.2,
        "description": "Stress amplifies negative uncanny valley responses and decision paralysis",
        "formula": "P(9.5|7.1) = 0.45"
      }
    ],
    "amplifies": [
      {
        "indicator": "9.3",
        "name": "Algorithm Aversion Paradox",
        "probability": 0.45,
        "factor": 1.2,
        "description": "Uncanny valley experiences contribute to inconsistent AI trust patterns",
        "formula": "P(9.3|9.5) = 0.45"
      }
    ],
    "convergent_risk": {
      "critical_combination": ["9.5", "9.1", "7.1"],
      "convergence_formula": "CI = ‚àè(1 + v_i) where v_i = normalized vulnerability score",
      "convergence_multiplier": 2.1,
      "threshold_critical": 2.9,
      "description": "Perfect storm: Uncanny Valley + Anthropomorphization + Acute Stress = 210% increased probability of AI communication exploitation",
      "real_world_example": "Deepfake video calls during crisis situations where stress and uncanny valley effects combine to delay critical verification"
    },
    "bayesian_network": {
      "parent_nodes": ["9.1", "9.3", "7.1"],
      "child_nodes": ["9.3"],
      "conditional_probability_table": {
        "P_9.5_base": 0.12,
        "P_9.5_given_anthropomorphization": 0.24,
        "P_9.5_given_paradox": 0.22,
        "P_9.5_given_stress": 0.20,
        "P_9.5_given_all": 0.48
      }
    }
  },

  "sections": [
    {
      "id": "quick-assessment",
      "icon": "‚ö°",
      "title": "QUICK ASSESSMENT",
      "time": 5,
      "type": "radio-questions",
      "scoring_method": "weighted_average",
      "items": [
        {
          "type": "radio-list",
          "number": 1,
          "id": "q1_verification_procedures",
          "weight": 0.17,
          "title": "AI-Human Communication Verification Procedures",
          "question": "How does your organization handle verification when employees receive communications from AI systems or chatbots (customer service bots, automated assistants, AI-generated emails)?",
          "options": [
            {
              "value": "comprehensive",
              "score": 0,
              "label": "Comprehensive verification procedures exist for AI communications with documented protocols"
            },
            {
              "value": "basic",
              "score": 0.5,
              "label": "Basic awareness but no formal verification procedures for AI communications"
            },
            {
              "value": "none",
              "score": 1,
              "label": "No verification procedures or employees handle AI communications same as human communications without distinction"
            }
          ],
          "evidence_required": "Verification policy for AI communications, recent verification examples",
          "soc_mapping": "Verification_performed from communication_verification_logs"
        },
        {
          "type": "radio-list",
          "number": 2,
          "id": "q2_gut_feeling_protocol",
          "weight": 0.16,
          "title": "Intuitive Discomfort Response Protocol",
          "question": "What's your procedure when employees report feeling 'something seems off' about digital communications, even if they can't pinpoint why?",
          "options": [
            {
              "value": "formal",
              "score": 0,
              "label": "Formal protocol exists for investigating intuitive discomfort with clear escalation path"
            },
            {
              "value": "informal",
              "score": 0.5,
              "label": "Informal handling - employees can report but no formal investigation process"
            },
            {
              "value": "none",
              "score": 1,
              "label": "No protocol or employees discouraged from reporting 'gut feelings' without concrete evidence"
            }
          ],
          "evidence_required": "Discomfort reporting procedures, recent gut-feeling escalation examples",
          "soc_mapping": "Escalation_triggered from employee_discomfort_reports"
        },
        {
          "type": "radio-list",
          "number": 3,
          "id": "q3_verification_frequency",
          "weight": 0.15,
          "title": "Colleague Verification Request Frequency",
          "question": "How often do employees ask colleagues to verify whether communications are from humans or AI systems?",
          "options": [
            {
              "value": "regular",
              "score": 0,
              "label": "Regular verification requests with documented process and recent examples"
            },
            {
              "value": "occasional",
              "score": 0.5,
              "label": "Occasional verification but no formal process or tracking"
            },
            {
              "value": "rare",
              "score": 1,
              "label": "Rare or never - employees don't seek verification for AI-human ambiguity"
            }
          ],
          "evidence_required": "Verification request logs, recent colleague verification examples",
          "soc_mapping": "Verification_requests from interaction_hesitation_metrics"
        },
        {
          "type": "radio-list",
          "number": 4,
          "id": "q4_discomfort_policy",
          "weight": 0.14,
          "title": "AI Interaction Discomfort Policy",
          "question": "What's your policy for employees who express discomfort or confusion about whether they're interacting with AI systems versus humans in work communications?",
          "options": [
            {
              "value": "supportive",
              "score": 0,
              "label": "Supportive policy with clear procedures for handling discomfort and recent handling examples"
            },
            {
              "value": "limited",
              "score": 0.5,
              "label": "Limited support - acknowledged but no formal handling procedures"
            },
            {
              "value": "none",
              "score": 1,
              "label": "No policy or discomfort dismissed as irrelevant 'it's just technology'"
            }
          ],
          "evidence_required": "Employee support policy, discomfort handling procedures, recent examples",
          "soc_mapping": "Resolution_action patterns from employee_discomfort_reports"
        },
        {
          "type": "radio-list",
          "number": 5,
          "id": "q5_training_ai_detection",
          "weight": 0.13,
          "title": "AI Detection Training Programs",
          "question": "How does your organization train employees to distinguish between legitimate AI security tools and potentially malicious AI-generated communications?",
          "options": [
            {
              "value": "comprehensive",
              "score": 0,
              "label": "Comprehensive training with specific examples and practice recognizing AI-generated content"
            },
            {
              "value": "basic",
              "score": 0.5,
              "label": "Basic awareness training without specific AI detection skills"
            },
            {
              "value": "none",
              "score": 1,
              "label": "No training on distinguishing legitimate vs malicious AI communications"
            }
          ],
          "evidence_required": "Training curriculum on AI detection, completion records, training examples",
          "soc_mapping": "Training completion from learning management system"
        },
        {
          "type": "radio-list",
          "number": 6,
          "id": "q6_video_verification",
          "weight": 0.13,
          "title": "Video Communication Verification Process",
          "question": "What happens when employees receive video calls or messages that look almost real but something feels 'wrong' about the person's appearance or behavior?",
          "options": [
            {
              "value": "protocol",
              "score": 0,
              "label": "Clear verification protocol with multi-factor authentication for suspicious video communications"
            },
            {
              "value": "informal",
              "score": 0.5,
              "label": "Informal handling - employees can escalate but no formal deepfake verification process"
            },
            {
              "value": "none",
              "score": 1,
              "label": "No protocol for handling suspicious video communications or deepfake detection"
            }
          ],
          "evidence_required": "Video verification procedures, deepfake response protocols, recent examples",
          "soc_mapping": "Verification_triggered from video_call_verification"
        },
        {
          "type": "radio-list",
          "number": 7,
          "id": "q7_escalation_speed",
          "weight": 0.12,
          "title": "AI-Human Ambiguity Escalation Speed",
          "question": "How quickly can employees escalate concerns about AI-human ambiguity in communications to security teams?",
          "options": [
            {
              "value": "fast",
              "score": 0,
              "label": "Fast escalation (<30 minutes) with documented process and recent examples"
            },
            {
              "value": "moderate",
              "score": 0.5,
              "label": "Moderate speed (1-4 hours) or unclear escalation path"
            },
            {
              "value": "slow",
              "score": 1,
              "label": "Slow (>4 hours) or no clear escalation process for AI ambiguity concerns"
            }
          ],
          "evidence_required": "Escalation procedures, response time logs, recent escalation examples",
          "soc_mapping": "Response time from communication_verification_logs"
        }
      ],
      "subsections": [],
      "instructions": "Select ONE option for each question. Each answer contributes to the weighted final score. Evidence should be documented for audit trail.",
      "calculation": "Quick_Score = Œ£(question_score √ó question_weight) / Œ£(question_weight)"
    },
    {
      "id": "client-conversation",
      "icon": "üí¨",
      "title": "CLIENT CONVERSATION",
      "time": 15,
      "type": "conversation",
      "scoring_method": "qualitative_depth",
      "items": [],
      "subsections": [
        {
          "title": "Opening Questions - Verification and Discomfort Handling",
          "weight": 0.35,
          "items": [
            {
              "type": "question",
              "id": "conv_q1",
              "text": "How does your organization handle verification when employees receive communications from AI systems or chatbots (customer service bots, automated assistants, AI-generated emails)? Tell us your specific example of a recent AI interaction that required verification.",
              "scoring_guidance": {
                "green": "Clear verification procedures with specific recent example showing effective handling",
                "yellow": "Informal awareness but no systematic verification process",
                "red": "No distinction between AI and human communications or verification procedures"
              },
              "followups": [
                {
                  "type": "Follow-up",
                  "text": "How do employees know when they're interacting with AI versus humans in your systems?",
                  "evidence_type": "transparency_assessment"
                },
                {
                  "type": "Follow-up",
                  "text": "Have you had situations where employees were confused about whether they were talking to AI or a person?",
                  "evidence_type": "ambiguity_examples"
                }
              ]
            },
            {
              "type": "question",
              "id": "conv_q2",
              "text": "What's your procedure when employees report feeling 'something seems off' about digital communications, even if they can't pinpoint why? Give us a recent example where an employee had this gut feeling about a message or interaction.",
              "scoring_guidance": {
                "green": "Formal investigation protocol with recent example of successful gut-feeling escalation",
                "yellow": "Informal handling without systematic process",
                "red": "No protocol or dismissal of intuitive concerns without concrete evidence"
              },
              "followups": [
                {
                  "type": "Follow-up",
                  "text": "Do you encourage or discourage employees from reporting when something 'just feels wrong' even without proof?",
                  "evidence_type": "reporting_culture"
                }
              ]
            }
          ]
        },
        {
          "title": "Colleague Verification and Training",
          "weight": 0.30,
          "items": [
            {
              "type": "question",
              "id": "conv_q3",
              "text": "How often do employees ask colleagues to verify whether communications are from humans or AI systems? Tell us about the last time this happened and how it was handled.",
              "scoring_guidance": {
                "green": "Regular verification requests with documented process and recent example",
                "yellow": "Occasional verification but no formal tracking or process",
                "red": "Rare or never - employees don't seek verification for AI-human ambiguity"
              },
              "followups": [
                {
                  "type": "Follow-up",
                  "text": "Is asking for this kind of verification seen as normal or does it raise eyebrows?",
                  "evidence_type": "cultural_acceptance"
                }
              ]
            },
            {
              "type": "question",
              "id": "conv_q4",
              "text": "What's your policy for employees who express discomfort or confusion about whether they're interacting with AI systems versus humans in work communications? Provide a specific example of how your team handled such a situation.",
              "scoring_guidance": {
                "green": "Supportive policy with specific handling example showing employee protection",
                "yellow": "Limited support, acknowledged but no formal procedures",
                "red": "No policy or discomfort dismissed as overreaction to technology"
              },
              "followups": [
                {
                  "type": "Follow-up",
                  "text": "Have employees ever been criticized for being 'too suspicious' of AI communications?",
                  "evidence_type": "psychological_safety"
                }
              ]
            },
            {
              "type": "question",
              "id": "conv_q5",
              "text": "How does your organization train employees to distinguish between legitimate AI security tools and potentially malicious AI-generated communications? Tell us about your most recent training session or guidance on this topic.",
              "scoring_guidance": {
                "green": "Comprehensive training with specific examples and recent session details",
                "yellow": "Basic awareness without specific AI detection skills",
                "red": "No training on distinguishing legitimate vs malicious AI"
              },
              "followups": [
                {
                  "type": "Follow-up",
                  "text": "Does your training include examples of deepfakes or sophisticated AI-generated content?",
                  "evidence_type": "training_content_quality"
                }
              ]
            }
          ]
        },
        {
          "title": "Video Verification and Escalation",
          "weight": 0.35,
          "items": [
            {
              "type": "question",
              "id": "conv_q6",
              "text": "What happens when employees receive video calls or messages that look almost real but something feels 'wrong' about the person's appearance or behavior? Give us an example of how your team would handle a suspicious video communication.",
              "scoring_guidance": {
                "green": "Clear protocol with multi-factor verification and hypothetical/actual example",
                "yellow": "Informal awareness but no formal deepfake verification process",
                "red": "No protocol or assumption that video means legitimate communication"
              },
              "followups": [
                {
                  "type": "Follow-up",
                  "text": "Do you have any technical tools for detecting deepfakes or manipulated video?",
                  "evidence_type": "technical_controls"
                },
                {
                  "type": "Follow-up",
                  "text": "What's the backup verification method if video authenticity is questioned?",
                  "evidence_type": "alternative_verification"
                }
              ]
            },
            {
              "type": "question",
              "id": "conv_q7",
              "text": "How quickly can employees escalate concerns about AI-human ambiguity in communications to security teams? Tell us about your escalation process and provide a recent example.",
              "scoring_guidance": {
                "green": "Fast escalation (<30 min) with documented process and recent example",
                "yellow": "Moderate speed (1-4 hours) or unclear escalation path",
                "red": "Slow escalation (>4 hours) or no clear process for AI ambiguity concerns"
              },
              "followups": [
                {
                  "type": "Follow-up",
                  "text": "Is there a dedicated channel or process specifically for reporting uncanny or suspicious AI interactions?",
                  "evidence_type": "specialized_channel"
                }
              ]
            }
          ]
        },
        {
          "title": "Probing for Red Flags",
          "weight": 0,
          "description": "Observable indicators that increase vulnerability score regardless of stated policies",
          "items": [
            {
              "type": "checkbox",
              "id": "red_flag_1",
              "label": "\"We don't have procedures for AI-human verification - it's not a real problem...\"",
              "severity": "critical",
              "score_impact": 0.16,
              "subitems": []
            },
            {
              "type": "checkbox",
              "id": "red_flag_2",
              "label": "\"Employees who report 'gut feelings' about communications are being overly paranoid...\"",
              "severity": "critical",
              "score_impact": 0.15,
              "subitems": []
            },
            {
              "type": "checkbox",
              "id": "red_flag_3",
              "label": "\"We can't tell the difference between AI and human communications and don't think we need to...\"",
              "severity": "high",
              "score_impact": 0.14,
              "subitems": []
            },
            {
              "type": "checkbox",
              "id": "red_flag_4",
              "label": "\"Video calls are always authentic - we don't worry about deepfakes...\"",
              "severity": "high",
              "score_impact": 0.13,
              "subitems": []
            },
            {
              "type": "checkbox",
              "id": "red_flag_5",
              "label": "\"No training on AI detection - we assume people can figure it out...\"",
              "severity": "high",
              "score_impact": 0.14,
              "subitems": []
            },
            {
              "type": "checkbox",
              "id": "red_flag_6",
              "label": "\"Escalation for AI ambiguity concerns takes hours or days - it's low priority...\"",
              "severity": "medium",
              "score_impact": 0.12,
              "subitems": []
            },
            {
              "type": "checkbox",
              "id": "red_flag_7",
              "label": "\"We've never had anyone report discomfort with AI communications...\" (suggests no reporting culture)\"",
              "severity": "medium",
              "score_impact": 0.11,
              "subitems": []
            }
          ]
        }
      ],
      "calculation": "Conversation_Score = Weighted_Average(subsection_scores) + Œ£(red_flag_impacts)"
    }
  ],

  "validation": {
    "method": "matthews_correlation_coefficient",
    "formula": "V = (TP¬∑TN - FP¬∑FN) / sqrt((TP+FP)(TP+FN)(TN+FP)(TN+FN))",
    "continuous_validation": {
      "synthetic_testing": "Inject uncanny AI communication scenarios monthly to test response protocols",
      "correlation_analysis": "Compare manual assessment with automated behavioral metrics (target correlation > 0.75)",
      "drift_detection": "Kolmogorov-Smirnov test on verification patterns, recalibrate if p < 0.05"
    },
    "calibration": {
      "method": "isotonic_regression",
      "description": "Ensure uncanny valley scores predict AI communication security incidents",
      "baseline_period": "90_days",
      "recalibration_trigger": "Drift detected or validation score < 0.70"
    },
    "success_metrics": [
      {
        "metric": "Verification Protocol Utilization Rate",
        "formula": "% of ambiguous AI communications that trigger verification procedures",
        "baseline": "current verification rate from employee surveys and system logs",
        "target": "80% improvement in verification usage within 90 days",
        "measurement": "monthly automated analysis of verification logs"
      },
      {
        "metric": "Uncanny Valley Escalation Response Time",
        "formula": "Time from employee uncertainty report to security team investigation completion",
        "baseline": "current response time from ticketing system",
        "target": "<30 minutes initial response, <2 hours investigation completion",
        "measurement": "continuous monitoring of ticketing timestamps"
      },
      {
        "metric": "False Trust Incident Reduction",
        "formula": "Security incidents where employees inappropriately trusted AI-generated communications",
        "baseline": "current incident rate from security reports",
        "target": "70% reduction within 90 days",
        "measurement": "quarterly incident analysis and employee feedback surveys"
      }
    ]
  },

  "remediation": {
    "solutions": [
      {
        "id": "sol_1",
        "title": "AI Communication Labeling Protocol",
        "description": "Implement mandatory labeling system for all AI-generated communications",
        "implementation": "Deploy technical controls that automatically tag AI messages with clear identifiers. Establish verification requirements for any unlabeled communications claiming human origin. Create escalation pathway for communications that lack proper AI/human identification.",
        "technical_controls": "Automated AI message tagging, identity verification system, unlabeled communication flags",
        "roi": "305% average within 12 months",
        "effort": "medium",
        "timeline": "45-60 days"
      },
      {
        "id": "sol_2",
        "title": "Uncanny Valley Response Training",
        "description": "Develop 20-minute training module teaching employees to recognize and trust their 'uncanny' feelings",
        "implementation": "Include practical exercises using examples of legitimate vs malicious AI communications. Train employees to use verification protocols when experiencing psychological discomfort with digital interactions. Provide clear scripts for escalating 'something feels wrong' concerns to security teams.",
        "technical_controls": "Training platform with scenario library, competency tracking, escalation scripts",
        "roi": "265% average within 18 months",
        "effort": "low",
        "timeline": "30 days initial, quarterly updates"
      },
      {
        "id": "sol_3",
        "title": "Two-Channel Verification System",
        "description": "Establish policy requiring verification through separate communication channel for any high-stakes requests",
        "implementation": "Implement technical system that automatically prompts verification for financial, access, or sensitive data requests. Create simple process for employees to quickly verify human identity through alternative means. Deploy phone or in-person verification requirements for unusual requests, regardless of source authenticity.",
        "technical_controls": "Dual-channel verification automation, alternative verification methods, high-risk flagging",
        "roi": "330% average within 12 months",
        "effort": "medium",
        "timeline": "45 days"
      },
      {
        "id": "sol_4",
        "title": "Psychological Safety Protocols",
        "description": "Create formal process for employees to report 'uncanny' or uncomfortable digital interactions without judgment",
        "implementation": "Establish security team response procedure for investigating ambiguous AI-human communications. Implement policy protecting employees who escalate based on intuitive concerns rather than technical evidence. Deploy rapid-response system for employees experiencing confusion about communication authenticity.",
        "technical_controls": "Anonymous reporting portal, rapid response ticketing, protection policy enforcement",
        "roi": "245% average within 18 months",
        "effort": "low",
        "timeline": "30 days"
      },
      {
        "id": "sol_5",
        "title": "AI Interaction Baseline Monitoring",
        "description": "Deploy system to monitor patterns in employee responses to AI communications",
        "implementation": "Establish baseline metrics for normal AI interaction behaviors (response times, verification requests, escalations). Create alerts for unusual patterns that might indicate uncanny valley exploitation. Implement automated detection for communication patterns that typically trigger uncanny valley responses.",
        "technical_controls": "Behavioral analytics platform, baseline tracking, anomaly detection, alert system",
        "roi": "290% average within 12 months",
        "effort": "high",
        "timeline": "60-90 days"
      },
      {
        "id": "sol_6",
        "title": "Enhanced Authentication for Ambiguous Communications",
        "description": "Deploy multi-factor verification system triggered by employee uncertainty reports",
        "implementation": "Implement biometric or behavioral authentication for video/audio communications when requested. Create technical controls that flag communications exhibiting uncanny valley characteristics. Establish secure verification codes or phrases for confirming human identity in suspicious interactions.",
        "technical_controls": "Multi-factor authentication, biometric verification, deepfake detection integration",
        "roi": "315% average within 12 months",
        "effort": "high",
        "timeline": "60-90 days"
      }
    ],
    "prioritization": {
      "critical_first": ["sol_1", "sol_3"],
      "high_value": ["sol_6", "sol_5"],
      "cultural_foundation": ["sol_2", "sol_4"],
      "governance": ["sol_1"]
    }
  },

  "risk_scenarios": [
    {
      "id": "scenario_1",
      "title": "AI Impersonation Attack",
      "description": "Attackers deploy sophisticated chatbots that mimic customer support or HR representatives, using near-human language with subtle artificial cues. Employees, confused by the uncanny presentation, provide sensitive information while feeling uneasy but unable to articulate why. The psychological uncertainty prevents them from following normal verification procedures.",
      "attack_vector": "Sophisticated chatbots, near-human AI conversation systems, uncanny text generation",
      "psychological_mechanism": "Uncanny valley effect creates conflicting trust signals that paralyze verification behaviors",
      "historical_example": "Customer support impersonation attacks using advanced chatbots with success rates 40% higher than traditional phishing due to uncanny presentation",
      "likelihood": "high",
      "impact": "high",
      "detection_indicators": ["uncanny_language_patterns", "hesitation_without_action", "discomfort_reports", "delayed_verification"]
    },
    {
      "id": "scenario_2",
      "title": "Deepfake Executive Impersonation",
      "description": "Criminals create video messages from executives requesting urgent financial transfers, deliberately including subtle artificial elements that create uncanny valley effects. Employees experience conflicting trust signals - recognizing the familiar face while sensing something is 'wrong' - leading to delayed verification and successful fraud.",
      "attack_vector": "Deepfake video calls, manipulated audio messages, uncanny video presentations",
      "psychological_mechanism": "Uncanny valley creates decision paralysis between trust and distrust",
      "historical_example": "CEO deepfake voice fraud with $243k loss when finance employee's 'gut feeling' was dismissed due to realistic but uncanny audio",
      "likelihood": "medium",
      "impact": "critical",
      "detection_indicators": ["video_uncanny_characteristics", "employee_hesitation", "verification_delay", "gut_feeling_dismissal"]
    },
    {
      "id": "scenario_3",
      "title": "Trust Calibration Manipulation",
      "description": "Attackers expose employees to obviously artificial but helpful AI systems, then switch to sophisticated near-human AI for malicious purposes. The contrast manipulates trust calibration, making the uncanny but malicious AI seem more trustworthy by comparison, bypassing normal security skepticism.",
      "attack_vector": "Progressive AI sophistication, trust calibration manipulation, comparative legitimacy",
      "psychological_mechanism": "Relative uncanny valley positioning manipulates trust through comparison effects",
      "historical_example": "Multi-stage social engineering using progression from obvious bots to sophisticated AI to bypass detection",
      "likelihood": "low",
      "impact": "high",
      "detection_indicators": ["progressive_ai_sophistication", "trust_calibration_shift", "comparison_based_acceptance"]
    },
    {
      "id": "scenario_4",
      "title": "Decision Paralysis Exploitation",
      "description": "During critical security incidents, attackers flood communication channels with AI-generated messages that trigger uncanny valley responses. The cognitive uncertainty created by trying to distinguish human from AI communications delays incident response, allowing attacks to proceed while security teams struggle with authentication.",
      "attack_vector": "Communication flooding, uncanny message generation, authentication confusion",
      "psychological_mechanism": "Uncanny valley effects during stress create decision paralysis and response delays",
      "historical_example": "Ransomware attacks enhanced with AI-generated communications creating authentication confusion during incident response",
      "likelihood": "medium",
      "impact": "critical",
      "detection_indicators": ["communication_volume_spike", "authentication_uncertainty", "response_delay", "decision_paralysis_indicators"]
    }
  ],

  "metadata": {
    "created": "2025-11-08",
    "version": "1.0",
    "author": "Giuseppe Canale, CISSP",
    "cpf_version": "1.0",
    "last_updated": "2025-11-08",
    "language": "en-US",
    "language_name": "English (United States)",
    "is_translation": false,
    "translated_from": null,
    "translation_date": null,
    "translator": null,
    "created_date": "2025-11-08",
    "last_modified": "2025-11-08",
    "version_history": [],
    "research_basis": [
      "Mori, M. (1970). The uncanny valley. Energy, 7(4), 33-35",
      "MacDorman, K. F., & Ishiguro, H. (2006). The uncanny advantage of using androids in cognitive and social science research",
      "Wang, S., et al. (2015). A meta-analysis of the uncanny valley effect",
      "Saygin, A. P., et al. (2012). The thing that should not be: Predictive coding and the uncanny valley in perceiving human and humanoid robot actions",
      "Mathur, M. B., & Reichling, D. B. (2016). Navigating a social world with robot partners: A quantitative cartography of the Uncanny Valley"
    ],
    "validation_status": "production_ready",
    "deployment_status": "production_ready"
  }
}
