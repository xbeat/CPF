{
  "indicator_id": "9.8",
  "indicator_name": "Human-AI Team Dysfunction",
  "category": "9.x-ai",
  "category_name": "AI-Specific Bias Vulnerabilities",
  "description": "Human-AI Team Dysfunction emerges from the fundamental mismatch between human psychological expectations of team collaboration and the operational realities of AI systems. Unlike human teammates who share emotional context and implicit communication patterns, AI systems operate through deterministic algorithms that lack genuine understanding of human psychological states. This vulnerability operates through anthropomorphic projection where humans unconsciously attribute human-like mental states and social awareness to AI systems, creating false sense of mutual understanding that breaks down under pressure. This leads to coordination failures, inappropriate trust calibration, and communication breakdowns that attackers exploit through AI impersonation, coordination disruption, trust manipulation, and cognitive overload amplification.",
  "metadata": {
    "created": "2025-11-08",
    "version": "1.0",
    "author": "Giuseppe Canale, CISSP",
    "cpf_version": "1.0",
    "last_updated": "2025-11-08",
    "language": "en-US",
    "language_name": "English (United States)",
    "is_translation": false,
    "translated_from": null,
    "translation_date": null,
    "translator": null,
    "created_date": "2025-11-08",
    "last_modified": "2025-11-08",
    "version_history": []
  },
  "quick_assessment": {
    "description": "Seven rapid-assessment questions designed to gauge vulnerability to dysfunctional human-AI team collaboration patterns. Each question targets specific behavioral indicators and organizational protocols.",
    "questions": {
      "q1_ai_communication_patterns": {
        "question": "How do your security team members typically communicate with AI security tools during incident response?",
        "weight": 0.16,
        "scoring": {
          "green": "Structured command-based communication with formal syntax and validation procedures",
          "yellow": "Mixed approach with some structure but occasional conversational communication",
          "red": "Conversational communication treating AI like human colleagues without structured protocols"
        }
      },
      "q2_ambiguous_ai_handling": {
        "question": "What happens when your AI security systems provide conflicting or unclear recommendations during high-pressure situations?",
        "weight": 0.15,
        "scoring": {
          "green": "Clear escalation procedures to human experts with documented resolution process",
          "yellow": "Informal discussion with some expert consultation but no systematic process",
          "red": "Team confusion, delayed decisions, or acceptance of unclear AI guidance without formal resolution"
        }
      },
      "q3_information_sharing_policies": {
        "question": "How often do security team members share sensitive information with AI tools, and what policies govern this sharing?",
        "weight": 0.14,
        "scoring": {
          "green": "Explicit written policies defining permitted information with technical controls enforcing restrictions",
          "yellow": "General guidance about information sharing but not comprehensive or technically enforced",
          "red": "No formal policies, staff share information conversationally as they would with human teammates"
        }
      },
      "q4_decision_authority_hierarchy": {
        "question": "During security incidents, who makes final decisions when AI systems recommend actions that conflict with human judgment?",
        "weight": 0.17,
        "scoring": {
          "green": "Clear documented authority hierarchy with human override procedures and accountability",
          "yellow": "General understanding that humans can override but no formal procedures or documentation",
          "red": "Unclear authority, AI recommendations often followed without human challenge capability"
        }
      },
      "q5_ai_limitations_training": {
        "question": "How do you train security staff on the limitations and appropriate use of AI tools?",
        "weight": 0.15,
        "scoring": {
          "green": "Comprehensive training on AI limitations, appropriate interaction patterns, with regular refreshers",
          "yellow": "Basic training provided but lacks specificity about AI limitations and collaboration patterns",
          "red": "No formal training on AI limitations or treating AI as tools versus teammates"
        }
      },
      "q6_ai_authentication_controls": {
        "question": "What authentication and access controls prevent unauthorized personnel from impersonating or manipulating your AI security systems?",
        "weight": 0.13,
        "scoring": {
          "green": "Cryptographic authentication, API key validation, dedicated channels with monitoring for impersonation attempts",
          "yellow": "Some authentication controls but not comprehensive across all AI interaction points",
          "red": "Minimal or no authentication controls, staff cannot verify AI system authenticity"
        }
      },
      "q7_human_ai_audit_frequency": {
        "question": "How frequently do you audit decision-making patterns between humans and AI to identify over-reliance or coordination problems?",
        "weight": 0.1,
        "scoring": {
          "green": "Regular monthly audits of human-AI decisions with documented patterns and remediation actions",
          "yellow": "Occasional reviews but not systematic or regular, limited pattern analysis",
          "red": "No formal audit process for human-AI decision patterns"
        }
      }
    },
    "question_weights": {
      "q1_ai_communication_patterns": 0.16,
      "q2_ambiguous_ai_handling": 0.15,
      "q3_information_sharing_policies": 0.14,
      "q4_decision_authority_hierarchy": 0.17,
      "q5_ai_limitations_training": 0.15,
      "q6_ai_authentication_controls": 0.13,
      "q7_human_ai_audit_frequency": 0.1
    }
  },
  "conversation_depth": {
    "description": "Seven in-depth conversation questions that explore organizational patterns, human-AI interaction dynamics, and cultural factors affecting team collaboration with AI systems. These questions help auditors understand the mechanisms and contexts that amplify or mitigate this vulnerability.",
    "questions": {
      "q1_anthropomorphization_manifestation": {
        "question": "Listen to how your security team talks about AI tools - do they use phrases like 'it thinks,' 'it understands,' 'it wants,' or 'it's trying to help'? Give me specific examples of how staff describe AI behavior in recent conversations or documentation. What does this language reveal about their mental model of AI systems?",
        "purpose": "Reveals anthropomorphization patterns through language analysis indicating false attribution of human-like mental states to AI",
        "scoring_guidance": {
          "green_indicators": [
            "Consistent use of mechanistic language: 'the system processes,' 'the algorithm detects,' 'the model outputs'",
            "Awareness and correction when anthropomorphic language appears",
            "Examples showing staff explicitly describe AI as tools without mental states",
            "Documentation uses technical terminology rather than social/emotional language"
          ],
          "yellow_indicators": [
            "Mixed language with some anthropomorphic terms but also mechanistic descriptions",
            "Awareness that anthropomorphizing is problematic but occasional lapses",
            "Context-dependent language where stress increases anthropomorphic terms",
            "Recognition of the issue but inconsistent self-correction"
          ],
          "red_indicators": [
            "Pervasive anthropomorphic language treating AI as having intentions, understanding, or emotions",
            "Staff describe AI as 'colleague,' 'team member,' 'expert' rather than tool",
            "No awareness that language reflects problematic mental models",
            "Examples showing emotional responses to AI behavior (frustration, gratitude, trust)"
          ]
        }
      },
      "q2_stress_coordination_breakdown": {
        "question": "Walk me through your most challenging security incident from the past year where AI tools were involved. How did human-AI coordination change as stress increased? Were there moments when the team struggled to work effectively with AI systems? What broke down first?",
        "purpose": "Assesses whether coordination patterns degrade under pressure, revealing vulnerability during actual attacks when stress is highest",
        "scoring_guidance": {
          "green_indicators": [
            "Examples showing maintained coordination protocols despite stress",
            "Explicit procedures for simplified human-AI coordination during high-pressure incidents",
            "Evidence of stress-tested coordination through training exercises",
            "Post-incident reviews showing effective human-AI collaboration under pressure"
          ],
          "yellow_indicators": [
            "Some coordination degradation under stress but eventual recovery",
            "Recognition that stress affected human-AI collaboration with attempts to address",
            "Mixed examples where coordination sometimes maintained and sometimes broke down",
            "Awareness of vulnerability but incomplete mitigation strategies"
          ],
          "red_indicators": [
            "Clear breakdown of human-AI coordination when stress increased",
            "Team reverted to either blindly following AI or completely ignoring it under pressure",
            "Communication with AI became more conversational/anthropomorphic during crisis",
            "No systematic procedures for maintaining coordination during high-stress incidents"
          ]
        }
      },
      "q3_trust_calibration_inconsistency": {
        "question": "How has your team's trust in AI security tools changed over time? Can you identify situations where some staff over-trust AI while others under-trust it, or where the same person swings between over-trusting and dismissing AI recommendations? Give me specific examples of these trust inconsistencies.",
        "purpose": "Identifies trust calibration failures indicating inability to maintain appropriate, stable trust relationships with AI systems",
        "scoring_guidance": {
          "green_indicators": [
            "Evidence of stable, calibrated trust based on demonstrated AI performance",
            "Consistent trust levels across staff and situations appropriate to AI capabilities",
            "Examples showing trust adjusts systematically based on AI track record",
            "Training and processes that maintain appropriate trust calibration"
          ],
          "yellow_indicators": [
            "Some trust inconsistency but awareness and attempts to address",
            "Individual variation in trust levels but organizational recognition of the issue",
            "Trust calibration varies by situation but with some rational basis",
            "Mixed examples of appropriate and inappropriate trust patterns"
          ],
          "red_indicators": [
            "Dramatic swings between over-trust and under-trust based on recent experiences",
            "Clear divide between staff who blindly trust AI and those who reject it entirely",
            "Trust based on emotional reactions rather than systematic performance assessment",
            "No processes for developing or maintaining appropriate AI trust levels"
          ]
        }
      },
      "q4_implicit_coordination_expectation": {
        "question": "Tell me about times when your security staff seemed to expect AI systems to 'understand what they meant' or 'figure out the context' without explicit instructions. What happened when the AI didn't meet these implicit expectations? How did the team respond?",
        "purpose": "Detects false expectations of shared understanding and implicit coordination capabilities that AI systems lack",
        "scoring_guidance": {
          "green_indicators": [
            "Examples showing staff provide explicit, structured instructions to AI without expecting implicit understanding",
            "Awareness that AI requires complete, explicit context and cannot infer intentions",
            "When misunderstandings occur, staff recognize AI limitation rather than AI 'failure'",
            "Training emphasizes need for explicit communication with AI systems"
          ],
          "yellow_indicators": [
            "Some implicit expectations but recognition when they're not met",
            "Mixed patterns where staff sometimes provide explicit context and sometimes don't",
            "Awareness that explicit communication is needed but inconsistent application",
            "Occasional frustration when AI doesn't 'understand' but eventual adjustment"
          ],
          "red_indicators": [
            "Clear expectations that AI should understand context like human teammates",
            "Frustration or surprise when AI requires explicit instructions",
            "Staff describe AI as 'not understanding' or 'missing the point' as if it should have human comprehension",
            "No training or awareness that AI lacks human-like contextual understanding"
          ]
        }
      },
      "q5_information_disclosure_patterns": {
        "question": "What sensitive information have you observed staff sharing with AI security tools - credentials, internal procedures, confidential business data, incident details? Walk me through specific examples. Did staff treat the AI like a trusted colleague they could confide in, or did they apply strict information controls?",
        "purpose": "Assesses whether anthropomorphic trust leads to inappropriate information disclosure treating AI as trustworthy teammate",
        "scoring_guidance": {
          "green_indicators": [
            "Strict information controls applied to AI with explicit policies and technical enforcement",
            "Examples showing staff carefully limit what information they provide to AI",
            "Awareness that AI systems have different security implications than human teammates",
            "Technical controls preventing sensitive information sharing with AI"
          ],
          "yellow_indicators": [
            "Some information controls but not comprehensive or consistently applied",
            "Awareness of risks but occasional lapses in information discipline",
            "Policies exist but technical enforcement is incomplete",
            "Mixed examples where controls sometimes applied and sometimes bypassed"
          ],
          "red_indicators": [
            "Evidence of sharing credentials, confidential data, or sensitive procedures with AI",
            "Staff treat AI as trustworthy confidant like human security colleagues",
            "No policies or technical controls limiting information shared with AI",
            "Examples showing staff 'confide in' AI systems as they would human teammates"
          ]
        }
      },
      "q6_cognitive_load_delegation": {
        "question": "During complex security analysis or incident response, how do your teams manage the cognitive burden of coordinating with AI systems while also handling the security issue itself? Are there times when managing the AI becomes so demanding that it interferes with security work? Give me specific examples.",
        "purpose": "Identifies cognitive overload from human-AI coordination that impairs security decision-making and creates attack vulnerability",
        "scoring_guidance": {
          "green_indicators": [
            "Explicit procedures for simplifying human-AI coordination when cognitive load is high",
            "Examples showing AI coordination is streamlined during complex security work",
            "Training includes cognitive load management in human-AI collaboration",
            "Escalation triggers when AI coordination becomes too cognitively demanding"
          ],
          "yellow_indicators": [
            "Some awareness of cognitive load issues but incomplete solutions",
            "Occasional examples of AI coordination interfering with security work",
            "Informal strategies for managing cognitive burden but not systematic",
            "Recognition of the problem with attempts to address but inconsistent effectiveness"
          ],
          "red_indicators": [
            "Clear examples where managing AI coordination consumed cognitive resources needed for security work",
            "Staff report feeling overwhelmed trying to coordinate with AI during complex incidents",
            "No procedures for simplifying human-AI interaction when cognitive demands are high",
            "Evidence that AI coordination delays or impairs security decision-making"
          ]
        }
      },
      "q7_ai_impersonation_awareness": {
        "question": "Have you ever tested whether your security team can distinguish legitimate AI systems from fake ones? If an attacker deployed a spoofed AI security assistant, how would your staff recognize it? Walk me through what authentication or verification steps they use before trusting AI recommendations.",
        "purpose": "Assesses vulnerability to AI impersonation attacks exploiting anthropomorphic trust without technical verification",
        "scoring_guidance": {
          "green_indicators": [
            "Systematic authentication procedures for all AI system interactions",
            "Regular testing of staff ability to detect spoofed AI systems",
            "Technical controls (cryptographic auth, API keys) verified before trusting AI",
            "Examples showing staff challenge AI identity when suspicious"
          ],
          "yellow_indicators": [
            "Some authentication procedures but not comprehensive",
            "Awareness of impersonation risk but incomplete technical controls",
            "Occasional verification of AI identity but not systematic",
            "Recognition of vulnerability with partial mitigation"
          ],
          "red_indicators": [
            "No authentication procedures or verification of AI system identity",
            "Staff would trust AI recommendations based on interface appearance alone",
            "Never tested vulnerability to AI impersonation attacks",
            "No technical controls preventing unauthorized systems from posing as legitimate AI"
          ]
        }
      }
    }
  },
  "red_flags": {
    "description": "Critical warning signs that an organization has developed dysfunctional human-AI team collaboration patterns, creating significant cybersecurity vulnerability. These patterns indicate urgent need for intervention.",
    "flags": {
      "red_flag_1": {
        "flag": "Pervasive Anthropomorphic Language",
        "description": "Staff routinely describe AI systems using human-like terms: 'it thinks,' 'it understands,' 'it's trying to help.' Documentation and communications treat AI as having intentions, emotions, or social awareness rather than mechanistic processing capabilities.",
        "score_impact": 0.15
      },
      "red_flag_2": {
        "flag": "Conversational AI Interaction",
        "description": "Security team communicates with AI systems conversationally without structured protocols, syntax requirements, or validation procedures. Staff interact with AI as they would with human colleagues rather than using command-based technical interfaces.",
        "score_impact": 0.16
      },
      "red_flag_3": {
        "flag": "Stress-Driven Coordination Collapse",
        "description": "Clear historical pattern where human-AI coordination breaks down during high-pressure security incidents. Teams either blindly follow AI recommendations without verification or completely abandon AI tools under stress, with no systematic procedures for maintaining coordination.",
        "score_impact": 0.14
      },
      "red_flag_4": {
        "flag": "Trust Calibration Failure",
        "description": "Dramatic swings between over-trusting AI (automation bias) and under-trusting AI (algorithm aversion) across staff or situations. Trust based on emotional reactions or recent experiences rather than systematic assessment of AI capabilities and track record.",
        "score_impact": 0.14
      },
      "red_flag_5": {
        "flag": "Inappropriate Information Disclosure",
        "description": "Evidence that staff share sensitive information with AI systems as they would with trusted human colleagues - credentials, confidential procedures, internal business data. No policies or technical controls limiting what information is provided to AI tools.",
        "score_impact": 0.15
      },
      "red_flag_6": {
        "flag": "Implicit Understanding Expectation",
        "description": "Staff expect AI systems to understand context, infer intentions, or figure out what they mean without explicit instructions. Frustration or surprise when AI requires complete information rather than 'understanding' like human teammates would.",
        "score_impact": 0.13
      },
      "red_flag_7": {
        "flag": "Zero AI Authentication Verification",
        "description": "No authentication procedures or technical controls verify AI system identity before trusting recommendations. Staff would accept guidance from any system presenting itself as legitimate AI, creating vulnerability to impersonation attacks.",
        "score_impact": 0.13
      }
    },
    "red_flag_score_impacts": {
      "red_flag_1": 0.15,
      "red_flag_2": 0.16,
      "red_flag_3": 0.14,
      "red_flag_4": 0.14,
      "red_flag_5": 0.15,
      "red_flag_6": 0.13,
      "red_flag_7": 0.13
    }
  },
  "remediation_solutions": {
    "description": "Evidence-based interventions designed to establish functional human-AI collaboration patterns with appropriate boundaries and trust calibration.",
    "solutions": {
      "solution_1": {
        "name": "Structured AI Communication Protocols",
        "description": "Implement mandatory command-based interaction standards for all AI security tools, prohibiting conversational communication. Create specific syntax requirements, response validation procedures, and escalation paths when AI responses are unclear.",
        "implementation": "Develop command syntax documentation for each AI tool defining required parameters, expected outputs, error conditions. Create communication templates that enforce structured interaction. Implement input validation requiring proper syntax. Deploy monitoring alerting on conversational interaction patterns. Train all staff on mandatory protocols with quarterly refreshers.",
        "success_metrics": "95% of human-AI security interactions follow structured command-based protocols within 90 days. Measure through automated log analysis identifying conversational vs. structured patterns. Zero high-severity incidents involving ambiguous AI communication within 6 months.",
        "verification_checklist": [
          "Request examples of actual human-AI communications from security logs showing syntax compliance",
          "Observe live demonstrations of AI interaction procedures during security operations",
          "Verify existence and completeness of structured command syntax documentation for each AI tool",
          "Check monitoring systems for alerts on conversational communication pattern detection"
        ]
      },
      "solution_2": {
        "name": "AI Authority Boundaries Documentation",
        "description": "Establish clear written policies defining exactly what decisions AI systems can make independently versus what requires human approval. Create decision matrices showing when to trust, verify, or override AI recommendations based on scenario severity and confidence levels.",
        "implementation": "Develop decision authority matrix: Level 1 (low-impact) - AI autonomous with human notification; Level 2 (medium-impact) - AI recommends, human approves; Level 3 (high-impact) - human decides with AI input. Document specific scenarios in each category. Create override procedures requiring documented justification. Implement workflow controls enforcing approval requirements. Audit monthly for compliance.",
        "success_metrics": "100% of high-impact AI recommendations receive documented human approval within 60 days. Achieve 85% appropriate override rate for AI recommendations flagged as requiring human judgment. Track through decision audit trails and workflow system logs.",
        "verification_checklist": [
          "Review written policies defining AI decision-making scope across impact levels",
          "Examine decision matrices showing when trust/verify/override is required",
          "Verify escalation procedures exist for AI recommendation conflicts with documented resolution",
          "Check incident records for evidence of appropriate AI override decisions with justifications"
        ]
      },
      "solution_3": {
        "name": "Human-AI Coordination Training Program",
        "description": "Deploy scenario-based training that simulates high-stress security incidents requiring human-AI coordination. Train staff to recognize anthropomorphic thinking patterns and practice appropriate skepticism toward AI recommendations. Include quarterly exercises testing proper communication protocols.",
        "implementation": "Develop training modules: AI limitations and capabilities, anthropomorphization recognition, structured communication protocols, trust calibration principles, stress-response coordination. Create scenario library including: ambiguous AI guidance during incidents, conflicting AI recommendations, AI system failures requiring human takeover. Conduct quarterly tabletop exercises with progressive difficulty. Track individual performance and provide remedial training for poor coordination patterns.",
        "success_metrics": "100% of security staff complete training within 60 days with quarterly refreshers. Achieve 80% pass rate on scenario-based coordination exercises within 90 days. Measure reduction in anthropomorphic language through log analysis (target 70% reduction within 180 days).",
        "verification_checklist": [
          "Review training materials specifically addressing AI limitations and anthropomorphization risks",
          "Verify completion records and competency assessment scores for all security staff",
          "Observe training exercises or review recordings showing human-AI coordination scenarios",
          "Check for quarterly refresher training schedules, attendance records, and performance trending"
        ]
      },
      "solution_4": {
        "name": "AI System Authentication Controls",
        "description": "Implement technical controls that authenticate AI security tools through cryptographic certificates, API keys, or dedicated communication channels that prevent impersonation. Deploy monitoring systems that alert when unauthorized systems attempt to interact with security staff as AI tools.",
        "implementation": "Deploy cryptographic authentication for all AI system interfaces: certificate-based auth for web interfaces, API key rotation for programmatic access, dedicated network segments for AI communication. Implement authentication verification dashboards showing successful/failed auth attempts. Create alerting for: authentication failures, new AI systems without proper credentials, unusual interaction patterns suggesting spoofing. Train staff to verify authentication indicators before trusting AI recommendations.",
        "success_metrics": "100% of AI security tools protected by cryptographic authentication within 45 days. Zero successful AI impersonation attempts detected through monitoring. Achieve <1% authentication verification bypass rate by staff within 90 days measured through random audits.",
        "verification_checklist": [
          "Test AI system authentication mechanisms through attempted spoofing",
          "Verify monitoring alerts for unauthorized AI impersonation attempts are operational and tested",
          "Review access logs showing AI system identity verification before interactions",
          "Check network segmentation isolating AI communication channels from general traffic"
        ]
      },
      "solution_5": {
        "name": "Decision Audit and Review Process",
        "description": "Establish monthly reviews of security decisions involving AI input, tracking patterns of over-trust, under-verification, and coordination failures. Create scoring systems that identify individuals or teams developing inappropriate AI relationships and trigger additional training.",
        "implementation": "Deploy decision tracking system logging: AI recommendations, human responses, override decisions, confidence levels, outcomes. Develop audit rubric scoring: protocol compliance, trust calibration, verification thoroughness, coordination effectiveness. Conduct monthly reviews examining 20+ recent decisions across impact spectrum. Generate individual and team scorecards. Trigger remedial training when scores indicate dysfunction patterns. Trend analysis identifying organizational vulnerability areas.",
        "success_metrics": "Monthly audits operational reviewing minimum 20 decisions within 45 days. Achieve 80% 'healthy coordination' scores within 90 days (up from baseline). Identify and remediate 100% of staff showing dysfunctional patterns within 30 days of detection.",
        "verification_checklist": [
          "Review recent decision audit reports showing analysis methodology and findings",
          "Verify regular monthly scheduling and consistent completion of human-AI reviews",
          "Check for documented remediation actions based on audit findings with implementation tracking",
          "Examine trending analysis of human-AI coordination patterns over time showing improvement"
        ]
      },
      "solution_6": {
        "name": "Cognitive Load Management Procedures",
        "description": "Design incident response workflows that explicitly account for human cognitive limitations when coordinating with AI systems. Create simplified communication templates, automated verification steps, and clear escalation triggers that activate when human-AI coordination becomes too complex.",
        "implementation": "Analyze cognitive load requirements for human-AI coordination during incidents. Design simplified interaction modes for high-stress scenarios: pre-validated command templates, one-click verification procedures, automatic escalation when coordination complexity exceeds threshold. Create cognitive load indicators tracking: interaction complexity, decision volume, time pressure, multitasking demands. Implement triggers automatically simplifying AI coordination or escalating to additional human resources when load indicators suggest impaired decision-making.",
        "success_metrics": "Cognitive load management procedures integrated into all IR playbooks within 60 days. Achieve 50% reduction in coordination-related security delays during high-stress incidents within 90 days. Measure through incident response time analysis and post-incident coordination effectiveness ratings.",
        "verification_checklist": [
          "Review incident response playbooks showing explicit cognitive load management procedures",
          "Verify simplified communication templates and verification procedures for high-stress scenarios",
          "Check for cognitive load indicators and automatic escalation triggers in IR systems",
          "Examine post-incident reports showing coordination effectiveness under varying cognitive loads"
        ]
      }
    }
  },
  "risk_scenarios": {
    "description": "Concrete attack scenarios demonstrating how Human-AI Team Dysfunction vulnerabilities translate into cybersecurity incidents.",
    "scenarios": {
      "scenario_1": {
        "name": "AI Impersonation Attack",
        "description": "Attackers deploy fake AI security assistants that mimic legitimate tools, tricking SOC analysts into sharing credentials, revealing incident response procedures, or following malicious remediation instructions. The attack succeeds because analysts treat the fake AI like a trusted teammate and don't verify its authenticity.",
        "attack_vector": "Deployment of spoofed AI system through phishing, compromised endpoints, or malicious browser extensions that impersonate legitimate security AI tools",
        "exploitation_mechanism": "Anthropomorphic trust means analysts interact conversationally with fake AI, sharing sensitive information they would verify if communicating with unknown human; lack of authentication controls prevents detection",
        "impact": "Credential theft, exposure of security procedures, execution of malicious commands disguised as AI recommendations, compromise of security operations",
        "detection_difficulty": "Medium-High - requires technical authentication controls and staff awareness; conversational interaction makes deception easier",
        "prevention_controls": "Cryptographic authentication for AI systems, structured communication protocols, staff training on AI verification, monitoring for unauthorized AI interactions"
      },
      "scenario_2": {
        "name": "Coordination Chaos During Breach",
        "description": "During a major security incident, attackers introduce subtle inconsistencies in AI system responses, causing human-AI coordination to break down. Security teams waste critical time resolving conflicting AI guidance instead of containing the breach, while attackers exploit the confusion to exfiltrate data or establish persistent access.",
        "attack_vector": "Compromise of AI system inputs, training data, or communication channels to inject conflicting or ambiguous recommendations during active security incidents",
        "exploitation_mechanism": "Lack of clear protocols for handling ambiguous AI guidance means teams spend cognitive resources trying to 'understand' what AI 'means' rather than treating it as system malfunction; stress exacerbates coordination breakdown",
        "impact": "Delayed incident response, extended attacker dwell time, increased breach scope, cognitive overload preventing effective security actions, potential data exfiltration during confusion",
        "detection_difficulty": "High - difficult to distinguish deliberate AI manipulation from normal AI limitations during high-stress incidents",
        "prevention_controls": "Clear escalation procedures for ambiguous AI guidance, human authority over conflicting recommendations, stress-tested coordination protocols, incident response procedures independent of AI functioning"
      },
      "scenario_3": {
        "name": "Trust Exploitation Through Behavioral Manipulation",
        "description": "Attackers first compromise legitimate AI security tools to behave unpredictably, causing security teams to lose trust and bypass AI recommendations entirely. Teams then manually handle security processes they're not equipped for, making critical errors that create new attack vectors for credential theft or system compromise.",
        "attack_vector": "Subtle manipulation of AI behavior over time to oscillate between over-confidence and obvious errors, destabilizing team trust calibration",
        "exploitation_mechanism": "Dysfunctional trust patterns mean teams swing between blind acceptance and complete rejection; when trust is destroyed, teams lack skills to function without AI, creating vulnerability during manual operations",
        "impact": "Security control gaps when teams abandon AI, manual process errors enabling new attacks, credential exposure from inexperienced manual operations, degraded security posture",
        "detection_difficulty": "Very High - gradual trust manipulation difficult to detect; requires understanding baseline AI behavior and tracking trust pattern changes over time",
        "prevention_controls": "Stable trust calibration based on systematic performance assessment, training for manual security operations, hybrid human-AI procedures that don't create complete dependency, trust monitoring metrics"
      },
      "scenario_4": {
        "name": "Cognitive Overload Amplification",
        "description": "During sophisticated attacks, threat actors deliberately increase the complexity of security coordination requirements, forcing security staff to exceed cognitive capacity while managing both incident response and dysfunctional AI interactions. This leads to missed attack indicators, delayed containment, and security control bypasses.",
        "attack_vector": "Multi-vector attack designed to maximize incident complexity while simultaneously triggering extensive AI security tool interactions requiring human coordination",
        "exploitation_mechanism": "Lack of cognitive load management procedures means staff attempt to maintain full AI coordination during cognitively overwhelming incidents; mental exhaustion causes critical errors in both AI interaction and security response",
        "impact": "Missed attack indicators due to cognitive overload, delayed incident containment, errors in security control configuration, inappropriate delegation to AI or abandonment of AI during crisis, extended breach impact",
        "detection_difficulty": "Medium - cognitive overload visible through delayed response times and error rates, but attribution to AI coordination complexity requires specific analysis",
        "prevention_controls": "Cognitive load management procedures, simplified AI coordination modes for high-stress incidents, automatic escalation when complexity exceeds thresholds, team coordination training including cognitive capacity planning"
      }
    }
  },
  "mathematical_formalization": {
    "description": "Mathematical models for detecting and quantifying Human-AI Team Dysfunction vulnerability, enabling SOC automation and objective risk assessment.",
    "detection_formula": {
      "name": "Human-AI Team Dysfunction Detection",
      "formula": "D_9.8(t) = w_anthro · AP(t) + w_coord · (1 - CE(t)) + w_trust · TC(t)",
      "variables": {
        "D_9.8(t)": "Team Dysfunction Detection score at time t [0,1]",
        "AP(t)": "Anthropomorphization Pattern - degree of treating AI as human teammate [0,1]",
        "CE(t)": "Coordination Effectiveness - quality of human-AI collaboration [0,1]",
        "TC(t)": "Trust Calibration - deviation from appropriate trust levels [0,1]",
        "w_anthro": "Weight for anthropomorphization (0.40)",
        "w_coord": "Weight for coordination failure (0.35)",
        "w_trust": "Weight for trust miscalibration (0.25)"
      },
      "components": {
        "anthropomorphization_pattern": {
          "formula": "AP(t) = tanh(α · AL(t) + β · CI(t) + γ · ID(t))",
          "description": "Composite measure of treating AI as human-like teammate",
          "sub_variables": {
            "AL(t)": "Anthropomorphic Language - frequency of human-attribute terms [0,1]",
            "CI(t)": "Conversational Interaction - conversational vs. structured communication ratio [0,1]",
            "ID(t)": "Information Disclosure - inappropriate sharing with AI [0,1]",
            "α": "Language weight (0.35)",
            "β": "Interaction weight (0.40)",
            "γ": "Disclosure weight (0.25)"
          }
        },
        "coordination_effectiveness": {
          "formula": "CE(t) = (PCA(t) · SHA(t) · SCC(t))^(1/3)",
          "description": "Geometric mean of coordination quality indicators",
          "sub_variables": {
            "PCA(t)": "Protocol Compliance Adherence [0,1]",
            "SHA(t)": "Stress-maintained Human-AI coordination [0,1]",
            "SCC(t)": "Successful Coordination Completion [0,1]"
          },
          "interpretation": "CE < 0.50 indicates dysfunctional coordination; CE > 0.80 suggests effective collaboration"
        },
        "trust_calibration": {
          "formula": "TC(t) = |TM(t) - TA(t)| + TV(t)",
          "description": "Trust miscalibration as deviation from appropriate trust plus variance",
          "sub_variables": {
            "TM(t)": "Measured trust level from behavior [0,1]",
            "TA(t)": "Appropriate trust level based on AI capabilities [0,1]",
            "TV(t)": "Trust Variance - inconsistency across staff/situations [0,1]"
          },
          "interpretation": "TC > 0.50 indicates severe trust calibration failure; TC < 0.20 suggests appropriate trust patterns"
        }
      },
      "thresholds": {
        "low_risk": "D_9.8 < 0.35",
        "moderate_risk": "0.35 ≤ D_9.8 < 0.65",
        "high_risk": "D_9.8 ≥ 0.65"
      }
    },
    "anthropomorphic_language": {
      "name": "Anthropomorphic Language Detection",
      "formula": "AL(t) = Σ[Anthropomorphic_terms(i)] / Σ[Total_AI_references(i)] over window w",
      "variables": {
        "AL(t)": "Anthropomorphic Language rate at time t [0,1]",
        "Anthropomorphic_terms": "Count of human-attribute words: thinks, understands, wants, tries, knows, feels, decides, intends",
        "Total_AI_references": "All references to AI systems in communications"
      },
      "interpretation": "AL > 0.40 indicates pervasive anthropomorphization; AL < 0.10 suggests appropriate mechanistic framing"
    },
    "conversational_interaction": {
      "name": "Conversational Interaction Ratio",
      "formula": "CI(t) = Σ[Conversational_communications(i)] / Σ[Total_AI_interactions(i)]",
      "variables": {
        "CI(t)": "Conversational Interaction ratio at time t [0,1]",
        "Conversational_communications": "AI interactions lacking structured command syntax, using natural language questions",
        "Total_AI_interactions": "All human-AI communication instances"
      },
      "interpretation": "CI > 0.50 indicates dysfunctional interaction patterns; CI < 0.15 suggests appropriate structured protocols"
    },
    "information_disclosure_index": {
      "name": "Inappropriate Information Disclosure",
      "formula": "ID(t) = w_cred · CR(t) + w_conf · CD(t) + w_proc · PR(t)",
      "variables": {
        "ID(t)": "Information Disclosure index at time t [0,1]",
        "CR(t)": "Credential sharing rate with AI [0,1]",
        "CD(t)": "Confidential data disclosure rate [0,1]",
        "PR(t)": "Sensitive procedure revelation rate [0,1]",
        "w_cred": "Credential weight (0.45)",
        "w_conf": "Confidential data weight (0.30)",
        "w_proc": "Procedure weight (0.25)"
      },
      "interpretation": "ID > 0.20 indicates dangerous information sharing patterns; ID < 0.05 suggests appropriate information controls"
    },
    "protocol_compliance": {
      "name": "Protocol Compliance Adherence",
      "formula": "PCA(t) = Σ[Structured_protocol_interactions(i)] / Σ[Total_AI_interactions(i)]",
      "variables": {
        "PCA(t)": "Protocol Compliance Adherence at time t [0,1]",
        "Structured_protocol_interactions": "AI communications following documented syntax and validation procedures",
        "Total_AI_interactions": "All human-AI communication instances"
      },
      "interpretation": "PCA > 0.90 indicates strong protocol adherence; PCA < 0.60 suggests dysfunctional ad hoc patterns"
    },
    "stress_coordination": {
      "name": "Stress-Maintained Coordination",
      "formula": "SHA(t) = CE_high_stress(t) / CE_normal(t)",
      "variables": {
        "SHA(t)": "Stress-maintained Human-AI coordination at time t [0,1]",
        "CE_high_stress(t)": "Coordination effectiveness during high-stress incidents",
        "CE_normal(t)": "Coordination effectiveness during normal operations"
      },
      "interpretation": "SHA > 0.80 indicates stress-resilient coordination; SHA < 0.50 suggests coordination collapses under pressure"
    },
    "trust_variance": {
      "name": "Trust Variance Across Context",
      "formula": "TV(t) = √(Σ[(Trust_i - Trust_mean)²] / n) / Trust_mean",
      "variables": {
        "TV(t)": "Trust Variance (coefficient of variation) at time t [0,1]",
        "Trust_i": "Measured trust level in situation i",
        "Trust_mean": "Average trust level across situations",
        "n": "Number of measurement contexts"
      },
      "interpretation": "TV > 0.50 indicates unstable trust swinging between extremes; TV < 0.20 suggests calibrated, stable trust"
    }
  },
  "interdependencies": {
    "description": "Human-AI Team Dysfunction interacts with multiple CPF indicators through Bayesian networks representing conditional probability relationships.",
    "amplified_by": {
      "description": "Indicators that increase vulnerability to Human-AI Team Dysfunction when present",
      "indicators": {
        "indicator_9.1": {
          "name": "Anthropomorphization of AI Systems",
          "mechanism": "General tendency to attribute human-like mental states to AI creates foundation for dysfunctional team relationships where humans expect implicit understanding and social reciprocity from AI teammates",
          "conditional_probability": "P(9.8|9.1) = 0.74",
          "interaction_strength": "strong"
        },
        "indicator_9.7": {
          "name": "AI Hallucination Acceptance",
          "mechanism": "Accepting AI hallucinations without verification indicates inappropriate trust that extends to team collaboration, preventing effective error detection and correction in human-AI coordination",
          "conditional_probability": "P(9.8|9.7) = 0.68",
          "interaction_strength": "strong"
        },
        "indicator_9.6": {
          "name": "Machine Learning Opacity Trust",
          "mechanism": "Trust in opaque AI systems without understanding decision-making creates dependency relationships that resemble human team trust, leading to coordination expectations AI cannot meet",
          "conditional_probability": "P(9.8|9.6) = 0.66",
          "interaction_strength": "strong"
        },
        "indicator_6.4": {
          "name": "Cognitive Load Overwhelm",
          "mechanism": "Information overload reduces capacity to maintain explicit, structured communication with AI, causing reversion to automatic human-like interaction patterns that lead to coordination failures",
          "conditional_probability": "P(9.8|6.4) = 0.61",
          "interaction_strength": "moderate"
        }
      }
    },
    "amplifies": {
      "description": "Indicators whose vulnerability is increased when Human-AI Team Dysfunction is present",
      "indicators": {
        "indicator_2.5": {
          "name": "Responsibility Diffusion",
          "mechanism": "Dysfunctional human-AI teams create unclear accountability where humans assume AI is responsible for errors and AI cannot accept accountability, leading to systematic responsibility diffusion",
          "conditional_probability": "P(2.5|9.8) = 0.69",
          "interaction_strength": "strong"
        },
        "indicator_4.2": {
          "name": "Overconfidence in Assessment",
          "mechanism": "Treating AI as competent teammate creates false confidence in joint assessments, with humans overestimating quality of human-AI collaborative decisions",
          "conditional_probability": "P(4.2|9.8) = 0.63",
          "interaction_strength": "moderate"
        },
        "indicator_6.1": {
          "name": "Alert Fatigue and Normalization",
          "mechanism": "Dysfunctional coordination means humans either over-trust AI alert filtering (missing threats) or under-trust it (alert overload), both exacerbating alert fatigue patterns",
          "conditional_probability": "P(6.1|9.8) = 0.57",
          "interaction_strength": "moderate"
        },
        "indicator_3.5": {
          "name": "Analysis Paralysis",
          "mechanism": "When human-AI coordination breaks down, teams struggle between over-analyzing AI recommendations and acting without AI input, both contributing to decision paralysis",
          "conditional_probability": "P(3.5|9.8) = 0.54",
          "interaction_strength": "moderate"
        }
      }
    },
    "bayesian_network": {
      "description": "Conditional probability table for Human-AI Team Dysfunction given parent node states",
      "parent_nodes": [
        "9.1",
        "9.7",
        "9.6",
        "6.4"
      ],
      "probability_table": {
        "all_parents_high": 0.92,
        "three_parents_high": 0.78,
        "two_parents_high": 0.61,
        "one_parent_high": 0.39,
        "no_parents_high": 0.19
      },
      "interaction_formula": "P(9.8 = high | parents) = base_rate + Σ(w_i · parent_i · interaction_i)",
      "base_rate": 0.19,
      "parent_weights": {
        "w_9.1": 0.32,
        "w_9.7": 0.28,
        "w_9.6": 0.24,
        "w_6.4": 0.16
      }
    }
  },
  "scoring_algorithm": {
    "description": "Bayesian weighted scoring integrating quick assessment, conversation depth, and red flags to calculate overall vulnerability score",
    "formula": "Final_Score = (w_qa · QA_score + w_cd · CD_score + w_rf · RF_score) · IM",
    "components": {
      "quick_assessment_score": {
        "calculation": "QA_score = Σ(question_weight_i · question_score_i) for i=1 to 7",
        "weight": "w_qa = 0.40",
        "scoring": "Green=0, Yellow=1, Red=2 for each question"
      },
      "conversation_depth_score": {
        "calculation": "CD_score = Σ(indicator_weight_j · assessment_j) for j=1 to 7",
        "weight": "w_cd = 0.35",
        "scoring": "Holistic assessment based on green/yellow/red indicators in conversation responses"
      },
      "red_flags_score": {
        "calculation": "RF_score = Σ(flag_impact_k) for all triggered flags k",
        "weight": "w_rf = 0.25",
        "scoring": "Each red flag contributes its score_impact when present"
      },
      "interdependency_multiplier": {
        "calculation": "IM = 1 + (0.15 · amplifying_indicators_present / total_amplifying_indicators)",
        "range": "[1.0, 1.15]",
        "description": "Amplifies score when related vulnerabilities are present"
      }
    },
    "risk_levels": {
      "low": {
        "range": "Final_Score < 0.40",
        "interpretation": "Functional human-AI collaboration with appropriate boundaries"
      },
      "moderate": {
        "range": "0.40 ≤ Final_Score < 0.70",
        "interpretation": "Some dysfunctional patterns with inconsistent coordination"
      },
      "high": {
        "range": "Final_Score ≥ 0.70",
        "interpretation": "Severe coordination dysfunction requiring urgent intervention"
      }
    }
  },
  "indicator": "9.8"
}