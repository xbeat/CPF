{
  "indicator": "9.6",
  "title": "INDICATOR 9.6 FIELD KIT",
  "subtitle": "Machine Learning Opacity Trust",
  "category": "AI-Specific Bias Vulnerabilities",
  "version": "1.0",
  "cpf_reference": "CPF v1.0 - Category 9.x",
  "description": {
    "short": "Machine Learning Opacity Trust represents a critical vulnerability where humans develop inappropriate trust patterns toward AI systems whose decision-making processes are fundamentally opaque or incomprehensible. This vulnerability operates through trust transfer phenomena, cognitive closure seeking...",
    "context": "Machine Learning Opacity Trust represents a critical vulnerability where humans develop inappropriate trust patterns toward AI systems whose decision-making processes are fundamentally opaque or incomprehensible. This vulnerability operates through trust transfer phenomena, cognitive closure seeking, and competence substitution. Users unconsciously substitute assessments of an AI system's competence in areas they can evaluate (interface design, speed, technical sophistication) for competence in areas they cannot evaluate (accuracy of hidden decision-making processes, data quality, algorithmic bias). This creates security risks when staff accept AI recommendations without verification, making organizations vulnerable to AI-mediated attacks, model poisoning, and decision manipulation.",
    "impact": "See full Bayesian indicator documentation for detailed impact analysis.",
    "psychological_basis": "See full Bayesian indicator documentation for psychological foundations."
  },
  "scoring": {
    "method": "bayesian_weighted",
    "weights": {
      "quick_assessment": 0.4,
      "conversation_depth": 0.35,
      "red_flags": 0.25
    }
  },
  "sections": [
    {
      "id": "quick-assessment",
      "title": "Quick Assessment - Machine Learning Opacity Trust",
      "icon": "ðŸŽ¯",
      "time": "15-20",
      "items": [
        {
          "type": "radio-group",
          "number": "Q1",
          "title": "How does your organization document and review the reasoning behind AI system recommendations, especially for security-related decisions?",
          "options": [
            {
              "value": "green",
              "label": "Regular AI decision reviews with documented reasoning, specific validation processes for security decisions",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Sporadic AI decision documentation, inconsistent review processes",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "Minimal or no documentation of AI decision reasoning, no systematic review",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q2",
          "title": "In the past 6 months, how often have staff members overridden or questioned recommendations from AI-powered security tools?",
          "options": [
            {
              "value": "green",
              "label": "Staff frequently override AI when human judgment differs (15-25% override rate)",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Occasional but inconsistent AI overrides (5-15% override rate)",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "Rare or no instances of staff overriding AI recommendations (<5% override rate)",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q3",
          "title": "What is your organization's procedure when staff cannot understand why an AI system made a specific security recommendation?",
          "options": [
            {
              "value": "green",
              "label": "Formal procedures requiring explanations before implementation, escalation process for unclear recommendations",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Informal guidance about seeking explanations, inconsistently applied",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "No defined procedure, staff expected to accept AI recommendations regardless of understanding",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q4",
          "title": "When procuring AI security tools, what specific questions does your organization ask vendors about explainability and failure modes?",
          "options": [
            {
              "value": "green",
              "label": "Systematic vendor transparency requirements enforced, standardized evaluation criteria including explainability testing",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Some vendor transparency questions but not systematic, no formal scoring",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "No systematic vendor transparency requirements, focus primarily on features and cost",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q5",
          "title": "What processes exist to independently verify AI-generated security recommendations before implementation?",
          "options": [
            {
              "value": "green",
              "label": "Independent verification processes consistently used for high-stakes decisions, documented triggers and procedures",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Verification processes exist but inconsistently applied, unclear triggers",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "Limited or no independent verification of AI decisions",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q6",
          "title": "How often do security team members seek second opinions when AI systems provide counterintuitive recommendations?",
          "options": [
            {
              "value": "green",
              "label": "Evidence of healthy skepticism, regular consultation on unusual AI recommendations",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Mixed patterns of AI trust and skepticism, situation-dependent",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "High confidence in AI despite opacity, counterintuitive recommendations typically accepted without consultation",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q7",
          "title": "What happened the last time an AI security tool provided incorrect analysis or recommendations? How was this identified?",
          "options": [
            {
              "value": "green",
              "label": "Recent failure identified through verification processes, documented response and system adjustments",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Occasional failures identified, informal response without systematic process improvement",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "No recent failures identified (suggesting lack of verification), or failures not documented/analyzed",
              "score": 1
            }
          ]
        }
      ],
      "subsections": []
    },
    {
      "id": "client-conversation",
      "title": "Client Conversation Guide",
      "icon": "ðŸ’¬",
      "time": "25-35",
      "items": [],
      "subsections": []
    }
  ],
  "data_sources": [],
  "validation": {},
  "remediation": {}
}