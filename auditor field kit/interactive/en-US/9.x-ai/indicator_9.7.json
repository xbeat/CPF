{
  "indicator": "9.7",
  "title": "INDICATOR 9.7 FIELD KIT",
  "subtitle": "AI Hallucination Acceptance",
  "category": "AI-Specific Bias Vulnerabilities",
  "version": "1.0",
  "cpf_reference": "CPF v1.0 - Category 9.x",
  "description": {
    "short": "AI Hallucination Acceptance represents a novel cognitive vulnerability where individuals systematically fail to question or verify AI-generated information, even when it contains demonstrable errors or fabrications. This vulnerability emerges from authority transfer mechanisms, anthropomorphization ...",
    "context": "AI Hallucination Acceptance represents a novel cognitive vulnerability where individuals systematically fail to question or verify AI-generated information, even when it contains demonstrable errors or fabrications. This vulnerability emerges from authority transfer mechanisms, anthropomorphization processes, and cognitive offloading. Humans naturally attribute human-like intelligence to AI systems and transfer authority they associate with human expertise to AI, while experiencing information overload leads to delegating fact-checking to AI systems. This creates security risks through AI-mediated social engineering, false authority creation, decision pollution, and trust exploitation where critical security decisions become based on unverified AI-generated content.",
    "impact": "See full Bayesian indicator documentation for detailed impact analysis.",
    "psychological_basis": "See full Bayesian indicator documentation for psychological foundations."
  },
  "scoring": {
    "method": "bayesian_weighted",
    "weights": {
      "quick_assessment": 0.4,
      "conversation_depth": 0.35,
      "red_flags": 0.25
    }
  },
  "sections": [
    {
      "id": "quick-assessment",
      "title": "Quick Assessment - AI Hallucination Acceptance",
      "icon": "ðŸŽ¯",
      "time": "15-20",
      "items": [
        {
          "type": "radio-group",
          "number": "Q1",
          "title": "How often does your organization verify AI-generated security recommendations with human experts or independent sources before implementation?",
          "options": [
            {
              "value": "green",
              "label": "Always/Usually (90%+ of the time) with documented verification processes",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Sometimes (50-89% of the time) with inconsistent verification",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "Rarely/Never (<50% of the time), AI content accepted without verification",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q2",
          "title": "What's your procedure when AI systems provide conflicting information about security threats or vulnerabilities?",
          "options": [
            {
              "value": "green",
              "label": "Formal escalation to human experts with documented resolution process",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Team discussion with some expert consultation but informal process",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "Accept the most recent or confident-sounding AI recommendation without formal review",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q3",
          "title": "How frequently do staff cite AI-generated content in security decisions without mentioning independent verification?",
          "options": [
            {
              "value": "green",
              "label": "Rarely - verification status is consistently mentioned and documented",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Sometimes - about half the time verification status is mentioned",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "Frequently - AI content cited as fact without verification mention",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q4",
          "title": "What's your policy for validating AI-generated threat intelligence or security market research before strategic decisions?",
          "options": [
            {
              "value": "green",
              "label": "Mandatory multi-source validation with documented expert review required",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Recommended validation with oversight approval but can be skipped",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "No specific policy - decisions made on AI content directly",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q5",
          "title": "How does your organization handle discovered errors or inaccuracies in AI-generated security content?",
          "options": [
            {
              "value": "green",
              "label": "Systematic processes that regularly catch and correct AI errors with documented responses",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Occasionally find errors through routine review but no systematic detection process",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "Rarely discover AI errors or have no systematic way to detect them",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q6",
          "title": "How do staff typically respond when asked to question or verify AI security recommendations?",
          "options": [
            {
              "value": "green",
              "label": "Comfortable and supported - questioning AI is considered good practice and encouraged",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Mixed responses - some comfort but some resistance to questioning AI",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "Defensive or resistant - questioning AI seen as doubting digital progress or sophistication",
              "score": 1
            }
          ]
        },
        {
          "type": "radio-group",
          "number": "Q7",
          "title": "What verification steps are built into your AI-assisted security workflows?",
          "options": [
            {
              "value": "green",
              "label": "Multiple mandatory checkpoints with human expert validation throughout workflow",
              "score": 0
            },
            {
              "value": "yellow",
              "label": "Some verification steps but they can be bypassed under time pressure",
              "score": 0.5
            },
            {
              "value": "red",
              "label": "Minimal or no systematic verification built into workflows",
              "score": 1
            }
          ]
        }
      ],
      "subsections": []
    },
    {
      "id": "client-conversation",
      "title": "Client Conversation Guide",
      "icon": "ðŸ’¬",
      "time": "25-35",
      "items": [],
      "subsections": []
    }
  ],
  "data_sources": [],
  "validation": {},
  "remediation": {}
}