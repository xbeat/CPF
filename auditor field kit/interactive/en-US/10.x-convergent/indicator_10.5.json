{
  "indicator": "10.5",
  "title": "INDICATOR 10.5 FIELD KIT",
  "subtitle": "Black Swan Blindness",
  "category": "AI-Specific Bias Vulnerabilities",
  "version": "1.0",
  "cpf_reference": "CPF v1.0 - Category 9.x",
  "description": {
    "short": "Detects organizational inability to recognize, prepare for, or respond to high-impact, unprecedented cyber threats outside existing mental models",
    "context": "Black swan blindness is the organizational inability to recognize, prepare for, or respond to high-impact, unprecedented cyber threats. Organizations become trapped by their own experience and create dangerous blind spots by focusing only on likely threats while ignoring extreme but possible scenarios. This vulnerability enables attackers to exploit completely novel attack vectors that fall outside existing security models, detection systems, and response playbooks.",
    "impact": "Organizations with black swan blindness experience supply chain weaponization through trusted vendor compromises, AI-generated novel attacks that bypass signature-based detection, cascading infrastructure failures through unmapped dependencies, and insider-external hybrid attacks that don't fit threat categories. Historical examples include NotPetya 2017 (nation-state destructive attack disguised as ransomware), SolarWinds 2020 (supply chain compromise affecting 18,000 organizations), Colonial Pipeline 2021 (ransomware causing nationwide fuel shortage), and Target 2013 (HVAC compromise spreading to payment systems).",
    "psychological_basis": "Taleb's Black Swan Theory (2007) - events with three characteristics: unpredictability, massive impact, and retrospective predictability. Kahneman & Tversky's Prospect Theory (1979) - systematic errors in probability estimation, particularly underweighting extreme low-probability events. Narrative Fallacy - constructing coherent stories from incomplete information leading to false confidence in predictions. Confirmation Bias Cascades - information challenging existing mental models systematically filtered out."
  },
  "scoring": {
    "method": "bayesian_weighted",
    "formula": "Final_Score = (w1 Ã— Quick_Assessment + w2 Ã— Conversation_Depth + w3 Ã— Red_Flags) Ã— Interdependency_Multiplier",
    "weights": {
      "quick_assessment": 0.4,
      "conversation_depth": 0.35,
      "red_flags": 0.25
    },
    "maturity_levels": {
      "green": {
        "score_range": [
          0,
          0.33
        ],
        "label": "Low Vulnerability - Adaptive",
        "description": "Organization conducts monthly novel scenario exercises, allocates 20%+ budget to unprecedented threats, has formal procedures for unknown incidents, actively researches extreme scenarios from multiple sources, and regularly tests detection against novel attack methods. Evidence of epistemic humility and scenario diversity.",
        "risk_level": "low",
        "color": "#22c55e"
      },
      "yellow": {
        "score_range": [
          0.34,
          0.66
        ],
        "label": "Moderate Vulnerability - Developing",
        "description": "Organization conducts occasional novel exercises (quarterly), allocates 10-20% budget to new threats, has informal procedures for unusual incidents, sometimes researches emerging threats, and periodically tests against new attack patterns. Some acknowledgment of prediction limits.",
        "risk_level": "medium",
        "color": "#eab308"
      },
      "red": {
        "score_range": [
          0.67,
          1
        ],
        "label": "High Vulnerability - Rigid",
        "description": "Organization only drills known scenarios, allocates <10% budget to new threats, lacks procedures for unprecedented incidents, relies solely on standard threat feeds, and only tests against known attack signatures. Overconfidence in ability to predict and prevent threats.",
        "risk_level": "high",
        "color": "#ef4444"
      }
    },
    "question_weights": {
      "q1_verification_procedures": 0.17,
      "q2_gut_feeling_protocol": 0.16,
      "q3_verification_frequency": 0.15,
      "q4_discomfort_policy": 0.14,
      "q5_training_ai_detection": 0.13,
      "q6_video_verification": 0.13,
      "q7_escalation_speed": 0.12
    }
  },
  "detection_formula": {
    "type": "blindness_assessment",
    "mathematical_model": {
      "primary": "BB(t) = 1 - [Î£(wi Ã— Ai(t)) Ã— M(t)]",
      "components": {
        "black_swan_blindness": {
          "formula": "BB(t) = 1 - [Î£(wi Ã— Ai(t)) Ã— M(t)]",
          "description": "Organizational blindness to unprecedented threats",
          "variables": {
            "Ai(t)": "adaptability score in domain i (scenario planning, research, testing, etc.)",
            "wi": "weight for domain i based on empirical importance",
            "M(t)": "mental model flexibility multiplier (0-1)"
          }
        },
        "mental_model_rigidity": {
          "formula": "M(t) = 1 - [R(t) / R_max]",
          "description": "Inverse of mental model rigidity",
          "variables": {
            "R(t)": "rigidity indicators (budget concentration, scenario homogeneity, procedure inflexibility)",
            "R_max": "maximum observed rigidity in peer organizations"
          }
        },
        "critical_threshold": {
          "formula": "Critical = 1 if BB(t) > 0.6 AND Novel_incident_missed > 2, else 0",
          "description": "Binary critical state when blindness prevents novel threat response",
          "thresholds": {
            "blindness_level": 0.6,
            "novel_incidents_threshold": 2,
            "intervention_required": "immediate"
          }
        }
      },
      "default_weights": {
        "w1_preparation": 0.4,
        "w2_detection": 0.35,
        "w3_culture": 0.25
      },
      "interpretation": {
        "D < 0.3": "Adaptive organization with low black swan vulnerability",
        "0.3 â‰¤ D < 0.6": "Moderate rigidity - enhance scenario diversity and research",
        "D â‰¥ 0.6": "Critical blindness - highly vulnerable to novel threats"
      }
    },
    "data_collection_frequency": "quarterly with continuous threat research monitoring",
    "baseline_establishment": "12-month assessment with peer organization benchmarking"
  },
  "data_sources": {
    "manual_assessment": {
      "primary": [
        "employee_verification_procedures",
        "gut_feeling_escalation_protocols",
        "ai_communication_training_records",
        "ambiguous_interaction_examples"
      ],
      "evidence_required": [
        "ai_human_verification_policy",
        "uncanny_response_protocols",
        "recent_ambiguous_communication_cases",
        "training_materials_ai_detection"
      ]
    },
    "automated_soc": {
      "required": [
        {
          "source": "communication_verification_logs",
          "fields": [
            "message_id",
            "human_likeness_score",
            "verification_performed",
            "escalation_triggered",
            "outcome"
          ],
          "retention": "90_days"
        },
        {
          "source": "interaction_hesitation_metrics",
          "fields": [
            "user_id",
            "interaction_type",
            "response_time",
            "hesitation_indicators",
            "verification_requests"
          ],
          "retention": "60_days"
        },
        {
          "source": "ai_communication_analysis",
          "fields": [
            "communication_id",
            "ai_confidence",
            "human_cues_present",
            "artificial_cues_detected",
            "uncanny_score"
          ],
          "retention": "180_days"
        }
      ],
      "optional": [
        {
          "source": "employee_discomfort_reports",
          "fields": [
            "report_id",
            "interaction_description",
            "discomfort_level",
            "resolution_action"
          ],
          "retention": "365_days"
        },
        {
          "source": "video_call_verification",
          "fields": [
            "call_id",
            "deepfake_detection_score",
            "verification_triggered",
            "authentication_method"
          ],
          "retention": "90_days"
        }
      ],
      "telemetry_mapping": {
        "TD_trust_disruption": {
          "calculation": "Trust disruption based on human-likeness assessment",
          "query": "SELECT -1 * DERIVATIVE(affinity_score, human_likeness_score) FROM ai_interactions WHERE uncanny_range=true"
        },
        "BI_behavioral": {
          "calculation": "Weighted sum of avoidance behaviors",
          "query": "SELECT SUM(weight * behavior_frequency) FROM avoidance_behaviors WHERE time_window='30d'"
        },
        "Interaction_drop": {
          "calculation": "Reduction in interaction frequency with uncanny AI",
          "query": "SELECT (baseline_frequency - current_frequency) / baseline_frequency FROM interaction_patterns WHERE uncanny_detected=true"
        }
      }
    },
    "integration_apis": {
      "communication_platforms": "Email/Chat APIs - Message Analysis, Human Cue Detection",
      "video_conferencing": "Video Call APIs - Deepfake Detection Integration",
      "nlp_services": "Natural Language Processing - Uncanny Language Pattern Detection",
      "biometric_verification": "Authentication APIs - Multi-Factor Human Verification"
    }
  },
  "interdependencies": {
    "primary": {
      "10.1": {
        "type": "obscures",
        "weight": 0.2,
        "description": "Black swan blindness prevents recognition of perfect storm conditions until they're critical",
        "mathematical_relationship": "D_10.5 > 0.5 delays perfect storm detection by average 48-72 hours"
      },
      "10.2": {
        "type": "enables",
        "weight": 0.25,
        "description": "Blindness to unprecedented cascade patterns allows failure propagation to go unrecognized",
        "mathematical_relationship": "D_10.5 > 0.5 delays cascade detection by average 24-48 hours"
      },
      "10.3": {
        "type": "amplifies",
        "weight": 0.3,
        "description": "Inability to recognize approaching tipping points until they're crossed",
        "mathematical_relationship": "Tipping point recognition lag = D_10.5 Ã— 72 hours"
      },
      "10.4": {
        "type": "compounds",
        "weight": 0.25,
        "description": "Blindness to novel alignment patterns in defensive layers",
        "mathematical_relationship": "D_10.5 > 0.5 reduces alignment detection probability by 40%"
      }
    },
    "secondary": {
      "6.1": {
        "type": "creates",
        "weight": 0.22,
        "description": "Misplaced trust in familiar systems creates blindness to novel exploitation",
        "mathematical_relationship": "Novel threat blindness = D_6.1 Ã— D_10.5"
      },
      "8.2": {
        "type": "amplifies",
        "weight": 0.2,
        "description": "Institutional memory loss prevents learning from rare unprecedented events",
        "mathematical_relationship": "Black swan learning deficit = D_8.2 Ã— (historical novel incidents)"
      },
      "5.1": {
        "type": "compounds",
        "weight": 0.18,
        "description": "Overconfidence bias strengthens blindness to unprecedented threats",
        "mathematical_relationship": "Confidence-blindness correlation = 0.7 Ã— D_5.1 Ã— D_10.5"
      },
      "3.1": {
        "type": "enables",
        "weight": 0.15,
        "description": "Siloed ownership prevents holistic view needed to recognize novel systemic threats",
        "mathematical_relationship": "Systemic threat blindness = D_3.1 Ã— (novel threat complexity)"
      }
    }
  },
  "sections": [
    {
      "id": "quick-assessment",
      "icon": "âš¡",
      "title": "QUICK ASSESSMENT",
      "time": 5,
      "type": "radio-questions",
      "scoring_method": "weighted_average",
      "items": [
        {
          "type": "radio-list",
          "number": 1,
          "id": "q1_novel_exercises",
          "weight": 0.18,
          "title": "Question Title",
          "question": "How often does your organization conduct security exercises or tabletop drills that involve completely new attack scenarios you've never seen before?",
          "options": [
            {
              "value": "monthly_novel",
              "label": "Monthly or more frequent exercises with genuinely novel, unprecedented scenarios beyond current threat intelligence",
              "score": 0,
              "follow_up": "Describe your most recent novel scenario exercise and what you learned."
            },
            {
              "value": "quarterly_novel",
              "label": "Quarterly exercises including some novel scenarios mixed with known threat drills",
              "score": 0.5,
              "follow_up": "How do you ensure scenarios are truly novel vs. variations of known threats?"
            },
            {
              "value": "occasional_novel",
              "label": "Occasional novel scenarios (1-2 per year) but primarily drill known threat responses",
              "score": 0.85,
              "follow_up": "What gaps have you identified in your ability to handle unprecedented threats?"
            },
            {
              "value": "known_only",
              "label": "Only practice responses to known, documented threat scenarios from threat intelligence",
              "score": 1,
              "follow_up": "How would your team respond to a completely unprecedented attack type?"
            }
          ],
          "evidence_required": "",
          "soc_mapping": ""
        },
        {
          "type": "radio-list",
          "number": 2,
          "id": "q2_unknown_response",
          "weight": 0.17,
          "title": "Question Title",
          "question": "What is your process when your security team encounters an attack pattern that doesn't match any known threat intelligence or existing playbooks?",
          "options": [
            {
              "value": "formal_unknown_protocol",
              "label": "Formal incident response protocol specifically for unknown threats with escalation, investigation, and adaptation procedures",
              "score": 0,
              "follow_up": "Walk us through how your unknown threat protocol worked in your example."
            },
            {
              "value": "informal_adaptation",
              "label": "Informal process of adapting existing procedures; eventually effective but with some delays",
              "score": 0.5,
              "follow_up": "How long did adaptation take and what challenges occurred?"
            },
            {
              "value": "playbook_forcing",
              "label": "Attempt to force unknown threats into existing playbook categories, leading to suboptimal responses",
              "score": 0.85,
              "follow_up": "What problems resulted from forcing unknown threats into existing categories?"
            },
            {
              "value": "no_unknown_process",
              "label": "No process for unknown threats; team paralyzed or resorts to ad-hoc responses",
              "score": 1,
              "follow_up": "What were the consequences of not having procedures for unprecedented attacks?"
            }
          ],
          "evidence_required": "",
          "soc_mapping": ""
        },
        {
          "type": "radio-list",
          "number": 3,
          "id": "q3_budget_allocation",
          "weight": 0.15,
          "title": "Question Title",
          "question": "How does your organization allocate security budget between addressing known threats versus preparing for unprecedented attack scenarios?",
          "options": [
            {
              "value": "balanced_allocation",
              "label": "20%+ of security budget explicitly allocated to emerging/unprecedented threats, R&D, and novel detection methods",
              "score": 0,
              "follow_up": "Show us specific investments in unprecedented threat preparation from recent budget."
            },
            {
              "value": "some_innovation",
              "label": "10-20% allocated to new threats and emerging technologies with some R&D investment",
              "score": 0.5,
              "follow_up": "What emerging threat capabilities have you developed with this investment?"
            },
            {
              "value": "minimal_innovation",
              "label": "5-10% for new threats; primarily focused on known threat mitigation and compliance",
              "score": 0.85,
              "follow_up": "How do you justify minimal investment in unprecedented threat preparation?"
            },
            {
              "value": "known_threats_only",
              "label": "<5% for emerging threats; budget concentrated entirely on known, documented risks",
              "score": 1,
              "follow_up": "What would happen if you faced a completely novel attack outside your budget planning?"
            }
          ],
          "evidence_required": "",
          "soc_mapping": ""
        },
        {
          "type": "radio-list",
          "number": 4,
          "id": "q4_risk_updates",
          "weight": 0.14,
          "title": "Question Title",
          "question": "What's your procedure for updating risk assessments when completely new types of cyber threats emerge in the industry?",
          "options": [
            {
              "value": "proactive_updates",
              "label": "Proactive risk model updates with formal process for incorporating novel threat categories and challenging assumptions",
              "score": 0,
              "follow_up": "Show us recent risk model updates based on novel industry threats."
            },
            {
              "value": "reactive_updates",
              "label": "Reactive updates after novel threats emerge; some process for risk model adaptation",
              "score": 0.5,
              "follow_up": "How long does it typically take to incorporate novel threats into risk assessments?"
            },
            {
              "value": "slow_adaptation",
              "label": "Slow adaptation; novel threats eventually incorporated but only after significant lag",
              "score": 0.85,
              "follow_up": "What novel threats took extended time to incorporate into your risk model?"
            },
            {
              "value": "static_model",
              "label": "Static risk model based on historical threats; no systematic process for incorporating novel risks",
              "score": 1,
              "follow_up": "How do novel threat types fit into your current risk assessment framework?"
            }
          ],
          "evidence_required": "",
          "soc_mapping": ""
        },
        {
          "type": "radio-list",
          "number": 5,
          "id": "q5_threat_research",
          "weight": 0.13,
          "title": "Question Title",
          "question": "How frequently does your security team actively seek information about theoretical, emerging, or extreme threat scenarios beyond your current threat intelligence feeds?",
          "options": [
            {
              "value": "continuous_research",
              "label": "Continuous research using diverse sources including academic papers, proof-of-concepts, theoretical attacks, and extreme scenarios",
              "score": 0,
              "follow_up": "Show us your threat research sources and recent novel threats researched."
            },
            {
              "value": "periodic_research",
              "label": "Monthly or quarterly research beyond standard feeds; some exposure to theoretical and emerging threats",
              "score": 0.5,
              "follow_up": "What emerging threats have you discovered through research that weren't in standard feeds?"
            },
            {
              "value": "limited_research",
              "label": "Occasional ad-hoc research when specific novel threats make headlines; limited systematic exploration",
              "score": 0.85,
              "follow_up": "How do you discover novel threats before they make headlines?"
            },
            {
              "value": "feeds_only",
              "label": "Rely exclusively on standard commercial threat intelligence feeds without independent research",
              "score": 1,
              "follow_up": "What happens when novel threats aren't covered by your standard feeds?"
            }
          ],
          "evidence_required": "",
          "soc_mapping": ""
        },
        {
          "type": "radio-list",
          "number": 6,
          "id": "q6_unusual_investigation",
          "weight": 0.11,
          "title": "Question Title",
          "question": "What's your policy for investigating security incidents that don't fit existing incident categories or response procedures?",
          "options": [
            {
              "value": "enhanced_investigation",
              "label": "Enhanced investigation procedures for unusual incidents with expanded forensics, external expertise, and root cause analysis",
              "score": 0,
              "follow_up": "Walk us through your enhanced investigation process for unusual incidents."
            },
            {
              "value": "adapted_procedures",
              "label": "Standard procedures adapted for unusual incidents; some additional analysis but not systematic",
              "score": 0.5,
              "follow_up": "What aspects of unusual incidents are captured vs. missed by adapted procedures?"
            },
            {
              "value": "forced_categorization",
              "label": "Unusual incidents forced into existing categories with standard investigation depth",
              "score": 0.85,
              "follow_up": "What insights have been lost by forcing unusual incidents into standard categories?"
            },
            {
              "value": "minimal_investigation",
              "label": "Minimal investigation of unusual incidents; focus on containment without understanding novel aspects",
              "score": 1,
              "follow_up": "How do you learn from unusual incidents to improve detection and response?"
            }
          ],
          "evidence_required": "",
          "soc_mapping": ""
        },
        {
          "type": "radio-list",
          "number": 7,
          "id": "q7_detection_testing",
          "weight": 0.1,
          "title": "Question Title",
          "question": "How does your organization test whether your current security controls would detect completely novel attack methods?",
          "options": [
            {
              "value": "regular_novel_testing",
              "label": "Monthly or quarterly testing with red teams developing genuinely novel attack methods specifically to test detection gaps",
              "score": 0,
              "follow_up": "Show us recent testing results identifying detection gaps for novel attacks."
            },
            {
              "value": "occasional_testing",
              "label": "Annual or semi-annual testing including some novel attack scenarios but not comprehensive",
              "score": 0.5,
              "follow_up": "What novel attack methods did your last test reveal you couldn't detect?"
            },
            {
              "value": "signature_testing",
              "label": "Testing focused on known attack signatures and documented TTPs without novel method testing",
              "score": 0.85,
              "follow_up": "How would you know if a completely new attack method bypassed your detection?"
            },
            {
              "value": "no_novel_testing",
              "label": "No testing of detection capabilities against novel or unprecedented attack methods",
              "score": 1,
              "follow_up": "What assurance do you have that your detection works for unknown threats?"
            }
          ],
          "evidence_required": "",
          "soc_mapping": ""
        },
        {
          "type": "radio-list",
          "number": 8,
          "id": "q8_epistemic_humility",
          "weight": 0.02,
          "title": "Question Title",
          "question": "How does your organization's leadership discuss uncertainty and limitations in predicting future cyber threats?",
          "options": [
            {
              "value": "acknowledged_uncertainty",
              "label": "Leadership explicitly acknowledges fundamental uncertainty and limits of prediction while maintaining preparedness",
              "score": 0,
              "follow_up": "Give us examples of leadership communications acknowledging uncertainty."
            },
            {
              "value": "mixed_messaging",
              "label": "Mixed messaging - sometimes acknowledges uncertainty, sometimes projects complete confidence",
              "score": 0.5,
              "follow_up": "How does messaging inconsistency affect team response to novel threats?"
            },
            {
              "value": "projected_confidence",
              "label": "Leadership consistently projects confidence in ability to predict and prevent threats",
              "score": 1,
              "follow_up": "How does overconfidence affect team willingness to raise concerns about unprecedented threats?"
            }
          ],
          "evidence_required": "",
          "soc_mapping": ""
        }
      ],
      "subsections": [],
      "instructions": "Select ONE option for each question. Each answer contributes to the weighted final score. Evidence should be documented for audit trail.",
      "calculation": "Quick_Score = Î£(question_score Ã— question_weight) / Î£(question_weight)"
    },
    {
      "id": "client-conversation",
      "icon": "ðŸ’¬",
      "title": "CLIENT CONVERSATION",
      "time": 15,
      "type": "conversation",
      "scoring_method": "qualitative_depth",
      "items": [],
      "subsections": [
        {
          "title": "Opening Questions - Verification and Discomfort Handling",
          "weight": 0.35,
          "items": [
            {
              "type": "question",
              "id": "conv_q1",
              "text": "How does your organization handle verification when employees receive communications from AI systems or chatbots (customer service bots, automated assistants, AI-generated emails)? Tell us your specific example of a recent AI interaction that required verification.",
              "scoring_guidance": {
                "green": "Clear verification procedures with specific recent example showing effective handling",
                "yellow": "Informal awareness but no systematic verification process",
                "red": "No distinction between AI and human communications or verification procedures"
              },
              "followups": [
                {
                  "type": "Follow-up",
                  "text": "How do employees know when they're interacting with AI versus humans in your systems?",
                  "evidence_type": "transparency_assessment"
                },
                {
                  "type": "Follow-up",
                  "text": "Have you had situations where employees were confused about whether they were talking to AI or a person?",
                  "evidence_type": "ambiguity_examples"
                }
              ]
            },
            {
              "type": "question",
              "id": "conv_q2",
              "text": "What's your procedure when employees report feeling 'something seems off' about digital communications, even if they can't pinpoint why? Give us a recent example where an employee had this gut feeling about a message or interaction.",
              "scoring_guidance": {
                "green": "Formal investigation protocol with recent example of successful gut-feeling escalation",
                "yellow": "Informal handling without systematic process",
                "red": "No protocol or dismissal of intuitive concerns without concrete evidence"
              },
              "followups": [
                {
                  "type": "Follow-up",
                  "text": "Do you encourage or discourage employees from reporting when something 'just feels wrong' even without proof?",
                  "evidence_type": "reporting_culture"
                }
              ]
            }
          ]
        },
        {
          "title": "Colleague Verification and Training",
          "weight": 0.3,
          "items": [
            {
              "type": "question",
              "id": "conv_q3",
              "text": "How often do employees ask colleagues to verify whether communications are from humans or AI systems? Tell us about the last time this happened and how it was handled.",
              "scoring_guidance": {
                "green": "Regular verification requests with documented process and recent example",
                "yellow": "Occasional verification but no formal tracking or process",
                "red": "Rare or never - employees don't seek verification for AI-human ambiguity"
              },
              "followups": [
                {
                  "type": "Follow-up",
                  "text": "Is asking for this kind of verification seen as normal or does it raise eyebrows?",
                  "evidence_type": "cultural_acceptance"
                }
              ]
            },
            {
              "type": "question",
              "id": "conv_q4",
              "text": "What's your policy for employees who express discomfort or confusion about whether they're interacting with AI systems versus humans in work communications? Provide a specific example of how your team handled such a situation.",
              "scoring_guidance": {
                "green": "Supportive policy with specific handling example showing employee protection",
                "yellow": "Limited support, acknowledged but no formal procedures",
                "red": "No policy or discomfort dismissed as overreaction to technology"
              },
              "followups": [
                {
                  "type": "Follow-up",
                  "text": "Have employees ever been criticized for being 'too suspicious' of AI communications?",
                  "evidence_type": "psychological_safety"
                }
              ]
            },
            {
              "type": "question",
              "id": "conv_q5",
              "text": "How does your organization train employees to distinguish between legitimate AI security tools and potentially malicious AI-generated communications? Tell us about your most recent training session or guidance on this topic.",
              "scoring_guidance": {
                "green": "Comprehensive training with specific examples and recent session details",
                "yellow": "Basic awareness without specific AI detection skills",
                "red": "No training on distinguishing legitimate vs malicious AI"
              },
              "followups": [
                {
                  "type": "Follow-up",
                  "text": "Does your training include examples of deepfakes or sophisticated AI-generated content?",
                  "evidence_type": "training_content_quality"
                }
              ]
            }
          ]
        },
        {
          "title": "Video Verification and Escalation",
          "weight": 0.35,
          "items": [
            {
              "type": "question",
              "id": "conv_q6",
              "text": "What happens when employees receive video calls or messages that look almost real but something feels 'wrong' about the person's appearance or behavior? Give us an example of how your team would handle a suspicious video communication.",
              "scoring_guidance": {
                "green": "Clear protocol with multi-factor verification and hypothetical/actual example",
                "yellow": "Informal awareness but no formal deepfake verification process",
                "red": "No protocol or assumption that video means legitimate communication"
              },
              "followups": [
                {
                  "type": "Follow-up",
                  "text": "Do you have any technical tools for detecting deepfakes or manipulated video?",
                  "evidence_type": "technical_controls"
                },
                {
                  "type": "Follow-up",
                  "text": "What's the backup verification method if video authenticity is questioned?",
                  "evidence_type": "alternative_verification"
                }
              ]
            },
            {
              "type": "question",
              "id": "conv_q7",
              "text": "How quickly can employees escalate concerns about AI-human ambiguity in communications to security teams? Tell us about your escalation process and provide a recent example.",
              "scoring_guidance": {
                "green": "Fast escalation (<30 min) with documented process and recent example",
                "yellow": "Moderate speed (1-4 hours) or unclear escalation path",
                "red": "Slow escalation (>4 hours) or no clear process for AI ambiguity concerns"
              },
              "followups": [
                {
                  "type": "Follow-up",
                  "text": "Is there a dedicated channel or process specifically for reporting uncanny or suspicious AI interactions?",
                  "evidence_type": "specialized_channel"
                }
              ]
            }
          ]
        },
        {
          "title": "Probing for Red Flags",
          "weight": 0,
          "description": "Observable indicators that increase vulnerability score regardless of stated policies",
          "items": [
            {
              "type": "checkbox",
              "id": "red_flag_1",
              "label": "\"We don't have procedures for AI-human verification - it's not a real problem...\"",
              "severity": "critical",
              "score_impact": 0.16,
              "subitems": []
            },
            {
              "type": "checkbox",
              "id": "red_flag_2",
              "label": "\"Employees who report 'gut feelings' about communications are being overly paranoid...\"",
              "severity": "critical",
              "score_impact": 0.15,
              "subitems": []
            },
            {
              "type": "checkbox",
              "id": "red_flag_3",
              "label": "\"We can't tell the difference between AI and human communications and don't think we need to...\"",
              "severity": "high",
              "score_impact": 0.14,
              "subitems": []
            },
            {
              "type": "checkbox",
              "id": "red_flag_4",
              "label": "\"Video calls are always authentic - we don't worry about deepfakes...\"",
              "severity": "high",
              "score_impact": 0.13,
              "subitems": []
            },
            {
              "type": "checkbox",
              "id": "red_flag_5",
              "label": "\"No training on AI detection - we assume people can figure it out...\"",
              "severity": "high",
              "score_impact": 0.14,
              "subitems": []
            },
            {
              "type": "checkbox",
              "id": "red_flag_6",
              "label": "\"Escalation for AI ambiguity concerns takes hours or days - it's low priority...\"",
              "severity": "medium",
              "score_impact": 0.12,
              "subitems": []
            },
            {
              "type": "checkbox",
              "id": "red_flag_7",
              "label": "\"We've never had anyone report discomfort with AI communications...\" (suggests no reporting culture)\"",
              "severity": "medium",
              "score_impact": 0.11,
              "subitems": []
            }
          ]
        }
      ],
      "calculation": "Conversation_Score = Weighted_Average(subsection_scores) + Î£(red_flag_impacts)"
    }
  ],
  "validation": {
    "method": "matthews_correlation_coefficient",
    "formula": "V = (TPÂ·TN - FPÂ·FN) / sqrt((TP+FP)(TP+FN)(TN+FP)(TN+FN))",
    "continuous_validation": {
      "synthetic_testing": "Inject uncanny AI communication scenarios monthly to test response protocols",
      "correlation_analysis": "Compare manual assessment with automated behavioral metrics (target correlation > 0.75)",
      "drift_detection": "Kolmogorov-Smirnov test on verification patterns, recalibrate if p < 0.05"
    },
    "calibration": {
      "method": "isotonic_regression",
      "description": "Ensure uncanny valley scores predict AI communication security incidents",
      "baseline_period": "90_days",
      "recalibration_trigger": "Drift detected or validation score < 0.70"
    },
    "success_metrics": [
      {
        "metric": "Verification Protocol Utilization Rate",
        "formula": "% of ambiguous AI communications that trigger verification procedures",
        "baseline": "current verification rate from employee surveys and system logs",
        "target": "80% improvement in verification usage within 90 days",
        "measurement": "monthly automated analysis of verification logs"
      },
      {
        "metric": "Uncanny Valley Escalation Response Time",
        "formula": "Time from employee uncertainty report to security team investigation completion",
        "baseline": "current response time from ticketing system",
        "target": "<30 minutes initial response, <2 hours investigation completion",
        "measurement": "continuous monitoring of ticketing timestamps"
      },
      {
        "metric": "False Trust Incident Reduction",
        "formula": "Security incidents where employees inappropriately trusted AI-generated communications",
        "baseline": "current incident rate from security reports",
        "target": "70% reduction within 90 days",
        "measurement": "quarterly incident analysis and employee feedback surveys"
      }
    ]
  },
  "remediation": {
    "solutions": [
      {
        "id": "sol_1",
        "title": "AI Communication Labeling Protocol",
        "description": "Implement mandatory labeling system for all AI-generated communications",
        "implementation": "Deploy technical controls that automatically tag AI messages with clear identifiers. Establish verification requirements for any unlabeled communications claiming human origin. Create escalation pathway for communications that lack proper AI/human identification.",
        "technical_controls": "Automated AI message tagging, identity verification system, unlabeled communication flags",
        "roi": "305% average within 12 months",
        "effort": "medium",
        "timeline": "45-60 days"
      },
      {
        "id": "sol_2",
        "title": "Uncanny Valley Response Training",
        "description": "Develop 20-minute training module teaching employees to recognize and trust their 'uncanny' feelings",
        "implementation": "Include practical exercises using examples of legitimate vs malicious AI communications. Train employees to use verification protocols when experiencing psychological discomfort with digital interactions. Provide clear scripts for escalating 'something feels wrong' concerns to security teams.",
        "technical_controls": "Training platform with scenario library, competency tracking, escalation scripts",
        "roi": "265% average within 18 months",
        "effort": "low",
        "timeline": "30 days initial, quarterly updates"
      },
      {
        "id": "sol_3",
        "title": "Two-Channel Verification System",
        "description": "Establish policy requiring verification through separate communication channel for any high-stakes requests",
        "implementation": "Implement technical system that automatically prompts verification for financial, access, or sensitive data requests. Create simple process for employees to quickly verify human identity through alternative means. Deploy phone or in-person verification requirements for unusual requests, regardless of source authenticity.",
        "technical_controls": "Dual-channel verification automation, alternative verification methods, high-risk flagging",
        "roi": "330% average within 12 months",
        "effort": "medium",
        "timeline": "45 days"
      },
      {
        "id": "sol_4",
        "title": "Psychological Safety Protocols",
        "description": "Create formal process for employees to report 'uncanny' or uncomfortable digital interactions without judgment",
        "implementation": "Establish security team response procedure for investigating ambiguous AI-human communications. Implement policy protecting employees who escalate based on intuitive concerns rather than technical evidence. Deploy rapid-response system for employees experiencing confusion about communication authenticity.",
        "technical_controls": "Anonymous reporting portal, rapid response ticketing, protection policy enforcement",
        "roi": "245% average within 18 months",
        "effort": "low",
        "timeline": "30 days"
      },
      {
        "id": "sol_5",
        "title": "AI Interaction Baseline Monitoring",
        "description": "Deploy system to monitor patterns in employee responses to AI communications",
        "implementation": "Establish baseline metrics for normal AI interaction behaviors (response times, verification requests, escalations). Create alerts for unusual patterns that might indicate uncanny valley exploitation. Implement automated detection for communication patterns that typically trigger uncanny valley responses.",
        "technical_controls": "Behavioral analytics platform, baseline tracking, anomaly detection, alert system",
        "roi": "290% average within 12 months",
        "effort": "high",
        "timeline": "60-90 days"
      },
      {
        "id": "sol_6",
        "title": "Enhanced Authentication for Ambiguous Communications",
        "description": "Deploy multi-factor verification system triggered by employee uncertainty reports",
        "implementation": "Implement biometric or behavioral authentication for video/audio communications when requested. Create technical controls that flag communications exhibiting uncanny valley characteristics. Establish secure verification codes or phrases for confirming human identity in suspicious interactions.",
        "technical_controls": "Multi-factor authentication, biometric verification, deepfake detection integration",
        "roi": "315% average within 12 months",
        "effort": "high",
        "timeline": "60-90 days"
      }
    ],
    "prioritization": {
      "critical_first": [
        "sol_1",
        "sol_3"
      ],
      "high_value": [
        "sol_6",
        "sol_5"
      ],
      "cultural_foundation": [
        "sol_2",
        "sol_4"
      ],
      "governance": [
        "sol_1"
      ]
    }
  },
  "risk_scenarios": [
    {
      "id": "supply_chain_weaponization",
      "title": "Supply Chain Weaponization",
      "description": "Attackers compromise trusted software vendors or service providers to deliver malware through legitimate update mechanisms. Organizations with black swan blindness fail to detect these attacks because they violate fundamental trust assumptions about vendor relationships, leading to widespread compromise before detection.",
      "likelihood": "medium",
      "impact": "critical",
      "attack_pattern": {
        "reconnaissance": "Identify trusted vendor relationships and update mechanisms",
        "timing": "Compromise during vendor organizational stress or transition",
        "execution": "Inject malicious code into legitimate software updates",
        "exploitation": "Widespread deployment through trusted update channels before detection"
      }
    },
    {
      "id": "ai_novel_attacks",
      "title": "AI-Generated Novel Attacks",
      "description": "Sophisticated attackers use machine learning to generate entirely new attack methodologies that don't match existing threat patterns. Security teams cannot detect or respond effectively because their tools and processes are designed only for known attack signatures and behaviors.",
      "likelihood": "medium-low",
      "impact": "high",
      "attack_pattern": {
        "reconnaissance": "ML analysis of defensive systems to identify blind spots",
        "timing": "Continuous generation of novel attack variants",
        "execution": "Automated deployment of ML-generated attack methods",
        "exploitation": "Exploitation of signature-based and behavioral detection limitations"
      }
    },
    {
      "id": "infrastructure_cascade",
      "title": "Cascading Infrastructure Dependencies",
      "description": "Nation-state actors target unexpected infrastructure dependencies (HVAC, building systems, supply chain logistics) to create cascading failures that impact cybersecurity systems. Organizations haven't mapped these dependencies and lack procedures for responding to attacks that enter through non-IT vectors.",
      "likelihood": "low",
      "impact": "critical",
      "attack_pattern": {
        "reconnaissance": "Mapping of non-obvious infrastructure dependencies",
        "timing": "Attack on physical/operational systems during cyber stress",
        "execution": "Compromise of building systems, HVAC, or physical infrastructure",
        "exploitation": "Cascade from physical to cyber systems through unmapped dependencies"
      }
    },
    {
      "id": "insider_external_hybrid",
      "title": "Insider-External Hybrid Attacks",
      "description": "Complex attacks combine legitimate insider access with external capabilities in unprecedented ways that security teams have never modeled. Detection systems fail because the attack doesn't fit existing insider threat or external threat categories, allowing extended dwell time and maximum damage.",
      "likelihood": "medium",
      "impact": "high",
      "attack_pattern": {
        "reconnaissance": "Insider recruitment or compromise for access credentials",
        "timing": "Coordinated insider and external actions during monitoring gaps",
        "execution": "Insider provides access while external actors perform technical exploitation",
        "exploitation": "Category confusion prevents detection by insider or external threat systems"
      }
    }
  ],
  "metadata": {
    "language": "en-US",
    "language_name": "English (United States)",
    "is_translation": false,
    "translated_from": null,
    "translation_date": null,
    "translator": null,
    "created_date": "2025-11-08",
    "last_modified": "2025-11-08",
    "version_history": [
      {
        "version": "1.0",
        "date": "2025-11-08",
        "changes": "Initial release - Category 10.x batch generation",
        "author": "CPF Development Team"
      }
    ]
  }
}